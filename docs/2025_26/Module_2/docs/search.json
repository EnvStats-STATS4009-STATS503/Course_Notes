[
  {
    "objectID": "slides/slides_6.html#section",
    "href": "slides/slides_6.html#section",
    "title": "Modelling Environmental Extremes",
    "section": "",
    "text": "“On the subject of climate change, the data clearly show that Earth is warming up, and that humans are contributing to the warming. That’s the fairest assessment of the evidence.”\nNeil deGrasse Tyson"
  },
  {
    "objectID": "slides/slides_6.html#recap",
    "href": "slides/slides_6.html#recap",
    "title": "Modelling Environmental Extremes",
    "section": "Recap",
    "text": "Recap\n\nFor the last two weeks, we have looked at time series, and how appropriate models can be fitted that deal with the temporal structures in the data.\nIn some cases, we want to specifically look at the maxima or minima of a particular environmental system.\nThis week, we will look at methods for modelling these statistical extremes."
  },
  {
    "objectID": "slides/slides_6.html#extremes",
    "href": "slides/slides_6.html#extremes",
    "title": "Modelling Environmental Extremes",
    "section": "Extremes",
    "text": "Extremes\n\n\nWe are trying to model rare events - by their very definition we won’t have a lot of data on these.\nThe bulk of the data in any statistical distribution will be in the centre.\nStandard density estimation techniques (e.g., the normal distribution) work well where the data have the greatest density, but that’s not what we need here.\nWe need to use a statistical model which is good at estimating the tails of our distribution."
  },
  {
    "objectID": "slides/slides_6.html#motivation",
    "href": "slides/slides_6.html#motivation",
    "title": "Modelling Environmental Extremes",
    "section": "Motivation",
    "text": "Motivation\nStatistical modelling of extreme environmental phenomena has a very practical motivation: reliability - anything we build needs to have a good chance of surviving the weather/environment for the whole of its working life.\nThis has obvious implications for civil engineers and planners. They need to know:\n\nhow strong to make buildings;\nhow high to build sea walls;\nhow tall to build reservoir dams;\nhow much fuel to stockpile."
  },
  {
    "objectID": "slides/slides_6.html#motivation-continued",
    "href": "slides/slides_6.html#motivation-continued",
    "title": "Modelling Environmental Extremes",
    "section": "Motivation (continued)",
    "text": "Motivation (continued)\nThis motivates the need to estimate what the:\n\nstrongest wind;\nhighest tide;\nheaviest rainfall;\nmost severe cold-spell;\n\netc. will be over some fixed period of future time.\nThe only sensible way to do this is to use data on the variable of interest (wind, rain etc.) and fit an appropriate statistical model.\nThe models themselves are motivated by asymptotic theory, and this is our starting point."
  },
  {
    "objectID": "slides/slides_6.html#what-are-your-thoughts",
    "href": "slides/slides_6.html#what-are-your-thoughts",
    "title": "Modelling Environmental Extremes",
    "section": "What are your thoughts?",
    "text": "What are your thoughts?\nJoin mentimeter"
  },
  {
    "objectID": "slides/slides_6.html#environmental-extremes-in-our-life",
    "href": "slides/slides_6.html#environmental-extremes-in-our-life",
    "title": "Modelling Environmental Extremes",
    "section": "Environmental extremes in our life",
    "text": "Environmental extremes in our life\nEnvironmental extreme events are closer than you think…"
  },
  {
    "objectID": "slides/slides_6.html#environmental-extremes-in-our-life-1",
    "href": "slides/slides_6.html#environmental-extremes-in-our-life-1",
    "title": "Modelling Environmental Extremes",
    "section": "Environmental extremes in our life",
    "text": "Environmental extremes in our life"
  },
  {
    "objectID": "slides/slides_6.html#defining-an-extreme",
    "href": "slides/slides_6.html#defining-an-extreme",
    "title": "Modelling Environmental Extremes",
    "section": "Defining an extreme",
    "text": "Defining an extreme\n\nThe first thing we have to consider is what actually represents an extreme observation.\nThis will vary depending on the context of the dataset.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe could just look at the biggest (or smallest) values.\nIf so, over what time period?"
  },
  {
    "objectID": "slides/slides_6.html#defining-an-extreme-1",
    "href": "slides/slides_6.html#defining-an-extreme-1",
    "title": "Modelling Environmental Extremes",
    "section": "Defining an extreme",
    "text": "Defining an extreme\n\nThe first thing we have to consider is what actually represents an extreme observation.\nThis will vary depending on the context of the dataset.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe could just look at the biggest (or smallest) values.\nIf so, over what time period? Annual maximum? Over the whole dataset? Both?"
  },
  {
    "objectID": "slides/slides_6.html#defining-an-extreme-2",
    "href": "slides/slides_6.html#defining-an-extreme-2",
    "title": "Modelling Environmental Extremes",
    "section": "Defining an extreme",
    "text": "Defining an extreme\n\nThe first thing we have to consider is what actually represents an extreme observation.\nThis will vary depending on the context of the dataset.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe could just look at the biggest (or smallest) values.\nIf so, over what time period? Annual maximum? Over the whole dataset? Both?"
  },
  {
    "objectID": "slides/slides_6.html#defining-an-extreme-3",
    "href": "slides/slides_6.html#defining-an-extreme-3",
    "title": "Modelling Environmental Extremes",
    "section": "Defining an extreme",
    "text": "Defining an extreme\n\nThe first thing we have to consider is what actually represents an extreme observation.\nThis will vary depending on the context of the dataset.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe could just look at the biggest (or smallest) values.\nIf so, over what time period? Annual maximum? Over the whole dataset? Both?\nOther times, it may be all observations above a certain threshold."
  },
  {
    "objectID": "slides/slides_6.html#defining-an-extreme-4",
    "href": "slides/slides_6.html#defining-an-extreme-4",
    "title": "Modelling Environmental Extremes",
    "section": "Defining an extreme",
    "text": "Defining an extreme\n\nThe first thing we have to consider is what actually represents an extreme observation.\nThis will vary depending on the context of the dataset.\n\n\n\n\n\n\n\n\n\n\nWe could just look at the biggest (or smallest) values.\nIf so, over what time period? Annual maximum? Over the whole dataset? Both?\nOther times, it may be all observations above a certain threshold.\nBut then how to select the threshold?"
  },
  {
    "objectID": "slides/slides_6.html#extreme-value-theory",
    "href": "slides/slides_6.html#extreme-value-theory",
    "title": "Modelling Environmental Extremes",
    "section": "Extreme Value theory",
    "text": "Extreme Value theory\n\n\n\nExtreme value modelling has a central theoretical result, analogous to the Central Limit Theorem…\n\nSuppose we have a series of random variables \\(X_1, \\ldots, X_n\\), each with cumulative distribution function \\(F\\), where \\(F(x) = \\text{P}(X \\leq x).\\)\nWe can define the maximum of this set of random variables as \\(M_n = \\max{\\{ X_1, \\ldots, X_n\\}}\\).\nThen we can show that \\(\\text{P}(M_n \\leq x) = P(X_1 \\leq x, \\ldots X_n \\leq x) = \\{F(x)\\}^n = F^n(x)\\)\nThe block maxima approach focuses on understanding and estimating this function \\(F(x)\\)"
  },
  {
    "objectID": "slides/slides_6.html#extreme-value-theory-1",
    "href": "slides/slides_6.html#extreme-value-theory-1",
    "title": "Modelling Environmental Extremes",
    "section": "Extreme Value Theory",
    "text": "Extreme Value Theory\n\n\nThe true cumulative distribution function, \\(F(x)\\), is unknown.\nWe could replace it with an estimate, but:\n\nAny small differences between the estimate and the truth for \\(F(x)\\) could lead to large differences in \\(F^n(x)\\).\nAs \\(n \\to \\infty\\) then the value of \\(F^n(x)\\) will be 0 or 1 depending on whether \\(F(x) &lt;1\\) or \\(F(x)=1\\). This means that the limit distribution of the maxima is a degenerate distribution.\n\nInstead, we tend to focus on the limiting distribution of \\(F^n(x)\\) as \\(n \\to \\infty\\), .i.e., the limiting distribution of the normalized maximum by introducing some constants to stabilize the maximum as \\(n\\) grows.\nThis is a distribution \\(G(x)\\) such that, for constants \\(a_n &gt; 0\\) and \\(b_n\\),\n\\[\\text{P}\\left(\\frac{M_n - b_n}{a_n} \\leq x\\right) = F^n(a_n x + b_n) \\to G(x) \\text{ as } n \\to \\infty\\]\n\\(G\\) belongs to the type of one of three distribution functions."
  },
  {
    "objectID": "slides/slides_6.html#extreme-value-distributions",
    "href": "slides/slides_6.html#extreme-value-distributions",
    "title": "Modelling Environmental Extremes",
    "section": "Extreme Value Distributions",
    "text": "Extreme Value Distributions\nThere are three main families of extreme value distribution which have our desired properties as limiting distributions of \\(F^n(x)\\).\nGumbel: \\(G(x) = \\exp \\left( -\\exp [\\frac{-(x-a_n)}{b_n} ]\\right)\\)\nFréchet: \\(G(x) = \\exp \\left( -[\\frac{(x-a_n)}{b_n} ]^{-\\xi}\\right)\\) for \\(z&gt;b_n\\), 0 otherwise.\nWeibull: \\(G(x) = \\exp \\left( -[\\frac{-(x-a_n)}{b_n} ]^{\\xi}\\right)\\) for \\(z&gt;b_n\\), 1 otherwise.\nHere, \\(a_n\\) is a location parameter, \\(b_n\\) is a scale parameter and \\(\\xi\\) is a shape parameter."
  },
  {
    "objectID": "slides/slides_6.html#block-maxima-1",
    "href": "slides/slides_6.html#block-maxima-1",
    "title": "Modelling Environmental Extremes",
    "section": "Block Maxima",
    "text": "Block Maxima\n\nA lot of investigation of environmental extremes will focus on time series data.\nWithin time series data, we typically have natural groupings or blocks of observations (days, months, years etc).\nTherefore a common approach for modelling extremes focuses on the idea of block maxima - identifying the maximum (or minimum) value in each block.\nFor example, if we have daily temperature data measured over 100 years, we could look at the highest temperature in each year."
  },
  {
    "objectID": "slides/slides_6.html#block-maxima-approach",
    "href": "slides/slides_6.html#block-maxima-approach",
    "title": "Modelling Environmental Extremes",
    "section": "Block Maxima Approach",
    "text": "Block Maxima Approach\n\n\nBreak up our sequence \\(X_1,X_2,\\ldots\\) into blocks of size \\(n\\) (with \\(n\\) reasonably large), and extract only the maximum observation from each block.\nNow we fit \\(G(x)\\) to the sequence of extracted maxima \\(M_{(1)},M_{(2)},\\ldots , M_{(N)}\\) and use this as the basis for statistical inference.\nFor example, consider the annual maxima of daily rainfall. Here our blocks have \\(n=365\\) observations, which is reasonably large, so we fit our model to \\(N\\) annual maxima (where \\(N\\) is the number of years).\nThis rough and ready approach has shown itself to be surprisingly robust."
  },
  {
    "objectID": "slides/slides_6.html#the-gev-distribution",
    "href": "slides/slides_6.html#the-gev-distribution",
    "title": "Modelling Environmental Extremes",
    "section": "The GEV Distribution",
    "text": "The GEV Distribution\n\nMore generally, we can model the maxima using the Generalised Extreme Value (GEV) distribution: \\[G(x) = \\exp \\left( - \\left[ 1 + \\frac{\\xi (x - \\mu)}{\\sigma} \\right]^{-\\frac{1}{\\xi}} \\right)\\]\nHere, \\(\\mu\\) is the location parameter, \\(\\sigma\\) is the scale parameter and \\(\\xi\\) is the shape parameter.\nThe Gumbel, Fréchet and Weibull distributions are all special cases depending on the value of \\(\\xi\\)."
  },
  {
    "objectID": "slides/slides_6.html#the-gev-distribution-1",
    "href": "slides/slides_6.html#the-gev-distribution-1",
    "title": "Modelling Environmental Extremes",
    "section": "The GEV Distribution",
    "text": "The GEV Distribution\n\n\n\\[G(x) = \\exp \\left( - \\left[ 1 + \\frac{\\xi (x - \\mu)}{\\sigma} \\right]^{-\\frac{1}{\\xi}} \\right)\\]\n\nIf \\(\\xi &lt; 0\\) then we have the Weibull distribution.\nIf \\(\\xi &gt; 0\\) then we have the Fréchet distribution.\nAs \\(\\xi \\to 0\\) then we have the Gumbel distribution."
  },
  {
    "objectID": "slides/slides_6.html#example",
    "href": "slides/slides_6.html#example",
    "title": "Modelling Environmental Extremes",
    "section": "Example",
    "text": "Example\nIn this course we will focus on the statistical inference of real data for which the underlying distribution \\(F(\\cdot)\\) is unknown.\nThus, the following example just illustrates how the choice of normalizing constants lead to a limit distribution within the GEV family.\n\n\n\nExample\n\n\nLet \\(X_1,\\ldots,X_n \\overset{iid}{\\sim} F\\), where \\(F\\) is Uniform(0,1). If we set the normalizing constants as:\n\\[\na_n = \\frac{1}{n} ~~~~~b_n=1,\n\\]\nthe limiting distribution of the maxima \\(M_n\\) as \\(n\\to \\infty\\) is a GEV with \\(\\mu= -1\\) ,\\(\\sigma = 1\\) and \\(\\xi = -1\\)"
  },
  {
    "objectID": "slides/slides_6.html#example-solution",
    "href": "slides/slides_6.html#example-solution",
    "title": "Modelling Environmental Extremes",
    "section": "Example: Solution",
    "text": "Example: Solution\nRecall that if \\(X_1,X_2,\\ldots\\) are a sequence of independent uniform U(0, 1) then, \\(F(x) = x ~(\\text{for } 0 \\leq x \\leq 1)\\)\n\\[\n\\begin{aligned}\n\\mathbb{Pr}\\left(\\dfrac{M_n -b_n}{a_n}\\leq x \\right) & = \\mathbb{Pr}(M_n \\leq b_n +a_n x)\\\\\n&= F\\left(b_n +a_n x\\right)^n\\\\\n& \\class{fragment}{= F\\left(1 + \\frac{1}{n}x\\right)^n}\\\\\n& \\class{fragment}{= \\left(1 + \\frac{1}{n}x\\right)^n ~~~\\text{for } 0 \\leq 1 + x/n   \\leq 1}\\\\\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/slides_6.html#example-solution-1",
    "href": "slides/slides_6.html#example-solution-1",
    "title": "Modelling Environmental Extremes",
    "section": "Example: Solution",
    "text": "Example: Solution\nRecall that if \\(X_1,X_2,\\ldots\\) are a sequence of independent uniform U(0, 1) then, \\(F(x) = x ~(\\text{for } 0 \\leq x \\leq 1)\\)\n\\[\n\\begin{aligned}\n\\mathbb{Pr}\\left(\\dfrac{M_n -b_n}{a_n}\\leq x \\right) & = \\mathbb{Pr}(M_n \\leq b_n +a_n x)\\\\\n&= F\\left(b_n +a_n x\\right)^n\\\\\n&= F\\left(1 + \\frac{1}{n}x\\right)^n\\\\\n&= \\left(1 + \\frac{1}{n}x\\right)^n ~~~\\text{for } 0 \\leq 1 + x/n   \\leq 1\\\\\n& \\class{fragment}{ \\rightarrow \\exp x~ \\text{as }  n \\to \\infty}\n\\end{aligned}\n\\]\n\nUsing the fact that\n\\[\n\\lim_{n \\to \\infty}\\left( 1 + \\frac{1}{n} x\\right)^n  = \\exp{x}\n\\]"
  },
  {
    "objectID": "slides/slides_6.html#example-solution-2",
    "href": "slides/slides_6.html#example-solution-2",
    "title": "Modelling Environmental Extremes",
    "section": "Example: Solution",
    "text": "Example: Solution\nRecall that if \\(X_1,X_2,\\ldots\\) are a sequence of independent uniform U(0, 1) then, \\(F(x) = x ~(\\text{for } 0 \\leq x \\leq 1)\\)\n\\[\n\\begin{aligned}\n\\mathbb{Pr}\\left(\\dfrac{M_n -b_n}{a_n}\\leq x \\right) & = \\mathbb{Pr}(M_n \\leq b_n +a_n x)\\\\\n&= F\\left(b_n +a_n x\\right)^n\\\\\n&= F\\left(1 + \\frac{1}{n}x\\right)^n\\\\\n&= \\left(1 + \\frac{1}{n}x\\right)^n ~~~\\text{for } 0 \\leq 1 + x/n   \\leq 1\\\\\n&  \\rightarrow \\exp x~ \\text{as }  n \\to \\infty\n\\end{aligned}\n\\]\n\nLet \\(G(x) = \\exp \\left\\{- (1 + \\xi \\frac{x -\\mu}{\\sigma})^{-1/xi}\\right\\}\\), setting \\(\\mu= -1\\) ,\\(\\sigma = 1\\) and \\(\\xi = -1\\) leads to:\n\n\n\\[\nG(x) =  \\exp \\left\\{- (1 -x + 1 )\\right\\} = \\exp x\n\\]"
  },
  {
    "objectID": "slides/slides_6.html#generating-samples-from-block-maxima-in-r",
    "href": "slides/slides_6.html#generating-samples-from-block-maxima-in-r",
    "title": "Modelling Environmental Extremes",
    "section": "Generating samples from Block Maxima in R",
    "text": "Generating samples from Block Maxima in R\n\n\n\ngenerate.max &lt;- function(N = 50,n=5,\n                         rdist,param=c(0,1),\n                         a=1,b=0,seed=pi){\n  set.seed(seed)\n  Mn &lt;- c()\n  for(i in 1:N){\n    sample &lt;- rdist(n,param[1],param[2])\n    Mn[i]&lt;- (max(sample)-b)/a\n  }\n  hist(Mn,breaks = \"FD\",\n       col=\"lightblue\",proba=T,\n       main=\"Histogram of maximums\",xlab='')\n}\n\ngenerate.max(N=50000,n=5,rdist=runif,\n             param=c(0,1),a=1/5,b=1)\ncurve(ExtremalDep::dGEV(x,-1,1,-1),type=\"l\",\n      n=500,add=T,lwd=3,lty=1,col=2)"
  },
  {
    "objectID": "slides/slides_6.html#generating-samples-from-block-maxima-in-r-1",
    "href": "slides/slides_6.html#generating-samples-from-block-maxima-in-r-1",
    "title": "Modelling Environmental Extremes",
    "section": "Generating samples from Block Maxima in R",
    "text": "Generating samples from Block Maxima in R\n\n\n\n\n\n\n\nNote\n\n\nFor \\(X_1,\\ldots,X_n \\overset{iid}{\\sim} F\\), where \\(F\\) is Uniform(0,1), if we normalise the maximum using \\(b_n=1\\) and \\(a_n=1/n\\), we know that the asymptotic distribution of \\(\\dfrac{M_n -b_n}{a_n}\\) is a GEV(\\(\\mu\\)=−1,\\(\\sigma\\)=1,\\(\\xi\\)=−1).\n\n\n\n\n\n\nNote\n\n\nLikewise, if \\(X_1, X_2 , \\ldots\\) is a sequence of independent standard exponential \\(Exp(1)\\) variables, using \\(b_n=1\\) and \\(a_n=\\log n\\) the asymptotic distribution of the normalised maxima is a GEV(\\(\\mu=0,\\sigma=1,\\xi=0\\)) i.e., a Gumbell distribution.\n\n\n\n\nLet’s see what this looks like, for a few different values of n"
  },
  {
    "objectID": "slides/slides_6.html#generating-samples-from-block-maxima-in-r-2",
    "href": "slides/slides_6.html#generating-samples-from-block-maxima-in-r-2",
    "title": "Modelling Environmental Extremes",
    "section": "Generating samples from Block Maxima in R",
    "text": "Generating samples from Block Maxima in R\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 500\n\nlibrary(shiny)\n\n# Function to generate block maxima\ngenerate_max &lt;- function(N, n, rdist, param, a, b, seed=pi){\n  set.seed(seed)\n  Mn &lt;- numeric(N)\n  for(i in 1:N){\n    sample &lt;- rdist(n, param[1], param[2])\n    Mn[i] &lt;- (max(sample) - b) / a\n  }\n  return(Mn)\n}\n\n# UI\niu &lt;- fluidPage(\n  titlePanel(\"Block Maxima Simulation\"),\n  sidebarLayout(\n    sidebarPanel(\n      selectInput(\"dist\", \"Choose Distribution:\", choices = c(\"Uniform(0,1)\" = \"uniform\", \"Exponential(1)\" = \"exponential\")),\n      sliderInput(\"n\", \"Observations per Block (n):\", min = 5, max = 100, value = 5, step = 5),\n      sliderInput(\"N\", \"Number of Blocks (N):\", min = 10000, max = 50000, value = 50000, step = 10000)\n    ),\n    mainPanel(\n      plotOutput(\"histogram\")\n    )\n  )\n)\n\n# Server\nserver &lt;- function(input, output) {\n  output$histogram &lt;- renderPlot({\n    if (input$dist == \"uniform\") {\n      a &lt;- 1 / input$n\n      b &lt;- 1\n      Mn &lt;- generate_max(input$N, input$n, runif, c(0,1), a, b)\n      true_gev &lt;- function(x) evd::dgev(x, loc = -1, scale = 1, shape = -1)\n    } else {\n      a &lt;- 1\n      b &lt;- log(input$n)\n      Mn &lt;- generate_max(input$N, input$n, rgamma, c(1,1), a, b)\n      true_gev &lt;- function(x) evd::dgev(x, loc = 0, scale = 1, shape = 0)\n    }\n    \n    hist(Mn, breaks = \"FD\", col = \"lightblue\", prob = TRUE, main = \"Histogram of Block Maxima\", xlab = \"Maxima\")\n    curve(true_gev, col = \"red\", lwd = 2, add = TRUE)\n  })\n}\n\n# Run App\nshinyApp(ui = iu, server = server)"
  },
  {
    "objectID": "slides/slides_6.html#maxima-and-minima",
    "href": "slides/slides_6.html#maxima-and-minima",
    "title": "Modelling Environmental Extremes",
    "section": "Maxima and Minima",
    "text": "Maxima and Minima\n\nCommunication of extremes typically focuses on maxima (or minima).\nEnvironmental or climate events are often described as the “worst/highest/lowest in X years”."
  },
  {
    "objectID": "slides/slides_6.html#return-levels",
    "href": "slides/slides_6.html#return-levels",
    "title": "Modelling Environmental Extremes",
    "section": "Return Levels",
    "text": "Return Levels\n\n\nIn statistics, this idea of the “highest in X years” can be related to the idea of a return level and return period.\nThe return level \\(z_p\\) is the value we would expect to be exceeded once every \\(p\\) years, where \\(\\frac{1}{p}\\) is the return period.\nThe return level can also be thought of as the value which has probability \\(\\frac{1}{p}\\) of being exceeded in a given year.\nNow consider the statement: “The temperature in Glasgow will reach 20 degrees once every 50 years”.\nHere, we have return period \\(\\frac{1}{50}=0.02\\) and return level \\(z_p=20\\)."
  },
  {
    "objectID": "slides/slides_6.html#return-levels-and-the-gev",
    "href": "slides/slides_6.html#return-levels-and-the-gev",
    "title": "Modelling Environmental Extremes",
    "section": "Return Levels and the GEV",
    "text": "Return Levels and the GEV\n\nThe return level \\(z_p\\) is the \\((1-\\frac{1}{p})\\) quantile of the GEV distribution, since we have a probability \\(\\frac{1}{p}\\) of the maximum exceeding that value, i.e., \\(P(M_n &gt; z_p) = \\frac{1}{p}\\).\nRecall that the GEV takes the form \\[\n\\begin{aligned}\nG(z_p) &= \\begin{cases}\n  \\exp \\left( - \\left[ 1 + \\frac{\\xi (z_p - \\mu)}{\\sigma} \\right]^{-\\frac{1}{\\xi}} \\right) &, \\xi \\neq 0 \\\\\n  \\exp\\left(-\\exp\\left(-\\frac{z_p-\\mu}{\\sigma}\\right)\\right) &, \\xi = 0\n\\end{cases}\\\\\nG(z_p) &= P(M_n \\leq z_p) = 1 - P(M_n &gt; z_p)\n\\end{aligned}\n\\]\nTherefore the return level can be obtained by inverting this distribution to obtain \\[\nz_p = \\begin{cases}\n  \\mu - \\frac{\\sigma}{\\xi}\\left[ 1 - \\{ -\\log(1-\\frac{1}{p}) \\}^\\xi \\right] & \\xi \\neq 0\\\\\n  \\mu - \\sigma \\log\\{ -\\log(1-\\frac{1}{p})\\} & \\xi = 0\n\\end{cases}\n\\]"
  },
  {
    "objectID": "slides/slides_6.html#example---venice-sea-levels",
    "href": "slides/slides_6.html#example---venice-sea-levels",
    "title": "Modelling Environmental Extremes",
    "section": "Example - Venice Sea Levels",
    "text": "Example - Venice Sea Levels\n\n\n\nSea levels in Venice are rising.\nThe city floods on a regular basis.\nWhat sea level can we expect in the next 5, 10, 100 years?"
  },
  {
    "objectID": "slides/slides_6.html#example---venice-sea-levels-1",
    "href": "slides/slides_6.html#example---venice-sea-levels-1",
    "title": "Modelling Environmental Extremes",
    "section": "Example - Venice Sea Levels",
    "text": "Example - Venice Sea Levels\n\nWe have daily sea level measurements from 1931-1981.\nThe plot below shows the 10 highest sea level measurements from each year."
  },
  {
    "objectID": "slides/slides_6.html#example---venice-sea-levels-2",
    "href": "slides/slides_6.html#example---venice-sea-levels-2",
    "title": "Modelling Environmental Extremes",
    "section": "Example - Venice Sea Levels",
    "text": "Example - Venice Sea Levels\n\nWe can apply a block maxima approach, treating each year as a block.\nThis requires us to identify and model the yearly maximum."
  },
  {
    "objectID": "slides/slides_6.html#fitting-the-gev-in-r",
    "href": "slides/slides_6.html#fitting-the-gev-in-r",
    "title": "Modelling Environmental Extremes",
    "section": "Fitting the GEV in R",
    "text": "Fitting the GEV in R\n\nWe use the extRemes package to fit a GEV distribution in R\nThe fevd() simply takes a data vector of the annual maxima and provides parameter estimates using maximum likelihood estimation.\n\n\nlibrary(extRemes)\nMn &lt;- apply(venice[,-1],1,function(x)max(x,na.rm = T))\nfit_gev &lt;- fevd(Mn,method=\"MLE\")\nfit_gev$results$par\n\n    location        scale        shape \n111.09726863  17.17910050  -0.07678211 \n\n\n\nFrom the output we can see that \\(\\mu = 111.1\\), \\(\\sigma = 17.2\\) and \\(\\xi = -0.077\\)."
  },
  {
    "objectID": "slides/slides_6.html#return-level-plot",
    "href": "slides/slides_6.html#return-level-plot",
    "title": "Modelling Environmental Extremes",
    "section": "Return Level Plot",
    "text": "Return Level Plot\n\nWe can assess the suitability of the GEV distribution using a return level plot.\nThis involves commuting the return level at various return periods, and comparing it to the theoretical result under the GEV.\n\n\n\n\n\nThe observed points lie along the theoretical line.\nOur proposed GEV distribution is appropriate.\nThe Venice maxima follow a Gumbel distribution.\n\n\n\n\nplot(fit_gev, type = \"rl\")"
  },
  {
    "objectID": "slides/slides_6.html#extending-block-maxima---k-largest",
    "href": "slides/slides_6.html#extending-block-maxima---k-largest",
    "title": "Modelling Environmental Extremes",
    "section": "Extending Block Maxima - \\(k\\)-largest",
    "text": "Extending Block Maxima - \\(k\\)-largest\n\nLooking at just the maxima means we throw away a lot of data, making it harder to accurately estimate parameters.\nWe could instead follow an approach which models the \\(k\\) largest values within a block."
  },
  {
    "objectID": "slides/slides_6.html#choice-of-block-size",
    "href": "slides/slides_6.html#choice-of-block-size",
    "title": "Modelling Environmental Extremes",
    "section": "Choice of Block Size",
    "text": "Choice of Block Size\n\n\nWe have to make difficult subjective choices when fitting a block maxima model.\nWhat do we choose as our block? Week? Month? Year? Decade?\nBigger blocks means we have fewer data points, but smaller blocks mean our ‘extremes’ might not be extreme at all, violating the assumptions of the GEV distribution.\nIf we use a \\(k\\)-largest approach, we have a similar decision on what value of \\(k\\) to choose."
  },
  {
    "objectID": "slides/slides_6.html#drawbacks-of-block-maxima",
    "href": "slides/slides_6.html#drawbacks-of-block-maxima",
    "title": "Modelling Environmental Extremes",
    "section": "Drawbacks of Block Maxima",
    "text": "Drawbacks of Block Maxima\n\nBlock maxima methods work well in many situations, and take advantage of natural blocks in the data.\nHowever, it does not work well if there is a lot of variability between blocks.\nIn that scenario, some blocks may have many more large counts than others, and much of the data will be discarded.\nWe can overcome this by using a threshold approach, which models all observations which exceed some pre-specified value."
  },
  {
    "objectID": "slides/slides_6.html#peak-over-threshold-1",
    "href": "slides/slides_6.html#peak-over-threshold-1",
    "title": "Modelling Environmental Extremes",
    "section": "Peak Over Threshold",
    "text": "Peak Over Threshold\n\nThis approach is known as peak over threshold (POT) modelling.\nAgain, we assume we have data represented by a time series, and some threshold \\(u\\).\nWe need a statistical model for the values which are above \\(u\\), known as exceedances.\nSometimes we may also wish to model the number of exceedances."
  },
  {
    "objectID": "slides/slides_6.html#peak-over-threshold-2",
    "href": "slides/slides_6.html#peak-over-threshold-2",
    "title": "Modelling Environmental Extremes",
    "section": "Peak Over Threshold",
    "text": "Peak Over Threshold\n\nAgain, let \\(X_1, \\ldots, X_n\\) be a sequence of independent random variables with a common distribution function \\(F\\).\nWe can consider our extreme values in terms of their threshold excess (how much they exceed the threshold by).\nFor an extreme value \\(X &gt; u\\), its threshold excess is given as \\(y = X - u\\)."
  },
  {
    "objectID": "slides/slides_6.html#peak-over-threshold-3",
    "href": "slides/slides_6.html#peak-over-threshold-3",
    "title": "Modelling Environmental Extremes",
    "section": "Peak Over Threshold",
    "text": "Peak Over Threshold\nThe probability of threshold excess of size \\(y\\) is given by\n\\[\n\\begin{aligned}\nP(X &gt; u+y~|~X &gt; u) &= \\frac{P(X&gt;u+y)}{P(X&gt;u)} \\quad\\text{where } y &gt; 0\\\\\n&=  \\frac{1 - F(u + y)}{1 - F(u)}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/slides_6.html#generalised-pareto",
    "href": "slides/slides_6.html#generalised-pareto",
    "title": "Modelling Environmental Extremes",
    "section": "Generalised Pareto",
    "text": "Generalised Pareto\n\nThe function \\(F\\) is still unknown, but the distribution of all threshold excesses can be approximated by a Generalised Pareto distribution (GPD).\nThe cdf of the Generalised Pareto distribution is given by \\[\nP(X \\leq y~|~X&gt;u) = G(y) =\n\\begin{cases}\n1 - \\left( 1 + \\frac{\\xi (y-u)}{\\sigma} \\right)^{-\\frac{1}{\\xi}} & \\xi \\neq 0\\\\\n1 - \\exp \\left( - \\frac{y-u}{\\sigma} \\right) & \\xi = 0\n\\end{cases}\n\\]\nAgain, \\(u\\) is the location parameter, \\(\\sigma\\) is the scale parameter and \\(\\xi\\) is the shape parameter."
  },
  {
    "objectID": "slides/slides_6.html#return-levels-for-pot-models",
    "href": "slides/slides_6.html#return-levels-for-pot-models",
    "title": "Modelling Environmental Extremes",
    "section": "Return Levels for POT Models",
    "text": "Return Levels for POT Models\n\nWe can define a return level for POT models in a roughly similar way to block maxima models.\nThe \\(m\\)-observation return level, \\(x_m\\) is defined as the level expected to be exceeded once every \\(m\\) observations, with \\[\nx_m =\n\\begin{cases}\nu + \\frac{\\sigma}{\\xi} \\left[ \\left(m P(X&gt;u)\\right)^\\xi - 1 \\right] & \\xi \\neq 0\\\\\nu + \\sigma \\log\\left(m P(X&gt;u)\\right) & \\xi = 0\n\\end{cases}\n\\]\nFor any given observation, the probability of exceeding \\(x_m\\) is simply \\(\\frac{1}{m}\\)."
  },
  {
    "objectID": "slides/slides_6.html#derivation-of-the-return-level-when-xi0",
    "href": "slides/slides_6.html#derivation-of-the-return-level-when-xi0",
    "title": "Modelling Environmental Extremes",
    "section": "Derivation of the return level when \\(\\xi=0\\)",
    "text": "Derivation of the return level when \\(\\xi=0\\)\n\n\nRecall that\n\nthe probability of exceeding \\(x_m\\) is given by\n\n\\[\n\\begin{aligned}\n\\mathbb{Pr}(x &gt; x_m) &= \\mathbb{Pr}(x &gt; x_m|x &gt; u)\\mathbb{Pr}(x&gt;u) \\\\\n&= 1/m\n\\end{aligned}\n\\]\n\nthe GEV with \\(\\xi = 0\\) is\n\n\\[\\underbrace{G(x_m)}_{\\mathbb{Pr}(x\\leq x_m | x &gt;u)} = 1- \\underbrace{\\exp \\left(-\\dfrac{x_m -u}{\\sigma}\\right)}_{\\mathbb{Pr}(x&gt;x_m|x&gt;u)}\\]\n\n\n\n\n\\[\n\\begin{aligned}\n\\mathbb{Pr}(x &gt; x_m) &= \\mathbb{Pr}(x &gt; x_m|x &gt; u)\\mathbb{Pr}(x&gt;u)\\\\\n& \\class{fragment}{ = \\exp \\left(-\\dfrac{x_m -u}{\\sigma}\\right) \\mathbb{Pr}(x&gt;u) = \\frac{1}{m}}\\\\\n& \\class{fragment}{ \\Rightarrow ~ \\exp \\left(-\\dfrac{x_m -u}{\\sigma}\\right) = \\dfrac{1}{m\\mathbb{Pr}(x&gt;u)}}\\\\\n& \\class{fragment}{-\\dfrac{x_m -u}{\\sigma}= \\log (1) -\\log (m\\mathbb{Pr}(x&gt;u))} \\\\\n  &~~~~~~~~~~~~~~~~~~~\\class{fragment}{\\vdots} \\\\\n&~~~~~~~~~~~~ \\class{fragment}{x_m =  u + \\sigma \\log (m\\mathbb{Pr}(x&gt;u))}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/slides_6.html#choosing-a-threshold",
    "href": "slides/slides_6.html#choosing-a-threshold",
    "title": "Modelling Environmental Extremes",
    "section": "Choosing a Threshold",
    "text": "Choosing a Threshold\n\nWe need a threshold low enough that we have sufficient data, but high enough that values above it are genuinely extreme."
  },
  {
    "objectID": "slides/slides_6.html#choosing-a-threshold-1",
    "href": "slides/slides_6.html#choosing-a-threshold-1",
    "title": "Modelling Environmental Extremes",
    "section": "Choosing a Threshold",
    "text": "Choosing a Threshold\n\n\nOccasionally there is a natural choice of threshold (e.g., a legal limit for a pollutant), but generally we need to choose it.\nOne approach is to use a mean residual life plot, which plots the sample mean excess (mean of \\(x&gt;u\\)) at a variety of thresholds \\(u\\).\nIf the GPD is appropriate, the mean excess should be linearly related to the threshold.\nTherefore, we can identify a suitable threshold as one which lies within an area of linearity on this plot."
  },
  {
    "objectID": "slides/slides_6.html#example---fort-collins-colorado",
    "href": "slides/slides_6.html#example---fort-collins-colorado",
    "title": "Modelling Environmental Extremes",
    "section": "Example - Fort Collins, Colorado",
    "text": "Example - Fort Collins, Colorado\n\nWe have daily precipitation data from 1900-1999, obtained from a rain gauge in Fort Collins, Colorado, taken from Katz et al, 2002."
  },
  {
    "objectID": "slides/slides_6.html#example---fort-collins-colorado-1",
    "href": "slides/slides_6.html#example---fort-collins-colorado-1",
    "title": "Modelling Environmental Extremes",
    "section": "Example - Fort Collins, Colorado",
    "text": "Example - Fort Collins, Colorado\nWe compare three different choices of threshold below (\\(u = 0.5, 1.0, 1.5\\)) to show the importance of getting the choice right.\n\n\n\n\n\n\n\n\\(u\\)\n\\(\\% &gt; u\\)\n\\(n &gt; u\\)\n\n\n\n\n0.5\n2.08\n759\n\n\n1.0\n0.58\n213\n\n\n1.5\n0.25\n91"
  },
  {
    "objectID": "slides/slides_6.html#mean-residual-life-plot",
    "href": "slides/slides_6.html#mean-residual-life-plot",
    "title": "Modelling Environmental Extremes",
    "section": "Mean Residual Life Plot",
    "text": "Mean Residual Life Plot\n\nWe can fit a mean residual life plot to identify a sensible choice of threshold.\nIt appears that a value of \\(u\\) somewhere between 1.2 and 1.8 would be an appropriate choice here - this is where the plot appears to be linear."
  },
  {
    "objectID": "slides/slides_6.html#sensitivity-analysis",
    "href": "slides/slides_6.html#sensitivity-analysis",
    "title": "Modelling Environmental Extremes",
    "section": "Sensitivity Analysis",
    "text": "Sensitivity Analysis\n\nWe can also carry out a sensitivity analysis to see the effect of choosing different threshold values on the estimated model parameters.\nThe plot below shows the parameter estimates at different thresholds - they seem fairly robust."
  },
  {
    "objectID": "slides/slides_6.html#pot-models-in-r",
    "href": "slides/slides_6.html#pot-models-in-r",
    "title": "Modelling Environmental Extremes",
    "section": "POT Models in R",
    "text": "POT Models in R\n\nWe can use the extRemes package to fit a Generalised Pareto distribution in R.\nThe function fevd allows several extreme value distributions (including GEV and GPD) to be fitted, and can also provide return levels.\n\n\nfitGP &lt;- fevd(Fort, threshold=1.5, type=\"GP\",\n                    time.units=\"365/year\")\n\nreturn.level(fitGP, c(10,20,100), type=\"GP\")\n\n[1] \"Return Levels for period units in years\"\n 10-year level  20-year level 100-year level\n      2.857184       3.340219       4.581339"
  },
  {
    "objectID": "slides/slides_6.html#correlation",
    "href": "slides/slides_6.html#correlation",
    "title": "Modelling Environmental Extremes",
    "section": "Correlation",
    "text": "Correlation\n\n\nThreshold exceedances are not always independent due to temporal correlation.\nIf we have high temperatures today, it’s likely we might also have high temperatures tomorrow.\nWe have to account for this dependence within our model, for example by using the autoregressive approaches outlined in the time series section.\nAlternatively, we could use a ‘declustering’ approach which identifies these temporal clusters and simply uses the cluster maxima."
  },
  {
    "objectID": "slides/slides_6.html#summary",
    "href": "slides/slides_6.html#summary",
    "title": "Modelling Environmental Extremes",
    "section": "Summary",
    "text": "Summary\n\n\nEstimating extremes is challenging and the results can be unreliable even with large datasets.\nWe have identified two main approaches for dealing with these data - block maxima and point over threshold.\nThe block maxima approach uses the Generalised Extreme Value (GEV) distribution to model the highest value(s) within a specific block of time.\nThe point over threshold approach uses the Generalised Pareto distribution (GPD) to model all observations which exceed a certain value."
  },
  {
    "objectID": "notes/notes_2.html",
    "href": "notes/notes_2.html",
    "title": "Temporal Correlation and Changepoints",
    "section": "",
    "text": "We begin this week by finishing our notes on additive models in R, and then move onto temporal correlation and changepoints."
  },
  {
    "objectID": "notes/notes_2.html#fitting-additive-models-in-r",
    "href": "notes/notes_2.html#fitting-additive-models-in-r",
    "title": "Temporal Correlation and Changepoints",
    "section": "\n1.1 Fitting Additive Models in R",
    "text": "1.1 Fitting Additive Models in R\nWe can fit GAMs in R using the package mgcv, which was designed to allow extensions of generalised linear models (GLMs). The generalised aspect means that we can also extend the standard additive model to situations where we have non-normal responses, but we will not focus on these in this course.\nWe use the function gam() to fit our model. This works in a very similar manner to the lm() function. The smooth functions are represented by s(). These use the penalised splines approach described last week. Any linear terms can be included additively as normal.\nThe model will take the form below, where you can include as many smooth or linear terms as you wish.\n\nCodelibrary(mgcv)\n\nmod &lt;- gam(response ~ s(smooth1) + s(smooth2) + linear)\n\n\n\n\n\n\n\n\n Example: River Tweed nitrate level\n\n\n\nThe nitrate levels in the River Tweed were measured monthly between 1997 and 2007. The red line is a simple LOWESS curve.\n\n\nLog(Nitrate) by time for River Tweed. Black points are data. Red line is LOWESS curve.\n\n\nCodem1 &lt;- gam(log_nitrate ~ s(Date))\n\n\nFamily: Gaussian\nLink function: identity\n\nParametric coefficients:\n             Estimate Std. Error  t value    Pr(&gt;|t|)\n(Intercept)   2.04454   0.03965     51.56     &lt;2e-16\n\nApproximate significance of smooth terms:\n          edf   Ref.df     F   p-value\ns(Date) 6.183   7.336   4.37   0.000272\n\nR-sq.(adj) = 0.242 Deviance explained = 29.3%\n\nGCV = 0.15847 Scale est. = 0.14623 n = 93\nWe are mainly interested in the output related to smooth terms.\nApproximate significance of smooth terms:\n          edf   Ref.df     F   p-value\ns(Date) 6.183   7.336   4.37   0.000272\nThe p-value tells us the significance of the term, i.e., whether the smooth term is significantly different from a flat (horizontal) line. (The p-value doesn’t tell us whether the smooth term is different from a linear term!)\nThe effective degrees of freedom (EDF) tells us how nonlinear the relationship is:\n\nHigher EDF means a more nonlinear relationship.\nAn EDF of 1 indicates a linear relationship.\n\nIn our example, the p-value is very small (&lt;&lt;0.05), and therefore we have evidence that this smooth term is necessary in our model.\nThe EDF for this term is 6.183, suggesting that this is far from linear and that a smooth term may be appropriate.\nWe can assess the significance of our smooth term using the anova function. We fit a simple linear regression and compare it to the additive model we have already fitted:\n\nCodem1 &lt;- gam(log_nitrate ~ s(Date))\nm2 &lt;- lm(log_nitrate ~ Date)\n\nanova(m2, m1)\n\n\nAnalysis of Variance Table\nModel 1: log_nitrate ~ Date\nModel 2: log_nitrate ~ s(Date)\n\n    Res.Df      RSS    Df    SS      F   Pr(&gt;F)\n1     91.000   14.883\n2     85.817   12.549 5.183 2.334 3.0794 0.01228\nThe p-value confirms that the smooth term is necessary instead of a linear term. (Note that we don’t really need to test for nonlinearity, since the model should penalise excess wiggliness, effectively fitting a linear term where appropriate.)"
  },
  {
    "objectID": "notes/notes_2.html#visualising-additive-models-in-r",
    "href": "notes/notes_2.html#visualising-additive-models-in-r",
    "title": "Temporal Correlation and Changepoints",
    "section": "\n1.2 Visualising additive models in R",
    "text": "1.2 Visualising additive models in R\nUnlike linear terms, we can’t simply report parameter estimates for smooth terms in additive models. We can instead simply use the plot function to visualise our smooth functions.\n\n\n\n\n\n\n Example: River Tweed nitrate level (continued)\n\n\n\nHere, we observe that we may have a bimodal shape, with peaks in 1999 and 2005:\n\nCodeplot(m1)\n\n\n\n\nPlot of fitted smooth of time for River Tweed log(Nitrate) data. The black points are partial residuals, the black line is the estimated smooth, and the dashed lines are the 95% interval bounds for the estimated smooth.\n\nIn reality, we might spot that there is a seasonal pattern evident in the plot, which our smooth does not capture. We could capture this by either:\n\nfitting a smooth to the months plus a smooth to years (to capture the seasonal pattern plus long term smooth trend), as we saw at the end of last week’s notes, or\nincreasing the basis dimension for our smooth, using the k argument of the s() function. (See the help file of the s function in R for details, by running ?mgcv::s.)"
  },
  {
    "objectID": "notes/notes_2.html#autocorrelation-function",
    "href": "notes/notes_2.html#autocorrelation-function",
    "title": "Temporal Correlation and Changepoints",
    "section": "\n2.1 Autocorrelation function",
    "text": "2.1 Autocorrelation function\nWe can estimate the strength of temporal dependence using a sample autocorrelation function (ACF). This function represents the autocorrelation of the data at a series of different lags in time. Assuming we have a regularly spaced time series, we compute the sample ACF at lag \\(k\\) as:\n\\[r(k) = \\frac{\\sum_{t=k+1}^n(x_t - \\bar{x})(x_{t-k} - \\bar{x})}{\\sum_{t=1}^n(x_t - \\bar{x})^2}\\]\nWe compute this for values from \\(k=1,\\ldots,K\\) where \\(K\\) is some sensible maximum lag.\nOur sample ACF is an estimate of the overall ACF, and as such we have to consider uncertainty. Typically we will compute a simple confidence interval around our point estimate at each lag as\n\\[r(k) \\pm 1.96\\sqrt{\\frac{1}{n}}\\]\nwhere \\(n\\) is the number of observations in the time series.\nWe plot the ACF function with a separate vertical line representing the size correlation at each lag, and dashed lines for the confidence intervals. If the lines lie within the confidence intervals, no autocorrelation is present.\nHere, we have several lines outside the confidence intervals, so autocorrelation exists in this dataset.\n\n\n\n\nEach of the three ACFs below are examples of cases where suggest autocorrelation is present.\n\nThe first has a repeating pattern — suggests seasonality:\n\n\n\n\n\n\nThe second has a decreasing pattern — likely caused by trend:\n\n\n\n\n\n\nThe third has both patterns — probably seasonality AND trend:\n\n\n\n\n\nIf we have identified autocorrelation in our data, we have to find a way to account for it in our model. In some cases we may choose to simply treat it as a nuisance, and make adjustments to our standard errors to reflect the reduced effective sample size.\nThe alternative is to explicitly account for the autocorrelation in our model. For seasonal patterns, we may be able to eliminate it using methods discussed previously, such as harmonics. For other types of autocorrelation, we may use approaches such as autoregressive integrated moving average (ARIMA)."
  },
  {
    "objectID": "notes/notes_2.html#autocorrelation-models",
    "href": "notes/notes_2.html#autocorrelation-models",
    "title": "Temporal Correlation and Changepoints",
    "section": "\n2.2 Autocorrelation models",
    "text": "2.2 Autocorrelation models\nAutoregressive integrated moving average (ARIMA) models are a general class of models which account for autocorrelation. These models combine aspects of two main classes of model: autoregressive (AR) and moving average (MA).\nBroadly speaking, AR(\\(p\\)) models assume that the current value is a function of the previous \\(p\\) observations. In contrast, MA(\\(q\\)) models assume that the current value can be computed by a linear regression on the \\(q\\) previous random error terms. These models are covered in more detail in the Time Series course, but will be addressed briefly here.\n\n2.2.1 AR model\nAn autoregressive (AR) model accounts for correlation by describing each value as a function of the previous values. The AR(\\(p\\)) process can be written as\n\\[X_t = \\sum_{i=1}^p \\phi_i X_{t-i} + \\epsilon_t.\\]\nHere \\(\\phi_i\\) is the “autoregressive parameter” which measures the strength of the autocorrelation. \\(\\epsilon_t \\sim \\mbox{N}(0, \\sigma^2)\\) is simply random error, often referred to as noise.\n\n2.2.2 MA model\nA moving average (MA) model accounts for correlation by describing each value as a function of the previous set of error terms. The MA(\\(q\\)) process can be written as\n\\[X_t = \\mu + \\sum_{i=1}^q \\theta_i \\epsilon_{t-i} + \\epsilon_t.\\]\nHere \\(\\mu\\) is the mean of the series and \\(\\theta_i\\) is the regression parameter associated with the \\(i\\)th lag.\n\n2.2.3 ARIMA\nThe ARIMA is a combination of AR and MA processes. The I stands for Integrated, which relates to “differencing”, i.e., replacing a value with the difference between itself and the previous value. We write this model as ARIMA(\\(p,d,q\\)), where \\(p\\) is the order of the AR process, \\(d\\) is the degree of differencing and \\(q\\) is the order of the MA.\nFor example, ARIMA(1,0,0) would be equivalent to an AR(1) model and ARIMA(0,0,1) is an MA(1) model.\nWe can use the sample ACF to suggest the appropriate model to account for our autocorrelation. A smooth decay suggests that we have AR components. A less structured ACF might suggest an MA is more appropriate. In practice, AR processes are less complex than MA and tend to be used more frequently as a result.\n\n\n\n\n\n\n Examples of ACF plots\n\n\n\n\nPlot A has a clear seasonal pattern, meaning that harmonics may be more appropriate than an ARIMA.\nIn Plot B, the value at lag 1 is outside the interval, and thus an AR(1) may be most suitable. (Note that we can likely ignore the spike at lag 12 as just random error).\nPlot C does not appear to have any correlations outside of the error bars, and so we can conclude that there is no evidence of autocorrelation. Note that the bars are wider. This is likely because we had less data available.\n\n\nARIMA methods are all based on regularly spaced data (measurements equally spaced in time). However, in some cases we may have irregularly spaced data. If the data are roughly regular (just a small deviation here and there), we may be able to treat them as though they are regular. If we have missing data, we may be able to impute or interpolate without too many issues. In cases where we have completely irregular data, we may need to use more complex statistical methods (which will not be covered in this course).\n\n2.2.4 ARIMA in R\nWe can use the arima() function in R to explore autocorrelation. We must first fit a linear model, and then extract the design matrix to use as an input to this function. For example, to fit an AR(1) model we would use the following code:\n\nCodetrend.model0 &lt;- lm(response ~ decimal.date)\n\nX &lt;- model.matrix(trend.model0)\n\ntrend.model1 &lt;- arima(y, order = c(1, 0, 0), xreg = X, \n                      include.mean = FALSE)"
  },
  {
    "objectID": "notes/notes_2.html#known-changepoint",
    "href": "notes/notes_2.html#known-changepoint",
    "title": "Temporal Correlation and Changepoints",
    "section": "\n3.1 Known changepoint",
    "text": "3.1 Known changepoint\nSometimes it is known that a change occurred at a specific timepoint, but the magnitude or shape of this change are not known.\n\n3.1.1 Known changepoint — mean shift\nSuppose that we have a series of data \\(Y_i\\) collected at a set of timepoints \\(t_i\\) with \\(i=1,\\ldots, n\\). If our known changepoint is at time \\(c\\), then we can construct an indicator function \\[\\mathcal{I}_{t_i} =\n\\begin{cases}\n0\\hspace{0.5cm} \\text{if } \\hspace{0.1cm} t_i &lt; c\\\\\n1\\hspace{0.5cm} \\text{if } \\hspace{0.1cm} t_i \\geq c\n\\end{cases}\\] This can then be included as a parameter in our regression model \\[Y_i = \\beta_0 + \\varphi\\mathcal{I}_{t_i} + \\epsilon_i\\]\nHere, \\(\\varphi\\), the coefficient of the indicator function ,can be described as the intervention effect. If this parameter is significant in our model, that implies that we have a significant change in mean at timepoint \\(c\\).\n\n3.1.2 Known changepoint — change in slope\nWe also need to consider examples where we observe a change in slope at a known timepoint.\n\n\nPlot of value by time, with data as black dots, and changepoint at time \\(c\\) illustrated by vertical dashed red line.\n\nIt would be possible to fit two separate regressions. However, this seems quite simplistic, and it would be better to have a single continuous model.\n\n\nPlot of value by time, with data as black dots, and changepoint at time \\(c\\) illustrated by vertical dashed red line. Blue regression lines represent the fitted separate regression models.\n\nWe want our regression to be continuous at \\(c\\) such that we have \\[\\alpha_1 + \\beta_1 c =  \\alpha_2 + \\beta_2 c\\] This can be rewritten in terms of a single model parameter, as \\[\\alpha_2 = \\alpha_1 + c(\\beta_1 - \\beta_2)\\] We can thus update our equations to the following, which is known as piecewise regression (or segmented regression):\n\\[\\begin{align*}\nY_i &= \\alpha_1 + \\beta_1 x_i + \\epsilon_i\\hspace{16mm} &\\mbox{ for } x&lt;c\\\\\nY_i &= \\alpha_1 +  (\\beta_1 - \\beta_2)c + \\beta_2 x_i + \\epsilon_i\\hspace{2mm} &\\mbox{ for } x\\geq c\n\\end{align*}\\]\n(Note that this could be expressed as a single model using our indicator function.)\nThe two linear parts of our model now meet at \\(c\\). Note that our piecewise model is more efficient than two separate regressions, since it uses one fewer parameter.\n\n\nPlot of value by time, with data as black dots, and changepoint at time \\(c\\) illustrated by vertical dashed red line. Grey regression lines represent the fitted piecewise regression model.\n\nIn many cases, we may have more complex changes to our trend. There are a variety of more advanced models for known changepoints, but these are all based on the same underlying principles. For example, the bent cable model allows for an extended “transition phase” between the two slopes, often represented by a smooth curve.\n\n\n\n\n\n\n Example: Chlorofluorocarbons (CFCs)\n\n\n\nChlorofluorocarbons (CFCs) are pollutants which were often used in aerosols. Their use was phased out in the 1990s as a result of environmental policy. We can see this “phasing out” period represented in the model.\n\n\nDiagram illustrating the bent cable model as fitted to CFC-11 (ppt) values between 1988 and 2000."
  },
  {
    "objectID": "notes/notes_2.html#unknown-changepoint",
    "href": "notes/notes_2.html#unknown-changepoint",
    "title": "Temporal Correlation and Changepoints",
    "section": "\n3.2 Unknown changepoint",
    "text": "3.2 Unknown changepoint\nIt can be more challenging to fit a changepoint model when you don’t clearly know exactly when the change occurred. One of the most popular methods is an iterative approach which searches across the entire range of our data for possible changepoints.\n\n\n\n\n\n\n Example: River Nile flow data\n\n\n\nWe have historic data on the levels of the River Nile around the city of Aswan, Egypt. Is there any evidence of a change in water volume? If so, when did it occur?\n\n\nRiver Nile.\n\n\n\nPlot of River Nile volume by year, Aswan.\n\nWe can examine the data by fitting a LOWESS curve. There does appear to be a change around 1900. However, we need to explore this further via a model.\n\n\nFitted LOWESS curve for River Nile data.\n\nWe use the segmented() function in R (in the package also called segmented) to fit an unknown changepoint model, using the following steps:\n\nFirst, fit a standard regression using lm().\nWe then pass the linear model into our segmented() function along with an initial estimate of the changepoint.\nThis initial estimate (psi = 1900) is used as a starting point for our iterative algorithm.\n\nWe run this in R:\n\nCodeout.lm &lt;- lm(Volume ~ Year)\nmod &lt;- segmented(out.lm, seg.Z = ~Year, psi = 1900)\n\n\npsi1.x  \n  1913  \n  \nslope(mod)\n$x\n          Est. St.Err. t value\nslope1 -8.1820   1.759  -4.650\nslope2  0.7458   1.084   0.688\nThe final model output suggests that the changepoint occurred in 1913. Prior to 1913, the volume was decreasing by 8.18 units per year. Afterwards, it was increasing by 0.75 units per year.\nThe Aswan Low Dam was constructed between 1899–1902, massively impacting river levels in the area. Therefore it is more sensible to fit a model which introduces a mean shift, rather than a change of slope. Subject matter expertise is key!\n\nIn this case, given there is a clear reason why the time series will change either side of the dam’s construction, we need to fit two separate models. The plot below shows two separate penalised spline models for the before and after periods.\n\n\nFitted LOWESS curves for River Nile data (separate for before/after 1900)."
  },
  {
    "objectID": "notes/tutorial_sheet_3_solutions.html",
    "href": "notes/tutorial_sheet_3_solutions.html",
    "title": "Tutorial Sheet 2",
    "section": "",
    "text": "1 Part A: Seasonal and trend analysis.\n\n\n\n\n\n\nNote\n\n\n\nSeveral of these examples are similar in nature to exam questions, in that you are asked to comment on statements being made or on analysis already completed.\n\n\n\n\n\n\n\n\n Task 1\n\n\n\nThe following excerpt is taken from the European Environment Agency web site, concerning one of their key indicators (CLIM008) for snow cover (accessed Jan 2011).\n“Data from satellite monitoring (NESDIS-database at NOAA) from 1966 to 2005 show that monthly snow-cover extent in the northern hemisphere is decreasing by 1.3 % per decade (Figure 2.1), with the strongest retreat in spring and summer (UNEP, 2007).”\nDiscuss critically the statement concerning the decrease per decade, and the comment concerning the spring and summer effects. Figure 1 shows a ‘trend’ a 12 month running mean and the original values expressed as anomalies, compare and contrast the three representations.\n\n\n\nFigure 1: Trends in Snow Cover\n\n\n\n\nSolution\n\nThe data plot (Fig 1) shows cycles and also a linear trend (with the slope estimated as % per decade). The green curve shows a 12 month running mean (so a smoothed curve), one could query the simplicity of the linear regression, since the running mean (moving average) does show fluctuations, although much smoothed in comparison to the original anomaly data. The comment that there is a strongest retreat in spring and summer suggest quite strongly that a simple straight line regression for the entire time period does not capture the ‘heterogeneity’ of rate of decline.\n\n\n\n\n\n\n\n\n\n Task 2\n\n\n\nIt is of interest to look at temperature records in Loch Lomond and to consider what the trend might be in water temperature. The data shown here are for Cailness in the north basin of the Loch. Comment on the patterns over time in this record. What other plots might you look at and how might you model this data to investigate the changes in temperature over time. There are some missing data values which you might consider how to impute. The data are available to explore on Moodle if you want to look at this in detail.\n\n\n\nFigure 2: Cailness temperature over time\n\n\n\n\nSolution\n\nTemperature records- here the approach since we have approximately monthly data requires both a trend and seasonality term, since we expect that temperature will be seasonal. This is a very similar problem to what we will look at with the Central England temperature record, in the lab. I would expect to see a time series plot of these data (with time expressed as year.decimal part of the year), and a seasonality plot, where each temperature is plotted against position within the year. This latter plot will for sure show a seasonal cycle and we could easily imagine fitting some sort of sin/cosine curve to this, hence the idea to use a harmonic regression. However we also have a problem in that there is some missing data, and so as a discussion point we could consider how best to impute these missing values.\nImputation rules (this again would be more typical of an exam style question): Possible answers here could range from replacement by a simple average of the values surrounding the missing value, to a more complex approach which would also use the seasonal signal (e.g. if the value was missing in January 2006, you could replace the missing value by the average of the January 2005 and Jan 2007 values). You could also use the fitted sine/cosine seasonal curve as a tool for imputation.\nYou could be asked to look at the original equation for the harmonic regression (which is non-linear in the params) and show how it can be linearised (straight from the notes).\n\n\n\n\n\n\n\n\n\n Task 3\n\n\n\nThe following excerpts are taken from the National Snow and Ice Data Centre (NSIDC) site, concerning arctic sea ice:\nStatement 3a\n“Arctic sea ice extent in January 2012 averaged 13.73 million square kilometers (5.30 million square miles). This is the fourth-lowest January ice extent in the 1979 to 2012 satellite data record, 1.10 million square kilometers (425,000 square miles) below the 1979 to 2000 average extent. Including the year 2012, the linear rate of decline for January ice extent over the satellite record is 3.2% per decade. Based on the satellite record, before 2005, average January ice extent had never been lower than 14 million square kilometers (5.41 million square miles). January ice extent has now fallen below that mark six out of the last seven years.”\nStatement 3b\n“The growth rate for Arctic sea ice in January was the slowest in the satellite record. After growing relatively quickly early in January, ice extent declined briefly in the middle of the month, and then grew more slowly than normal for the rest of the month. Overall, the Arctic gained 765,000 square kilometers (295,000 square miles) of ice during the month. This was 545,000 square kilometers (210,000 square miles) less than the average ice growth rate for January 1979 to 2000.”\n \nDiscuss critically the two statements above concerning the 2012 sea-ice making reference to Figures 3.1 and 3.2, specifically the decrease per decade. Comment on the statistical methods and assumption which could be used to fit the models shown in the Figures. How would you interpret Figure 3.2 in terms of the comparison of the 5 curves shown to the 1979-2000 average curve?\n\n\nSolution\n\nStatement 3a, several aspects to comment on, the most straightforward concerns the linear rate of decline (3.2% per decade), no uncertainty mentioned. Fig 3.1, shows the straight line fit and the fluctuations around the line (why no uncertainty bands?). The second aspect to comment on really is about the rank information, fourth lowest, and January ice extent fallen below 14 million km\\(^2\\) 6 out of last 7 years (these latter comments would be picked up when we discuss extremes in the next section of the lectures). Figure 3.2 shows several curves, including a global average curve and its standard deviation band and then the curves for individual decades. We can see that several of the decade curves lie outside the grey bands, all are below the mean curve.\n\n\n\n\n\n\n\n\n\n Task 4\n\n\n\nCarbon dioxide concentrations are routinely measured at many places around the globe- one such data set is shown below with also some text describing how the plot and smooth curve was produced.\n“To reduce noise in the determination of the global estimate due to atmospheric variability and measurement gaps, we fit a smooth curve to the weekly measurements.\nTo approximate the long-term trend and average seasonal cycle at a site, a function of the form\n\\[S(t) = \\alpha_o  + \\alpha_1 t + \\alpha_2 t^2  + \\sum_{k=1:4}\\left[b_{2k-1} \\text{sin}(2\\pi kt) + b_{2k} \\text{cos}(2\\pi kt)\\right]\\]\nis fitted to the measurements [Thoning et al., 1989]. The above function includes 3 polynomial parameters (quadratic) and 8 harmonic parameters, associated with the sine and cosine terms which can be converted to amplitude and phase of each harmonic, if desired.”\nFigure 4 shows the smooth curve, \\(S(t)\\), in red fitted to weekly background CO2 measurements from Ascension Island for 2000-2009.\nDiscuss the approach described in the statement concerning Figure 4 to quantify trend.\n\n\n\nFigure 4: CO\\(_2\\) data from Ascension Island\n\n\n\n\nSolution\n\nCritically, we have time series data, there are gaps, so a smooth curve is fitted, using a quadratic as in the equation. However there is also a need to include some cyclical terms, again introduced in the linear modelling framework using sin and cosine terms. Very standard regression modelling, but could and should also pick up on the fact there is no mention of the error terms or assumptions and since this is time series, we could expect that observations at a weekly scale are correlated, and what effect that might have on the estimates and any inference – i.e. less independent data so we might expect the standard estimates of our parameters to increase. An alternative approach may be to use a more flexible method (e.g. smoothing/GAM) to incorporate the seasonal pattern but with a linear term for trend. Models could be compared using an approximate F test. The advantage of the parametric approach adopted is the (relative) ease of interpretation.\n\n\n\n\n\n\n\n\n\n Task 5\n\n\n\nThe following excerpt is taken from the European Environment Agency web site, concerning one of their key indicators (CSI012) for European temperature (accessed Jan 2010):\n“The Earth has experienced considerable temperature increases in the last 100 years, especially in the most recent decades. These changes are unusual in terms of both magnitude and rate of change. The rate of change in the global average temperature is accelerating from 0.08\\(^\\circ\\)C per decade over the last 100 years, to 0.13\\(^\\circ\\)C per decade over the past 50 years up to 0.23\\(^\\circ\\)C per decade over the last 10 years (all values represent land & ocean area) (IPCC, 2007a). As such the indicative target (to keep climate change within ‘safe’ limits) of 0.2\\(^\\circ\\)C per decade has been exceeded in the recent years.”\nThe figure below shows the time series plot of the CSI012 indicator, which has been developed to address specific policy issues concerning the trend and rate of change in the European annual and seasonal temperature.\n\n\n\nFigure 5: CSI012 European Land Temperature Deviations\n\n\nDiscuss critically the statement and the above figure and how they might be related. Suggest how you might model such temperature data above to address the policy issues.\n\n\nSolution\n\nThis example is again all about trends, expressed one presumes as a linear trend, and a slope in the quote. The interesting thing is that the slope is different in different time periods and over different lengths of time. Again, no uncertainty is quoted. The plot shows deviations using a 10 year moving average, so a smoothed plot, and we can also see differences between summer and winter, suggesting temperature change may indeed be occurring at different rates in different times of the year. How might we model absolute temperature and the deviations? We can certainly imagine therefore that temperature will have a strong seasonal cycle (so could use a harmonic regression), that a simple linear regression is not likely to be complex enough since the rate of change is changing, so could fit some sort of piecewise regression but what is the justification for the changepoints? Or we could fit some sort of smooth curve (not necessarily monotonic).\n\n\n\n\n\n\n\n\n\n Task 6\n\n\n\nThe plot below shows the Monthly time series of phosphorous (P) concentrations in mg/l, Speed River, Guelph, Ontario between 1972.1–1977.1.\n\n\n\n\n\nComment subjectively on the patterns in the plots P in the River Guelph and how you might statistically model any trends/patterns over time observed.\nThe output below shows the summary information for the Mann Kendall score for all time points and for the data split by month (Seasonal). Use the information to carry out a Mann Kendall test and seasonal Mann Kendall test and compute the Mann Kendall correlation coefficient for each. Interpret your results.\nAll Time Points          Seasonal\nD = 2546.482             D = 178.4556\nS =  -1171               S = -99\nVar(S) = 42292.33        Var(S) = 337\n\n\nSolution\n\nThe plots show a decreasing trend. The plot of the data on the original scale indicates non-constant variability but this seems to have been dealt with by the application of the log transformation. Subjectively there may be an indication of a change point in 1974 were there is a large jump in the level of P and so a changepoint analysis may be applied here.\nThe data are quite noisy but there is some indication of a seasonal pattern. This could be incorporated using a harmonic regression term within a linear regression. Alternatively, we could assess if there was a trend using a seasonal Mann Kendall test to check if there was any evidence of a monotonic change in P over the time period considered.\nAll Time Points        Seasonal\nD = 2546.482           D = 178.4556\nS =  -1171             S = -99\nVar(S) = 42292.33      Var(S) = 337\nAll data:\n\n\\(\\text{Tau} = S/D = -1171/2546.482= -0.46\\)\n\\(Z= (-1171+1)/\\sqrt{42292.33} = -5.7\\).\n\nSeasonal:\n\n\\(\\text{Tau} = S/D = -99/178.4556= -0.55\\)\n\\(Z= (-99+1)/\\sqrt{337} = -5.3\\).\n\nBoth the Z score for all data together and the Z score after the seasonal adjustment are greater than the critical value of 1.96 (in absolute value terms). So there is evidence of a statistically significant monotonic trend in the data. The tau values of -0.46 for all data together, and -0.55 for the data taking into account the seasonal component both suggest there is a moderate negative monotonic trend. This supports the initial impression gained from inspection of the data.\n\n\n\n\n\n\n\n\n\n Task 7\n\n\n\nThe figure below shows a regression example from a recent paper in Science (Johnson et al, 2014) which investigated the rate of thinning of the Pine Island glacier. They fit a “2-segment, piecewise-linear age-elevation history to the Mt Moses data”.\n\n\n\n\n\nExplain what the phrase in bold above means, and write down the equation for a 2 stage regression, assuming a known changepoint. Explain how the parameters of the model can be estimated. Comment briefly on the figure above.\n\n\nSolution\n\nThe model that the authors have fitted is one where there are two straight lines which are constrained to meet at a changepoint.\nA change-point could be defined as a point that separates a series of observations into two groups, each following a different model, in this case a different slope and intercept.\nKnown changepoint:\n\\[f(x_i) = \\begin{cases} \\alpha_1 + \\beta_1x_i \\hspace{1cm} x_i\\leq \\delta\\\\ \\alpha_2 + \\beta_2 x_i \\hspace{1cm} \\delta \\leq x_i \\end{cases}\\]\nconstrained such that\n\\[\\alpha_1 + \\beta_1 \\delta = \\alpha_2 + \\beta_2 \\delta\\]\nParameters can then be estimated by ordinary least squares. Plot shows the diverging line, no uncertainty estimates, nor justification of where the changepoint occurs.\n\n\n\n\n\n2 Part B: Models for extremes\n\n\n\n\n\n\n Task 8\n\n\n\nIf \\(X_1, \\dots, X_n\\) is a sequence of independent standard exponential \\(Exp(1)\\) variables,\n\nShow that \\(F(x)= 1-e^{-x}\\) for \\(x&gt; 0\\).\nShow that, for \\(a_n=1\\) and \\(b_n=\\log(n)\\), the limit distribution of \\(M_n\\) as \\(n \\rightarrow \\infty\\) is the Gumbel distribution, corresponding to \\(\\zeta = 0\\) in the GEV family.\n\n\n\nSolution\n\n\nIf \\(X_1, \\dots, X_n \\sim Exp(1)\\),\n\nrange of \\(x\\) is \\(X&gt;0\\)\npdf of \\(Exp(\\lambda)\\) is \\(f(x) = \\lambda e^{-\\lambda x}\\), so for \\(Exp(1)\\), \\(f(x) = e^{-x}\\)\n\n\n\\[\\begin{split}\\therefore F(x) &= P(X \\leq x)\\\\ &= \\int_0^x f(t) dt\\\\ &= [-e^{-t}]_0^x\\\\ &= -e^{-x} - (-e^0)\\\\ &= 1 - e^{-x} \\hspace{1cm} (\\text{for } x&gt;0)\\end{split}\\]\n\nLet \\(M_n = \\max\\{X_1, \\dots, X_n\\}\\), let \\(a_n = 1\\) and let \\(b_n = \\log(n)\\). Then:\n\n\\[Pr\\left\\{\\frac{M_n - b_n}{a_n} \\leq z \\right\\} = F^n(a_nz + b_n) \\rightarrow G(z) \\text{ as } n \\rightarrow \\infty\\] \\[\\begin{split} F_n(a_nz + b_n) & = F^n(z+\\log(n))\\\\ & = [1 - e^{-(z+\\log(n))}]^n\\\\ &\\rightarrow \\left[1-\\frac{e^{-z}}{n} \\right]^n \\ \\text{ as } n \\rightarrow \\infty\\\\ &= -\\exp(-\\exp(-z))\\end{split}\\]\n\n\n\n\nNote: \\[\\exp(x) = \\lim_{n \\rightarrow \\infty}\\left( 1+\\frac{x}{n}\\right)^n \\ \\ \\ \\ \\ \\ \\ \\ \\]\n\n\n\n\n\n Task 9\n\n\n\nIf \\(X_1, \\dots, X_n\\) is a sequence of independent standard exponential \\(Exp(\\lambda)\\) variables,\n\nShow that \\(F_n(x)= (1-e^{-\\lambda x})^n\\) for \\(x&gt; 0\\), where \\(F_n(x)\\) is the distribution of \\(X(n)\\).\nFind the distribution function of \\(X_{(1)}\\), and show that \\(X_{(1)}\\) has an Exponential distribution with parameter \\(n\\lambda\\).\n\n\n\nSolution\n\n\n\\(F(x) = 1 - e^{-\\lambda x}\\)\n\n\\[\\begin{split} F_n(x) &= [P(X \\leq x)]^n\\\\ &=\\left[ \\int_0^x f(t) dt\\right]^n\\\\ &=\\left[\\int_0^x \\lambda e^{-\\lambda t} dt \\right]^n\\\\ &=\\left[ \\left[ e^{-\\lambda t}\\right]_0^x\\right]^n\\\\ &=\\left[ 1 - e^{-\\lambda x}\\right]^n \\end{split}\\]\n\nLet \\(X_{(1)} = \\min\\left\\{X_1, \\dots, X_n \\right\\}\\). Then:\n\n\\[\\begin{split} \\Pr\\left\\{ X_{(1)} \\leq t\\right\\} &= \\Pr\\left\\{ \\min\\left\\{X_1, \\dots, X_n \\right\\} \\leq t\\right\\}\\\\ &= 1 - \\Pr\\left\\{ \\min\\left\\{X_1, \\dots, X_n \\right\\} \\geq t\\right\\}\\\\ &= 1 - \\Pr \\left\\{ X_i \\geq t \\ \\text{for all} \\ i\\right\\}\\\\ &= 1 - (e^{-\\lambda t})^n\\\\ &= 1 - e^{-\\lambda nt} \\end{split}\\] \\[\\therefore X_{(1)} \\sim Exp(n \\lambda)\\]\n\n\n\n\n\n\n\n\n\n Task 10\n\n\n\n\nDaily wind speed data are available for a location in the Netherlands. A POT modelling approach has been used to model the data. The figure below shows a mean residual life plot for the series. Comment on how you could use this plot to identify a suitable threshold for the POT model. What threshold would you select here?\n\n\n\n\n\n\n\nThe figure below shows the diagnostic plots for a fitted POT model for the wind speed data. Comment on the model fit using these plots.\n\n\n\n\n\n\n\nThe threshold selected was 10. Comment on this choice using the sensitivity analysis below.\n\n\n\n\n\n\n\nThe R output for the model is shown below.\n\n$threshold\n[1] 10\n\n$nexc\n[1] 105\n\n$conv\n[1] 0\n\n$nllh\n[1] 92.17218\n\n$mle\n[1]  0.9278907 -0.0473220\n\n$rate\n[1] 0.00684485\n\n$se\n[1] 0.12364549 0.09084164\nEstimate the 1 in 100 year wind speed.\n\n\nSolution\n\n\nLooking for area where (after taking confidence bands into account) the mean excess is linearly related to the threshold. Not immediately obvious, somewhat subjective. I would say somewhere between 10 and 13. Start as low as possible so 10 would be a good threshold to start with.\nQuantile and probability plots both have good agreement between points (corresponding to the sample) and the theoretical line, therefore the fit seems reasonable. Histogram also seems to have good agreement between the sample and the solid line which represents the estimated density using the fitted model. Have to be careful when interpreting histograms as interpretation can change with the number of bins used, particularly when the sample size is small. Return level plot, points lie within confidence interval in general. Another indication the model is suitable.\nThe threshold value of 10 seems like a good threshold. There is very little variability in the sensitivity analysis fit, the parameter estimates seem constant across a range of thresholds, only at 12 and above do estimates seem to vary. Maybe could argue we could look at thresholds even lower than 10 —- need to balance what is truly extreme, while ensuring there is sufficient data available above the threshold to use to estimate our model.\n\n\n\n\n\n\n\n\n\n\n Task 11\n\n\n\nThe generalized extreme value distribution (GEV) has three parameters and distribution function\n\\[G(z) = \\exp \\left\\{ -\\left[ 1+\\xi\\frac{(z_i - \\mu)}{\\sigma}\\right]^{\\frac{-1}{\\xi}}\\right\\}\\]\nGEV is often used when modelling block maxima. Identify the particular family and write down its distribution function when \\(\\xi\\) is assumed equal to 0.\nShow that when \\(G(z_p) =1-p\\), then\n\\[z_p = \\begin{cases} \\mu - \\sigma \\frac{[1-\\{-\\log(1-p)\\}^{-\\xi}]}{\\xi} & \\text{for } \\xi \\neq 0\\\\\n\\mu - \\sigma[\\log\\{-\\log(1-p)\\}] & \\text{for } \\xi = 0\\end{cases}\\]\nFor the distribution \\(G\\) in the case \\(\\xi=0\\), show that if \\(z_p\\) is plotted against \\(\\log[–\\log(1-p)]\\), the plot should be linear.\n\n\nSolution\n\n\\[G(z) = \\exp \\left\\{ -\\left[ 1+\\xi\\frac{(z - \\mu)}{\\sigma}\\right]^{\\frac{-1}{\\xi}}\\right\\}\\]\nSet \\(G(z_p) = 1 - p\\) and solve equation above for \\(z_p\\):\n\\[\\begin{split} \\exp \\left\\{ -\\left[ 1+\\xi\\frac{(z_p - \\mu)}{\\sigma}\\right]^{\\frac{-1}{\\xi}}\\right\\} &= 1 - p\\\\\n\\left\\{ -\\left[ 1+\\xi\\frac{(z_p - \\mu)}{\\sigma}\\right]^{\\frac{-1}{\\xi}}\\right\\} &= \\log(1-p)\\\\\n-\\left[ 1+\\xi\\frac{(z_p - \\mu)}{\\sigma}\\right] &= \\log(1-p)^{-\\xi}\\\\\n\\left[ 1+\\xi\\frac{(z_p - \\mu)}{\\sigma}\\right] &= -\\log(1-p)^{-\\xi}\\\\\n\\xi\\frac{(z_p - \\mu)}{\\sigma} &= \\{ -\\log(1-p)^{-\\xi}\\}-1\\\\\n\\frac{(\\mu - z_p)}{\\sigma} &= \\frac{1}{\\xi}[1-(-\\log(1-p)^{-\\xi})]\\\\\n-z_p &= -\\mu + \\frac{\\sigma}{\\xi}(1-(-\\log(1-p)^{-\\xi}))\\\\z_p &= \\mu - \\frac{\\sigma}{\\xi}(1-(-\\log(1-p)^{-\\xi}))\\end{split}\\]\nWhen \\(\\xi = 0\\), we have a Gumbel distribution:\n\\[\\begin{split}\\exp\\left[ -\\exp\\left( \\frac{-(z_p - \\mu)}{\\sigma}\\right)\\right] &= 1-p\\\\\n\\left[ -\\exp\\left( \\frac{-(z_p - \\mu)}{\\sigma}\\right)\\right] &=\\log(1-p)\\\\\n\\left( \\frac{-(z_p - \\mu)}{\\sigma}\\right) &= \\log(-\\log(1-p))\\\\\n-z_p + \\mu &= \\sigma\\log(-\\log(1-p))\\\\\n-z_p &= -\\mu + \\sigma\\log(-\\log(1-p))\\\\\nz_p &= \\mu - \\sigma\\log(-\\log(1-p))\\end{split}\\]\n\\(z_p\\) is the return level associated with the return period \\(1/p\\).\nLet \\(s_p = \\log(-\\log(1-p))\\). Then \\(z_p = \\mu - \\sigma\\log(-\\log(1-p)) = \\mu - \\sigma s_p\\) when \\(\\xi = 0\\). Therefore, \\(z_p\\) is linearly related to \\(\\log(-\\log(1-p))\\) in the case \\(\\xi = 0\\)."
  },
  {
    "objectID": "notes/tutorial_sheet_3.html",
    "href": "notes/tutorial_sheet_3.html",
    "title": "Tutorial Sheet 2",
    "section": "",
    "text": "Note\n\n\n\nSeveral of these examples are similar in nature to exam questions, in that you are asked to comment on statements being made or on analysis already completed.\n\n\n\n\nThe following excerpt is taken from the European Environment Agency web site, concerning one of their key indicators (CLIM008) for snow cover (accessed Jan 2011).\n“Data from satellite monitoring (NESDIS-database at NOAA) from 1966 to 2005 show that monthly snow-cover extent in the northern hemisphere is decreasing by 1.3 % per decade (Figure 2.1), with the strongest retreat in spring and summer (UNEP, 2007).”\nDiscuss critically the statement concerning the decrease per decade, and the comment concerning the spring and summer effects. Figure 1 shows a ‘trend’ a 12 month running mean and the original values expressed as anomalies, compare and contrast the three representations.\n\n\n\nFigure 1: Trends in Snow Cover\n\n\n\n\n\nIt is of interest to look at temperature records in Loch Lomond and to consider what the trend might be in water temperature. The data shown here are for Cailness in the north basin of the Loch. Comment on the patterns over time in this record. What other plots might you look at and how might you model this data to investigate the changes in temperature over time. There are some missing data values which you might consider how to impute. The data are available to explore on Moodle if you want to look at this in detail.\n\n\n\nFigure 2: Cailness temperature over time\n\n\n\n\n\nThe following excerpts are taken from the National Snow and Ice Data Centre (NSIDC) site, concerning arctic sea ice:\nStatement 3a\n“Arctic sea ice extent in January 2012 averaged 13.73 million square kilometers (5.30 million square miles). This is the fourth-lowest January ice extent in the 1979 to 2012 satellite data record, 1.10 million square kilometers (425,000 square miles) below the 1979 to 2000 average extent. Including the year 2012, the linear rate of decline for January ice extent over the satellite record is 3.2% per decade. Based on the satellite record, before 2005, average January ice extent had never been lower than 14 million square kilometers (5.41 million square miles). January ice extent has now fallen below that mark six out of the last seven years.”\nStatement 3b\n“The growth rate for Arctic sea ice in January was the slowest in the satellite record. After growing relatively quickly early in January, ice extent declined briefly in the middle of the month, and then grew more slowly than normal for the rest of the month. Overall, the Arctic gained 765,000 square kilometers (295,000 square miles) of ice during the month. This was 545,000 square kilometers (210,000 square miles) less than the average ice growth rate for January 1979 to 2000.”\n \nDiscuss critically the two statements above concerning the 2012 sea-ice making reference to Figures 3.1 and 3.2, specifically the decrease per decade. Comment on the statistical methods and assumption which could be used to fit the models shown in the Figures. How would you interpret Figure 3.2 in terms of the comparison of the 5 curves shown to the 1979-2000 average curve?\n\n\n\nCarbon dioxide concentrations are routinely measured at many places around the globe- one such data set is shown below with also some text describing how the plot and smooth curve was produced.\n“To reduce noise in the determination of the global estimate due to atmospheric variability and measurement gaps, we fit a smooth curve to the weekly measurements.\nTo approximate the long-term trend and average seasonal cycle at a site, a function of the form\n\\[S(t) = \\alpha_o  + \\alpha_1 t + \\alpha_2 t^2  + \\sum_{k=1:4}\\left[b_{2k-1} \\text{sin}(2\\pi kt) + b_{2k} \\text{cos}(2\\pi kt)\\right]\\]\nis fitted to the measurements [Thoning et al., 1989]. The above function includes 3 polynomial parameters (quadratic) and 8 harmonic parameters, associated with the sine and cosine terms which can be converted to amplitude and phase of each harmonic, if desired.”\nFigure 4 shows the smooth curve, \\(S(t)\\), in red fitted to weekly background CO2 measurements from Ascension Island for 2000-2009.\nDiscuss the approach described in the statement concerning Figure 4 to quantify trend.\n\n\n\nFigure 4: CO\\(_2\\) data from Ascension Island\n\n\n\n\n\nThe following excerpt is taken from the European Environment Agency web site, concerning one of their key indicators (CSI012) for European temperature (accessed Jan 2010):\n“The Earth has experienced considerable temperature increases in the last 100 years, especially in the most recent decades. These changes are unusual in terms of both magnitude and rate of change. The rate of change in the global average temperature is accelerating from 0.08\\(^\\circ\\)C per decade over the last 100 years, to 0.13\\(^\\circ\\)C per decade over the past 50 years up to 0.23\\(^\\circ\\)C per decade over the last 10 years (all values represent land & ocean area) (IPCC, 2007a). As such the indicative target (to keep climate change within ‘safe’ limits) of 0.2\\(^\\circ\\)C per decade has been exceeded in the recent years.”\nThe figure below shows the time series plot of the CSI012 indicator, which has been developed to address specific policy issues concerning the trend and rate of change in the European annual and seasonal temperature.\n\n\n\nFigure 5: CSI012 European Land Temperature Deviations\n\n\nDiscuss critically the statement and the above figure and how they might be related. Suggest how you might model such temperature data above to address the policy issues.\n\n\n\nThe plot below shows the Monthly time series of phosphorous (P) concentrations in mg/l, Speed River, Guelph, Ontario between 1972.1–1977.1.\n\n\n\n\n\nComment subjectively on the patterns in the plots P in the River Guelph and how you might statistically model any trends/patterns over time observed.\nThe output below shows the summary information for the Mann Kendall score for all time points and for the data split by month (Seasonal). Use the information to carry out a Mann Kendall test and seasonal Mann Kendall test and compute the Mann Kendall correlation coefficient for each. Interpret your results.\nAll Time Points          Seasonal\nD = 2546.482             D = 178.4556\nS =  -1171               S = -99\nVar(S) = 42292.33        Var(S) = 337\n\n\n\nThe figure below shows a regression example from a recent paper in Science (Johnson et al, 2014) which investigated the rate of thinning of the Pine Island glacier. They fit a “2-segment, piecewise-linear age-elevation history to the Mt Moses data”.\n\n\n\n\n\nExplain what the phrase in bold above means, and write down the equation for a 2 stage regression, assuming a known changepoint. Explain how the parameters of the model can be estimated. Comment briefly on the figure above."
  },
  {
    "objectID": "notes/tutorial_sheet_3.html#task-1",
    "href": "notes/tutorial_sheet_3.html#task-1",
    "title": "Tutorial Sheet 2",
    "section": "",
    "text": "The following excerpt is taken from the European Environment Agency web site, concerning one of their key indicators (CLIM008) for snow cover (accessed Jan 2011).\n“Data from satellite monitoring (NESDIS-database at NOAA) from 1966 to 2005 show that monthly snow-cover extent in the northern hemisphere is decreasing by 1.3 % per decade (Figure 2.1), with the strongest retreat in spring and summer (UNEP, 2007).”\nDiscuss critically the statement concerning the decrease per decade, and the comment concerning the spring and summer effects. Figure 1 shows a ‘trend’ a 12 month running mean and the original values expressed as anomalies, compare and contrast the three representations.\n\n\n\nFigure 1: Trends in Snow Cover"
  },
  {
    "objectID": "notes/tutorial_sheet_3.html#task-2",
    "href": "notes/tutorial_sheet_3.html#task-2",
    "title": "Tutorial Sheet 2",
    "section": "",
    "text": "It is of interest to look at temperature records in Loch Lomond and to consider what the trend might be in water temperature. The data shown here are for Cailness in the north basin of the Loch. Comment on the patterns over time in this record. What other plots might you look at and how might you model this data to investigate the changes in temperature over time. There are some missing data values which you might consider how to impute. The data are available to explore on Moodle if you want to look at this in detail.\n\n\n\nFigure 2: Cailness temperature over time"
  },
  {
    "objectID": "notes/tutorial_sheet_3.html#task-3",
    "href": "notes/tutorial_sheet_3.html#task-3",
    "title": "Tutorial Sheet 2",
    "section": "",
    "text": "The following excerpts are taken from the National Snow and Ice Data Centre (NSIDC) site, concerning arctic sea ice:\nStatement 3a\n“Arctic sea ice extent in January 2012 averaged 13.73 million square kilometers (5.30 million square miles). This is the fourth-lowest January ice extent in the 1979 to 2012 satellite data record, 1.10 million square kilometers (425,000 square miles) below the 1979 to 2000 average extent. Including the year 2012, the linear rate of decline for January ice extent over the satellite record is 3.2% per decade. Based on the satellite record, before 2005, average January ice extent had never been lower than 14 million square kilometers (5.41 million square miles). January ice extent has now fallen below that mark six out of the last seven years.”\nStatement 3b\n“The growth rate for Arctic sea ice in January was the slowest in the satellite record. After growing relatively quickly early in January, ice extent declined briefly in the middle of the month, and then grew more slowly than normal for the rest of the month. Overall, the Arctic gained 765,000 square kilometers (295,000 square miles) of ice during the month. This was 545,000 square kilometers (210,000 square miles) less than the average ice growth rate for January 1979 to 2000.”\n \nDiscuss critically the two statements above concerning the 2012 sea-ice making reference to Figures 3.1 and 3.2, specifically the decrease per decade. Comment on the statistical methods and assumption which could be used to fit the models shown in the Figures. How would you interpret Figure 3.2 in terms of the comparison of the 5 curves shown to the 1979-2000 average curve?"
  },
  {
    "objectID": "notes/tutorial_sheet_3.html#task-4",
    "href": "notes/tutorial_sheet_3.html#task-4",
    "title": "Tutorial Sheet 2",
    "section": "",
    "text": "Carbon dioxide concentrations are routinely measured at many places around the globe- one such data set is shown below with also some text describing how the plot and smooth curve was produced.\n“To reduce noise in the determination of the global estimate due to atmospheric variability and measurement gaps, we fit a smooth curve to the weekly measurements.\nTo approximate the long-term trend and average seasonal cycle at a site, a function of the form\n\\[S(t) = \\alpha_o  + \\alpha_1 t + \\alpha_2 t^2  + \\sum_{k=1:4}\\left[b_{2k-1} \\text{sin}(2\\pi kt) + b_{2k} \\text{cos}(2\\pi kt)\\right]\\]\nis fitted to the measurements [Thoning et al., 1989]. The above function includes 3 polynomial parameters (quadratic) and 8 harmonic parameters, associated with the sine and cosine terms which can be converted to amplitude and phase of each harmonic, if desired.”\nFigure 4 shows the smooth curve, \\(S(t)\\), in red fitted to weekly background CO2 measurements from Ascension Island for 2000-2009.\nDiscuss the approach described in the statement concerning Figure 4 to quantify trend.\n\n\n\nFigure 4: CO\\(_2\\) data from Ascension Island"
  },
  {
    "objectID": "notes/tutorial_sheet_3.html#task-5",
    "href": "notes/tutorial_sheet_3.html#task-5",
    "title": "Tutorial Sheet 2",
    "section": "",
    "text": "The following excerpt is taken from the European Environment Agency web site, concerning one of their key indicators (CSI012) for European temperature (accessed Jan 2010):\n“The Earth has experienced considerable temperature increases in the last 100 years, especially in the most recent decades. These changes are unusual in terms of both magnitude and rate of change. The rate of change in the global average temperature is accelerating from 0.08\\(^\\circ\\)C per decade over the last 100 years, to 0.13\\(^\\circ\\)C per decade over the past 50 years up to 0.23\\(^\\circ\\)C per decade over the last 10 years (all values represent land & ocean area) (IPCC, 2007a). As such the indicative target (to keep climate change within ‘safe’ limits) of 0.2\\(^\\circ\\)C per decade has been exceeded in the recent years.”\nThe figure below shows the time series plot of the CSI012 indicator, which has been developed to address specific policy issues concerning the trend and rate of change in the European annual and seasonal temperature.\n\n\n\nFigure 5: CSI012 European Land Temperature Deviations\n\n\nDiscuss critically the statement and the above figure and how they might be related. Suggest how you might model such temperature data above to address the policy issues."
  },
  {
    "objectID": "notes/tutorial_sheet_3.html#task-6",
    "href": "notes/tutorial_sheet_3.html#task-6",
    "title": "Tutorial Sheet 2",
    "section": "",
    "text": "The plot below shows the Monthly time series of phosphorous (P) concentrations in mg/l, Speed River, Guelph, Ontario between 1972.1–1977.1.\n\n\n\n\n\nComment subjectively on the patterns in the plots P in the River Guelph and how you might statistically model any trends/patterns over time observed.\nThe output below shows the summary information for the Mann Kendall score for all time points and for the data split by month (Seasonal). Use the information to carry out a Mann Kendall test and seasonal Mann Kendall test and compute the Mann Kendall correlation coefficient for each. Interpret your results.\nAll Time Points          Seasonal\nD = 2546.482             D = 178.4556\nS =  -1171               S = -99\nVar(S) = 42292.33        Var(S) = 337"
  },
  {
    "objectID": "notes/tutorial_sheet_3.html#task-7",
    "href": "notes/tutorial_sheet_3.html#task-7",
    "title": "Tutorial Sheet 2",
    "section": "",
    "text": "The figure below shows a regression example from a recent paper in Science (Johnson et al, 2014) which investigated the rate of thinning of the Pine Island glacier. They fit a “2-segment, piecewise-linear age-elevation history to the Mt Moses data”.\n\n\n\n\n\nExplain what the phrase in bold above means, and write down the equation for a 2 stage regression, assuming a known changepoint. Explain how the parameters of the model can be estimated. Comment briefly on the figure above."
  },
  {
    "objectID": "notes/tutorial_sheet_3.html#task-8",
    "href": "notes/tutorial_sheet_3.html#task-8",
    "title": "Tutorial Sheet 2",
    "section": "Task 8",
    "text": "Task 8\nIf \\(X_1, \\dots, X_n\\) is a sequence of independent standard exponential \\(Exp(1)\\) variables,\n\nShow that \\(F(x)= 1-e^{-x}\\) for \\(x&gt; 0\\).\nShow that, for \\(a_n=1\\) and \\(b_n=\\log(n)\\), the limit distribution of \\(M_n\\) as \\(n \\rightarrow \\infty\\) is the Gumbel distribution, corresponding to \\(\\zeta = 0\\) in the GEV family."
  },
  {
    "objectID": "notes/tutorial_sheet_3.html#task-9",
    "href": "notes/tutorial_sheet_3.html#task-9",
    "title": "Tutorial Sheet 2",
    "section": "Task 9",
    "text": "Task 9\nIf \\(X_1, \\dots, X_n\\) is a sequence of independent standard exponential \\(Exp(\\lambda)\\) variables,\n\nShow that \\(F_n(x)= (1-e^{-\\lambda x})^n\\) for \\(x&gt; 0\\), where \\(F_n(x)\\) is the distribution of \\(X(n)\\).\nFind the distribution function of \\(X_{(1)}\\), and show that \\(X_{(1)}\\) has an Exponential distribution with parameter \\(n\\lambda\\)."
  },
  {
    "objectID": "notes/tutorial_sheet_3.html#task-10",
    "href": "notes/tutorial_sheet_3.html#task-10",
    "title": "Tutorial Sheet 2",
    "section": "Task 10",
    "text": "Task 10\n\nDaily wind speed data are available for a location in the Netherlands. A POT modelling approach has been used to model the data. The figure below shows a mean residual life plot for the series. Comment on how you could use this plot to identify a suitable threshold for the POT model. What threshold would you select here?\n\n\n\n\n\n\n\nThe figure below shows the diagnostic plots for a fitted POT model for the wind speed data. Comment on the model fit using these plots.\n\n\n\n\n\n\n\nThe threshold selected was 10. Comment on this choice using the sensitivity analysis below.\n\n\n\n\n\n\n\nThe R output for the model is shown below.\n\n$threshold\n[1] 10\n\n$nexc\n[1] 105\n\n$conv\n[1] 0\n\n$nllh\n[1] 92.17218\n\n$mle\n[1]  0.9278907 -0.0473220\n\n$rate\n[1] 0.00684485\n\n$se\n[1] 0.12364549 0.09084164\nEstimate the 1 in 100 year wind speed."
  },
  {
    "objectID": "notes/tutorial_sheet_3.html#task-11",
    "href": "notes/tutorial_sheet_3.html#task-11",
    "title": "Tutorial Sheet 2",
    "section": "Task 11",
    "text": "Task 11\nThe generalized extreme value distribution (GEV) has three parameters and distribution function\n\\[G(z) = \\exp \\left\\{ -\\left[ 1+\\xi\\frac{(z_i - \\mu)}{\\sigma}\\right]^{\\frac{-1}{\\xi}}\\right\\}\\]\nGEV is often used when modelling block maxima. Identify the particular family and write down its distribution function when \\(\\xi\\) is assumed equal to 0.\nShow that when \\(G(z_p) =1-p\\), then\n\\[z_p = \\begin{cases} \\mu - \\sigma \\frac{[1-\\{-\\log(1-p)\\}^{-\\xi}]}{\\xi} & \\text{for } \\xi \\neq 0\\\\\n\\mu - \\sigma[\\log\\{-\\log(1-p)\\}] & \\text{for } \\xi = 0\\end{cases}\\]\nFor the distribution \\(G\\) in the case \\(\\xi=0\\), show that if \\(z_p\\) is plotted against \\(\\log[–\\log(1-p)]\\), the plot should be linear."
  },
  {
    "objectID": "lab_2.html",
    "href": "lab_2.html",
    "title": "Lab session 2",
    "section": "",
    "text": "Aim of this practical session:\nIn this practical session we are going\nDownload Lab 2 R script"
  },
  {
    "objectID": "lab_2.html#sulphur-dioxide-in-the-air",
    "href": "lab_2.html#sulphur-dioxide-in-the-air",
    "title": "Lab session 2",
    "section": "Sulphur dioxide in the air",
    "text": "Sulphur dioxide in the air\nSulphur dioxide is an atmospheric pollutant that is monitored in most cities. The data here are weekly concentrations from 1989, in the file SO2.csv.\n Download SO2 data set \nThe dataset contains (amongst others) the following variables of interest:\n\n\n\nVariable\nMeaning\n\n\n\n\nYears\nYear of observation\n\n\nWeeks\nWeek of observation (within the year)\n\n\nln.SO2.\nLog(SO\\(_\\text{2}\\))\n\n\n\nA natural logarithm transformation has been applied to the SO\\(_\\text{2}\\) data.\n\n# Load required R packages:\n\nlibrary(ggplot2)    # For visualizing our data\nlibrary(broom)      # For model predictions\n\n\n# Read in data (making sure to set the working directory to the \n# appropriate location):\nSO2 &lt;- read.csv(\"datasets/SO2.csv\", header = TRUE)\n\n# Examine structure of the dataset:\nstr(SO2)\n\n'data.frame':   516 obs. of  7 variables:\n $ Years         : int  1989 1989 1989 1989 1989 1989 1989 1989 1989 1989 ...\n $ Weeks         : int  2 5 7 8 9 10 11 12 13 14 ...\n $ Log.Rain      : num  2.001 -0.916 3.19 2.51 2.398 ...\n $ Mean.Temp     : num  2.157 -0.929 0.986 5.271 3.243 ...\n $ Humidity      : num  87.9 92.9 84.4 78 76.9 ...\n $ Wind.Dir.Speed: num  -111 -168 -117 -171 -134 ...\n $ ln.SO2.       : num  0.3086 0.3552 0.4732 0.0487 -0.4865 ...\n\n# Add year as a decimal (continuous variable):\nSO2$year_num &lt;- SO2$Years + SO2$Weeks / 53"
  },
  {
    "objectID": "lab_2.html#visualizing-our-data",
    "href": "lab_2.html#visualizing-our-data",
    "title": "Lab session 2",
    "section": "1.1 Visualizing our data",
    "text": "1.1 Visualizing our data\nLets create some exploratory plots to visualize the relationship between log SO\\(^2\\) and the week while adding some loess curves with different spans (to examine possible seasonal patterns):\n\nggplot(data = SO2,\n       aes(y = ln.SO2., x = Weeks ))+\n  geom_point()+\n  geom_smooth(method = \"loess\",span=0.75,aes(color=\"0.75\"))+\n  geom_smooth(method = \"loess\",span=0.5,aes(color=\"0.5\"))+\n    geom_smooth(method = \"loess\",span=0.3,aes(color=\"0.3\"))+\n  labs(y=expression(log~SO^2))\n\n\n\n\n\n\n\n\nThere seems to be some evidence of a seasonal pattern in log(SO\\(_2\\)), with higher values towards the starts and ends of the week within years.\n\n\n\n\n\n\n Task 1\n\n\n\nAnother way of exploring potential seasonality is by using boxplots. Try to produce some boxplots that show the Log(SO\\(_\\text{2}\\)) against the weeks. Recall that boxplots are meant to summarise the distribution of a continuous variable against categorical ones -thus, you need to declare the Weeks variable as a factor (this can be done directly on ggplot)\n\n\nTake hint\n\nWe can achieve this by adding a geom_boxplot() layer to a ggplot object.\n\n\n\n\nClick here to see the solution\n\n\nCode\nggplot(data = SO2,\n       aes(y = ln.SO2., x = as.factor(Weeks) ))+\n  geom_boxplot()+\n   labs(y=expression(log~SO^2),x=\"Weeks\")\n\n\n\n\n\n\n\n\n\n\n\n\nNow to examine the trend we can plot our data over years as follows:\n\nggplot(data = SO2,\n       aes(y = ln.SO2., x = year_num ))+\n  geom_point()+\n  geom_smooth(method = \"loess\",aes(color=\"0.75\"))\n\n\n\n\n\n\n\n\nThere is an overall decreasing (not necessarily linear) trend in log(SO\\(_2\\)) over time. However, we need to account for the seasonal pattern when trying to assess the trend, so we fit a harmonic model to the data, with a long-term trend also in the model:\n\n# Create a model with seasonal pattern (harmonic model) plus trend:\nharmonic_model &lt;- lm(ln.SO2. ~ sin(2 * pi * (Weeks - 53) / 53) + \n              cos(2 * pi * (Weeks - 53) / 53) + year_num,\n            data = SO2)\n\nLets look at the summary of the model:\n\nsummary(harmonic_model)\n\n\nCall:\nlm(formula = ln.SO2. ~ sin(2 * pi * (Weeks - 53)/53) + cos(2 * \n    pi * (Weeks - 53)/53) + year_num, data = SO2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.44795 -0.42321  0.03846  0.42933  1.64858 \n\nCoefficients:\n                                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                   331.667008  16.390999  20.235  &lt; 2e-16 ***\nsin(2 * pi * (Weeks - 53)/53)   0.246362   0.039360   6.259 8.19e-10 ***\ncos(2 * pi * (Weeks - 53)/53)   0.326653   0.040119   8.142 2.98e-15 ***\nyear_num                       -0.166260   0.008215 -20.239  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6365 on 512 degrees of freedom\nMultiple R-squared:  0.5082,    Adjusted R-squared:  0.5053 \nF-statistic: 176.4 on 3 and 512 DF,  p-value: &lt; 2.2e-16\n\n\nWe can also compute the predicted values by first generating a grid of values where we want to predict:\n\n# Create a prediction grid over the weeks of the years 1989 to 2000:\npred_data &lt;- expand.grid(Weeks = 1:53, year = 1989:2000)\npred_data$year_num &lt;- pred_data$year + pred_data$Weeks / 53\n\nThen we can use the augment function from the broom library to compute the model predictions for (i) the mean (i.e., \\(E(\\mu_i|Y_i) = \\beta_0 + \\beta_1 \\text{year} + \\gamma_1 \\sin\\left(\\frac{2\\pi \\text{week}}{p}\\right) + \\gamma_2 \\cos\\left(\\frac{2\\pi \\text{week}}{p}\\right)\\) ) or (ii) for new observations (i.e., including the uncertainty associated with the observational error \\(\\epsilon_i\\)) as follows:\n\n# Model predictions with confidence interval for the mean\n\npred_mean = broom::augment(x=harmonic_model,\n                     newdata = pred_data,\n                     interval = \"confidence\",\n                     type.predict = \"response\")\n\n# Model predictions with prediction interval for the new observations\n\npred_obs = broom::augment(x=harmonic_model,\n                     newdata = pred_data,\n                     interval = \"prediction\",\n                     type.predict = \"response\")\n\nWe can visualize the model predictions as follows:\n\nggplot() +\n  geom_line(data=SO2,aes(y=ln.SO2.,x=year_num),alpha=0.25)+\n  geom_ribbon(data= pred_obs,\n                aes(x=year_num,\n                    ymin = .lower,\n                    ymax = .upper,\n                    fill=\"Prediction Interval\"),\n                alpha = 0.15)+\n    geom_ribbon(data= pred_mean,\n                aes(x=year_num,\n                    ymin = .lower,\n                    ymax = .upper,\n                    fill=\"Confidence Interval\"),\n                alpha = 0.5)+\n  geom_line(data=pred_mean,\n            aes(y=.fitted,x=year_num),\n            color=\"tomato\")+\n  scale_fill_discrete(name=\"\")+\n  labs(y=expression(log~SO^2),x=\"Time\")\n\n\n\n\n\n\n\n\nThe plot shows the data (grey lines) and the fitted model predictions (as red lines). Then, the blue and red shaded regions indicate the prediction and confidence intervals respectively (notice how the prediction intervals is wider because it takes the observational error into account).\nHow well does this model fit the data? It seems to capture the trend well (following the general pattern in the data) and it seems to fit the seasonal pattern fairly well (having peaks and troughs at the same locations as in the data). However, we note that the model is not perfect: it does not, e.g., capture the higher peak in year 1996. Perhaps, a more flexible model like an additive model (a type of GAM) might be more appropriate?\nWe should always check our diagnostic plots when assessing a model:\n\nlayout(matrix(1:4, nrow = 2, byrow = TRUE))\nplot(harmonic_model)\n\n\n\n\n\n\n\n\n\nThe plot of residuals versus fitted values (top left) looks OK, since we don’t see any clear patterns here.\nThe Normal quantile-quantile (Q-Q) plot (top right) also looks OK, since most of the data points follow the line.\n\nWhat about temporal autocorrelation?\n\nacf(resid(harmonic_model))\n\n\n\n\n\n\n\n\n\nThe plot of the autocorrelation function (ACF; bottom) shows that many of the lower lags have values lying outwith the intervals (i.e. lying outwith the blue dashed lines). Therefore, there is autocorrelation in the residuals, and the assumptions of the model are not satisfied. Therefore, this model is not appropriate for the data. We could consider fitted an AR(1) model here to try to address this. (Our more flexible additive model proposed above may also be worth trying.)"
  },
  {
    "objectID": "lab_2.html#trend-detection-in-haddock-stocks",
    "href": "lab_2.html#trend-detection-in-haddock-stocks",
    "title": "Lab session 2",
    "section": "1.2 Trend detection in Haddock stocks",
    "text": "1.2 Trend detection in Haddock stocks\nWe will study data relating to the annual estimates of biomass for North Sea Haddock from 1963–2000.\nThe data are stored in haddock.dat, which contains the columns Year and Biomass.\n Download haddock data set \n\n# Read in data:\nhaddock.all &lt;- read.table(\"datasets/haddock.dat\",header=TRUE)\n# Remove the data from year 2001 onwards:\nhaddock.data &lt;- haddock.all[haddock.all$Year &lt;= 2000,]\n\n\n\n\n\n\n\n Task 2\n\n\n\nUsing ggplot produce a line plot that shows biomass (measured in thousands of tonnes) over the years\n\n\nTake hint\n\nWe can add a geom_line() layer to produce a line graph in ggplot.\n\n\n\n\nClick here to see the solution\n\n\nCode\nggplot(data=haddock.data,aes(x=Year,y=Biomass))+\n  geom_line()+\n  labs(y=\"Thousands of tonnes\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Question\n\n\n\nWhat does the plot produced in the previous task tells you?\n\n\nSee Solution\n\nThis plot shows that there is some kind of nonlinear trend in the data, with some higher values earlier on (around 1970) and then generally lower values later on. There seems to be higher variability for the time when there were some higher values.\n\n\n\n\n\n\n\n\n\n Task 3\n\n\n\nNotice that a log-transformation may be appropriate to address the large variability observed in the data. Plot the \\(\\log_{10}\\) of the biomass agianst years using ggplot, what do you notice?\n\n\nTake hint\n\nWe can use apply the log10 function directly to the variable(s) in the ggplot aes argument.\n\n\n\n\nClick here to see the solution\n\n\nCode\nggplot(data=haddock.data,aes(x=Year,y=log10(Biomass)))+\n  geom_line()+\n  labs(y=expression(paste(log[10], \"(1000 tonnes)\")))\n\n\n\n\n\n\n\n\n\n\n\n\nWe can fit a simple model to the log-transformed biomass (see previous task) below:\n\n# Linear trend model:\n\n## Set up data by creating a Time index 1:38 years\nhaddock.data$Time &lt;- haddock.data$Year-1962\n## Fit model:\ntrend.model0 &lt;- lm(log10(Biomass) ~ Time, data = haddock.data)\nsummary(trend.model0)\n\n\nCall:\nlm(formula = log10(Biomass) ~ Time, data = haddock.data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.46891 -0.06912  0.01352  0.09904  0.45118 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.631107   0.063576  41.385  &lt; 2e-16 ***\nTime        -0.016065   0.002842  -5.653 2.02e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1921 on 36 degrees of freedom\nMultiple R-squared:  0.4703,    Adjusted R-squared:  0.4556 \nF-statistic: 31.96 on 1 and 36 DF,  p-value: 2.021e-06\n\n\nThe p-value for Time is \\(&lt;0.05\\), so there is a statistically significant trend. Since the coefficient of this is \\(&lt;0\\) (and the p-value is \\(&lt;0.05\\)), this is a statistically significant decreasing trend.\nLet’s predict into the future:\n\n## Predict for years 2001 to 2010, i.e., time index 39:48\npred.t &lt;- data.frame(Time = 39:48)  \n\nNow we use the augment function from broom to get the model predictions:\n\ntrend_preds = broom::augment(x=trend.model0,\n                     newdata = pred.t,\n                     interval = \"prediction\",\n                     type.predict = \"response\")\n\n# This will allow a plot labelled with \"Year\"\ntrend_preds$Year &lt;- 1962 + trend_preds$Time \n\nLastly, we use ggplot to plot our predictions:\n\nggplot()+\n  geom_line(data=haddock.data,\n            aes(x=Year,y=log10(Biomass)),\n            alpha=0.25)+\n   geom_ribbon(data= trend_preds,\n                aes(x=Year,\n                    ymin = .lower,\n                    ymax = .upper),alpha=0.25)+\n  geom_line(data=trend_preds,aes(x=Year,y=.fitted),color=\"tomato\")\n\n\n\n\n\n\n\n\nSince we’re predicting for 1 to 10 years ahead, we can extract the 6th and 10th values:\n\ntrend_preds[c(6,10),]\n\n# A tibble: 2 × 5\n   Time .fitted .lower .upper  Year\n  &lt;int&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1    44    1.92   1.51   2.34  2006\n2    48    1.86   1.43   2.29  2010\n\n\nso we have:\n\nThe predicted log(biomass) in 2006 is 1.92, with a 95% prediction interval of (1.51, 2.34).\nThe predicted log(biomass) in 2010 is 1.86, with a 95% prediction interval of (1.43, 2.29).\n\nSince we are on the log scale, we can exponentiate to get predictions on the original scale:\n\nexp(trend_preds[c(6,10),2:4])\n\n# A tibble: 2 × 3\n  .fitted .lower .upper\n    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1    6.85   4.50  10.4 \n2    6.42   4.19   9.85\n\n\n\nThe predicted biomass in 2006 is 6.85, with a 95% prediction interval of (4.50, 10.42).\nThe predicted biomass in 2010 is 6.42, with a 95% prediction interval of (4.19, 9.85).\n\n\nlayout(matrix(1:4, nrow = 2, byrow = TRUE))\n# Check model diagnostic plots:\nplot(trend.model0, which = 1:4)\n\n\n\n\n\n\n\n\nThe plot of residuals versus fitted values (top left) shows a clear pattern. This means that the deterministic part of the model does not capture the patterns in the data (a problem!).\n\nlayout(matrix(1:3, nrow = 1, byrow = TRUE))\ne0 &lt;- resid(trend.model0)\n\nplot(haddock.data$Year, e0, type = \"l\", xlab = \"Year\", \n     ylab = \"Residual\", main = \"Residual time series\")\nabline(h = 0, lty=2)\nacf(e0,main=\"Residual ACF\")\npacf(e0,main=\"Residual PACF\")\n\n\n\n\n\n\n\n\nThe lag 1 coefficient is outwith the intervals, so we have statistically significant evidence of residual autocorrelation in the data. This shows that the model is not appropriate for the data. Let’s try a time series regression approach.\nTime series model\nLets begin with a simple AR(1) model:\n\nlibrary(forecast)\n\nar1_model &lt;- Arima(log10(haddock.data$Biomass), order=c(1,0,0),\n                   xreg = haddock.data$Time,\n                   include.constant = TRUE) # Include intercept\n\nsummary(ar1_model)\n\nSeries: log10(haddock.data$Biomass) \nRegression with ARIMA(1,0,0) errors \n\nCoefficients:\n         ar1  intercept     xreg\n      0.6141     2.5611  -0.0134\ns.e.  0.1405     0.1240   0.0054\n\nsigma^2 = 0.02539:  log likelihood = 17.2\nAIC=-26.4   AICc=-25.19   BIC=-19.85\n\nTraining set error measures:\n                      ME      RMSE       MAE         MPE    MAPE      MASE\nTraining set 0.008715057 0.1529264 0.1141291 -0.07817945 4.91856 0.8889311\n                  ACF1\nTraining set 0.1968254\n\n\nNow we can forecast into the future as follows:\n\nforecast_result &lt;- forecast::forecast(\n  ar1_model,\n  xreg = trend_preds$Time,\n  h = 10  # forecast horizon\n)\nautoplot(forecast_result)\n\n\n\n\n\n\n\n\nNotice that our predictions are added as a blue solid line, with\n\nDarker/lighter inner region indicating 80% prediction interval\nLighter/outer region indicating 95% prediction interval\n\nWe check the time series diagnostic plots:\n\nlayout(matrix(1:2, nrow = 1, byrow = TRUE))\n\n# Check model diagnostic plots:\ne1 &lt;- resid(ar1_model)\nacf(e1, main = \"Residual ACF for AR(1) model\")\npacf(e1, main = \"Residual PACF for AR(1) model\")\n\n\n\n\n\n\n\n\nThis looks much better, since the lag 1 coefficient is now within the confidence interval bounds, so we no longer have any statistically significant evidence of residual autocorrelation. Therefore, the AR(1) model appears to be appropriate for the data.\nWe should also check the other diagnostic plots:\n\nlayout(matrix(1:2, nrow = 2, byrow = TRUE))\n\nplot(resid(ar1_model), type = \"o\")\nqqnorm(resid(ar1_model))\nqqline(resid(ar1_model))\n\n\n\n\n\n\n\n\nThese do not look too bad — there is little remaining pattern in the plot of residuals versus time (left) and the points seem to follow the line fairly well on the Q-Q plot (right). therefore, this model appears to be appropriate for the data."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Module 2",
    "section": "",
    "text": "This module provides the theoretical foundation and practical skills for analysing temporal environmental and ecological data, with emphasis on quantifying ecological trends, detecting structural changes, and modelling extremes. Using contemporary environmental challenges as case studies, we will learn how to extract meaningful signals from noisy temporal data and make robust inferences about environmental change.\nWe will focus on methods that move beyond modelling central tendencies to explicitly quantify what happens at the tails of distributions—where many critical environmental phenomena occur. Floods, droughts, heatwaves, and other extremes define system boundaries, drive ecological transitions, and inform the design of critical infrastructure."
  },
  {
    "objectID": "index.html#lectures",
    "href": "index.html#lectures",
    "title": "Module 2",
    "section": "2.1 Lectures",
    "text": "2.1 Lectures\nThere will be two - 1 hr lectures per week at 42 Bute Gardens:916\n\n\n\n\n\n\nNote\n\n\n\nLectures will be recorded if the room’s technology allows them to be."
  },
  {
    "objectID": "index.html#tutorials",
    "href": "index.html#tutorials",
    "title": "Module 2",
    "section": "2.2 Tutorials",
    "text": "2.2 Tutorials\nIn addition, there will be four tutorials for this course. There are two tutorial groups - please check on MyCampus which one you are in.\n\nTutorial Group 1 - Monday 10amTutorial Group 2- Wednesday 12 noon\n\n\nTutorial groups:\n\nSTATS 4009 - TU01 (23738)\nSTATS 5031 - TU01 (24174)\n\nVenue:\nAdam Smith: 281\nTutorial dates:\n\n26-Jan-2026\n09-Feb-2026\n23-Feb-2026\n09-Mar-2026\n\n\n\nTutorial groups:\n\nSTATS 4009 - TU02 (23739)\nSTATS 5031 - TU02 (24175)\n\nVenue:\nJoseph Black Building:C407 Agricultm\nTutorial dates:\n\n28 -Jan-2026\n11-Feb-2026\n25-Feb-2026\n11-Mar-2026\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nYou are expected to have attempted the exercise sheets before the tutorial - they will be available in advance."
  },
  {
    "objectID": "index.html#labs",
    "href": "index.html#labs",
    "title": "Module 2",
    "section": "2.3 Labs",
    "text": "2.3 Labs\nThere will be three labs taking place in Boyd Orr Building:418 Lab from 15:00-17:00pm on the following dates (clicking on the date will direct you to the lab material):\n\nLab session 1 - Jan 30th\nLab session 2 - Feb 27th\nMarch 13th"
  },
  {
    "objectID": "reading_list.html",
    "href": "reading_list.html",
    "title": "Reading list",
    "section": "",
    "text": "Coles, S., Bawa, J., Trenner, L., & Dorazio, P. (2001). An introduction to statistical modeling of extreme values (Vol. 208, p. 208). London: Springer.\nCowpertwait, P. S., & Metcalfe, A. V. (2009). Introductory time series with R. Springer Science & Business Media.\nReeves, J., Chen, J., Wang, X. L., Lund, R., & Lu, Q. Q. (2007). A review and comparison of changepoint detection techniques for climate data. Journal of applied meteorology and climatology, 46(6), 900-915.\nSharma, S., Swayne, D. A., & Obimbo, C. (2016). Trend analysis and change point techniques: a survey. Energy, ecology and environment, 1(3), 123-130.\nShunway, R. H., & Stoffer, D. S. (2019). Time series: a data analysis approach using R. Chapman and Hall/CRC.\nWood, S. N. (2017). Generalized additive models: an introduction with R. chapman and hall/CRC."
  },
  {
    "objectID": "notes/tutorial_sheet_2.html",
    "href": "notes/tutorial_sheet_2.html",
    "title": "Tutorial Sheet 2",
    "section": "",
    "text": "This week we will focus on a reading group to discuss the following paper:\n\nLinder, H. L., & Horne, J. K. (2018). Evaluating statistical models to measure environmental change: A tidal turbine case study. Ecological Indicators, 84, 765-792.\n\nThe paper can be accesses through the following DOI: https://doi.org/10.1016/j.ecolind.2017.09.041\nOr downloaded using the following link:\n\n\n Download Linder & Horne (2018) paper \nThis week, we will focus on how to read a methodological paper in applied ecological & environmental statistics. The Goal of the Session is to evaluate and critique the statistical methods, results & recommendations presented in a scientific paper related to an environmental problem. We will focus on understanding the author’s framework, assessing the practical implications of their findings, and identifying the validity of the conclusions drawn from the study.\n\n\n\n\n\n\nNote\n\n\n\nThis tutorial is designed as direct practice for your final exam, which will require you to write a critical essay on a given scientific topic. The skills practiced here (e.g., deconstructing a paper’s core argument, evaluating its methodology, and synthesizing its contributions) are precisely the skills that will help you with that task. Think of this as a collaborative training session.\n\n\n\n\nYour Preparation (Please complete BEFORE the session):\n\nRead Strategically: Don’t get bogged down in every statistical detail on the first pass. Focus on understanding the narrative.\n\nAbstract & Introduction: What is the context of the problem? What is the core problem they are solving, and why is it important?\nSection 2 (Data): Why have the authors selected this specific case study? How is data been collected and can you identify any potential sources of bias?\nSection 2 (Methods): What models have the authors used and why? how are these methods being compared? Understand the principles of the intervention analysis and how the model’s ability to detect change has been measured? Can you think of any caveats on the usage of these metrics?\nSection 3 (Results): Focus on the take-home messages from tables and figures. How would you summaries the paper key findings?\nDiscussion & Conclusion: What do the authors claim is their key contribution? What are the broader implications? Have you identified any limitations with the study?\n\nTake Notes on These Four Key Questions:\n\nThe Core Problem: In your own words, what is the “gap” in standard monitoring practices that this paper addresses?\nThe Evaluation Framework: How did the authors test the models? What were the criteria for “best”?\nMain Recommendation: What is the “best practice” they propose, and does it vary by objective (detect, quantify, forecast)?\nYour Critical Assessment: What is one major strength of this study’s approach? What is one potential limitation or remaining question you have?\n\nBring: Your annotated copy of the paper and your prepared notes.\n\n\n\n\nActivity Goal: To collaboratively break down the paper’s core components, evaluate its methodological framework, and share your insights with the whole class.\nWork in small groups to tackle key questions about the paper’s argument and methods. Your tutor will lead the session by posing specific questions, giving your group time to discuss, and then facilitating a class-wide conversation to compare insights."
  },
  {
    "objectID": "notes/tutorial_sheet_2.html#pre-session-work",
    "href": "notes/tutorial_sheet_2.html#pre-session-work",
    "title": "Tutorial Sheet 2",
    "section": "",
    "text": "Your Preparation (Please complete BEFORE the session):\n\nRead Strategically: Don’t get bogged down in every statistical detail on the first pass. Focus on understanding the narrative.\n\nAbstract & Introduction: What is the context of the problem? What is the core problem they are solving, and why is it important?\nSection 2 (Data): Why have the authors selected this specific case study? How is data been collected and can you identify any potential sources of bias?\nSection 2 (Methods): What models have the authors used and why? how are these methods being compared? Understand the principles of the intervention analysis and how the model’s ability to detect change has been measured? Can you think of any caveats on the usage of these metrics?\nSection 3 (Results): Focus on the take-home messages from tables and figures. How would you summaries the paper key findings?\nDiscussion & Conclusion: What do the authors claim is their key contribution? What are the broader implications? Have you identified any limitations with the study?\n\nTake Notes on These Four Key Questions:\n\nThe Core Problem: In your own words, what is the “gap” in standard monitoring practices that this paper addresses?\nThe Evaluation Framework: How did the authors test the models? What were the criteria for “best”?\nMain Recommendation: What is the “best practice” they propose, and does it vary by objective (detect, quantify, forecast)?\nYour Critical Assessment: What is one major strength of this study’s approach? What is one potential limitation or remaining question you have?\n\nBring: Your annotated copy of the paper and your prepared notes."
  },
  {
    "objectID": "notes/tutorial_sheet_2.html#in-person-group-activity-paper-discussion-critical-review",
    "href": "notes/tutorial_sheet_2.html#in-person-group-activity-paper-discussion-critical-review",
    "title": "Tutorial Sheet 2",
    "section": "",
    "text": "Activity Goal: To collaboratively break down the paper’s core components, evaluate its methodological framework, and share your insights with the whole class.\nWork in small groups to tackle key questions about the paper’s argument and methods. Your tutor will lead the session by posing specific questions, giving your group time to discuss, and then facilitating a class-wide conversation to compare insights."
  },
  {
    "objectID": "notes/tutorial_sheet_2_solutions.html",
    "href": "notes/tutorial_sheet_2_solutions.html",
    "title": "Tutorial Sheet 2",
    "section": "",
    "text": "This week we will focus on a reading group to discuss the following paper:\n\nLinder, H. L., & Horne, J. K. (2018). Evaluating statistical models to measure environmental change: A tidal turbine case study. Ecological Indicators, 84, 765-792.\n\nThe paper can be accesses through the following DOI: https://doi.org/10.1016/j.ecolind.2017.09.041\nOr downloaded using the following link:\n\n\n Download Linder & Horne (2018) paper \nThis week, we will focus on how to read a methodological paper in applied ecology. The Goal of the Session is to evaluate and critique the statistical methods presented in a scientific paper related to an environmental problem. We will focus on understanding the author’s framework, assessing the practical implications of their findings, and identifying the validity of the conclusions drawn from the study.\n\n\n\n\n\n\nNote\n\n\n\nThis tutorial is designed as direct practice for your final exam, which will require you to write a critical essay on a given scientific topic. The skills practiced here (e.g., deconstructing a paper’s core argument, evaluating its methodology, and synthesizing its contributions) are precisely the skills that will help you with that task. Think of this as a collaborative training session.\n\n\n\n\nYour Preparation (Please complete BEFORE the session):\n\nRead Strategically: Don’t get bogged down in every statistical detail on the first pass. Focus on understanding the narrative.\n\nAbstract & Introduction: What is the context of the problem? What is the core problem they are solving, and why is it important?\nSection 2 (Data): Why have the authors selected this specific case study? How is data been collected and can you identify any potential sources of bias?\nSection 2 (Methods): What models have the authors used and why? how are these methods being compared? Understand the principles of the intervention analysis and how the model’s ability to detect change has been measured? Can you think of any caveats on the usage of these metrics?\nSection 3 (Results): Focus on the take-home messages from tables and figures. How would you summaries the paper key findings?\nDiscussion & Conclusion: What do the authors claim is their key contribution? What are the broader implications? Have you identified any limitations with the study?\n\nTake Notes on These Four Key Questions:\n\nThe Core Problem: In your own words, what is the “gap” in standard monitoring practices that this paper addresses?\nThe Evaluation Framework: How did the authors test the models? What were the criteria for “best”?\nMain Recommendation: What is the “best practice” they propose, and does it vary by objective (detect, quantify, forecast)?\nYour Critical Assessment: What is one major strength of this study’s approach? What is one potential limitation or remaining question you have?\n\nBring: Your annotated copy of the paper and your prepared notes.\n\n\n\n\nActivity Goal: To collaboratively break down the paper’s core components, evaluate its methodological framework, and share your insights with the whole class.\nWork in small groups to tackle key questions about the paper’s argument and methods. Your tutor will lead the session by posing specific questions, giving your group time to discuss, and then facilitating a class-wide conversation to compare insights.\n\n\n\n\nAsk the students to work on small groups (e.g, 4-5 students per group)\nBegin the warmo-up session with a small group discussion and ask the students about what the found most interesting about the paper - give them around 5 minutes to discuss and then ask each group to present their answers.\nFor the following part each group will discuss different sections of the paper. This is a rough plan for you to guide this discussion but it could (and probably will) be adapted based on the discussion student have. Please spend some time discussing some of the ideas with each group\n\n\n\nTotal session: 60minutes\n\nIntro & warm-up: 5-10 min\nGroup work: 25-30 min\nGroup presentations: 25 min per group (5 min per group approx)\n\nEach group should prepare:\n\n2-3 key insights to share with the class.\n1 remaining question or uncertainty\nA visual aid (sketch/diagram on whiteboard or paper) explaining their section of the paper.\n\n\n\n\nThe number of groups depends on the number of students so there could me more than one group working on a specific topic.\n\n\n\nDuring group work: - Rotate between groups, listen first before intervening - Ask probing questions: “Why do you think that?” “What evidence supports that?” - Try that all students participate\nDuring presentations: - Connect different groups’ insights: “Group 2’s point about simulation relates to what Group 4 found…” - Synthesize conflicting interpretations - Highlight particularly insightful critiques\n\n\n\n\n\n\nNote\n\n\n\n\nFor students who finish early: - You can ask the to compare this paper’s methods to another paper they’ve read or that is cited on the same paper. Ask them to sketch how they’d apply these methods if they were hired for example as statistics consultants - ask them to identify one ore more concept you’d like to learn more about.\nFor advanced groups: Challenge them to identify what’s missing from the paper\nFor struggling groups: Provide more directed questions or a summary template\nMixed-skill groups: Assign roles within groups (summarizer, questioner, connector, presenter)\n\n\n\n\n\n\n\nFor quiet groups: Use think-pair-share or assign specific speaking roles\nFor fast groups: Add extension questions about broader implications\nFor technical confusion: Clarify one key concept, then let them apply it\n\n\n\n\nYou can also consider a “jigsaw” approach if things are moving fast where:\n\nOriginal groups become experts on their section\nRegroup so new groups have one expert from each section\nExperts teach their section to their new group\nThen each group can take on a “consultancy role” scenario. E.g., tell the students : “You’re a consulting firm hired to evaluate this research for a client”\n\nThen each member of the team is assingned a role:\n\nProject Manager (oversees)\nStatistician (methods expert)\nSubject Expert (domain knowledge)\nCommunications Officer (presentation)\n\n\nThen, on the Consultation Phase each student analyze paper from their role’s perspective.\nFinally, students finish the session with a “Client Meeting” where they present their findings to the whole group.\n\nTime structure for jigsaw activity:\n\nExpert groups: 15 min approx\nTeaching groups & Consultancy role-play: 15 min approx\nClient meeting/group presentation: 25 (5 min per group approx)\n\n\n\n\nHere are some of examples of the focus- groups:\n\n\n\n\n\n\nExample: Group 1\n\n\n\nTopic: General summary of the paper, background and data.\nStudents should provide (i) an overall summary of the paper highlighting the main findings and (ii) discuss the context of the problem in detail (i.e. sections 1 & 2.1 , 2.2) . Questions to motivate the discussion can be:\n\nWhat is the purpose of the study?\nWhat is the motivation behind the case study the authors presented and is the explanation about the data collection process clear? Can you think of any potential sources of bias that the original paper has not considered?\nIs the choice of statistical methods well-justified? Would you do things differently?\n\n\n\n\n\n\n\n\n\nExample: Group 2\n\n\n\nTopic: Simulated scenarios.\nStudents should discuss the baseline simulation models and change scenarios (sections 2.3 thru 2.6) . Questions to motivate the discussion can be:\n\nIs the explanation about the baseline simulation models clear? discuss how authors have chosen the amount of observation error .\nPerformance of candidate models has been tested using “Before-After simulated datasets” - what problem you might face in a real-world data scenario? If you had unlimited resources would you propose a different design? (e.g., student can discuss BACI)\nExplain and discuss Table 1 in detail.\n\n\n\n\n\n\n\n\n\nExample: Group 3\n\n\n\nTopic: Modelling approaches\nStudents should discuss the candidate models to detect change and how are they being assessed (sections 2.7 thru 2.10) . Questions to motivate the discussion can be:\n\nAre the choice of statistical model well-justified? Discuss how the authors have presented the different candidate models, do you think is a clear manner of explaining this?\nHow would the Intervention analysis presented in section 2.8. to a non-statistician (e.g., the general public)\nExplain the power analysis that was conducted to evaluate model ability to detect change.\nSummarise the different model fit and forecast accuracy metrics used by the authors.\n\n\n\n\n\n\n\n\n\nExample: Group 4\n\n\n\nTopic: Results\nStudents should discuss the results of the paper (section 3) . Questions to motivate the discussion can be:\n\nWhat are the main findings reported on this section?\nStudents should explain to their classmates the figures presented in this section and what are the implications for the study.\nThen, ask them how would they communicate these results to the general public? Imagine they are working for a consultancy company and they have to explain this to a group of environmental scientists and conservationists.\n\n\n\n\n\n\n\n\n\nExample: Group 5\n\n\n\nTopic: Discussion and recommendations\nStudents should discuss Section 4 & 5 . Questions to motivate the discussion can be:\n\nSummarise the main points the authors have discussed? What have the authors recommended on each scenario and why? Do you agree with the authors recommendation?\nDiscuss figure 9. what do you think of it? would you presented the recommended models differently?\nHow valid are the conclusions drawn from this study? Can you think a situation in which authors recommendation would be difficult or even impossible to implement?"
  },
  {
    "objectID": "notes/tutorial_sheet_2_solutions.html#pre-session-work",
    "href": "notes/tutorial_sheet_2_solutions.html#pre-session-work",
    "title": "Tutorial Sheet 2",
    "section": "",
    "text": "Your Preparation (Please complete BEFORE the session):\n\nRead Strategically: Don’t get bogged down in every statistical detail on the first pass. Focus on understanding the narrative.\n\nAbstract & Introduction: What is the context of the problem? What is the core problem they are solving, and why is it important?\nSection 2 (Data): Why have the authors selected this specific case study? How is data been collected and can you identify any potential sources of bias?\nSection 2 (Methods): What models have the authors used and why? how are these methods being compared? Understand the principles of the intervention analysis and how the model’s ability to detect change has been measured? Can you think of any caveats on the usage of these metrics?\nSection 3 (Results): Focus on the take-home messages from tables and figures. How would you summaries the paper key findings?\nDiscussion & Conclusion: What do the authors claim is their key contribution? What are the broader implications? Have you identified any limitations with the study?\n\nTake Notes on These Four Key Questions:\n\nThe Core Problem: In your own words, what is the “gap” in standard monitoring practices that this paper addresses?\nThe Evaluation Framework: How did the authors test the models? What were the criteria for “best”?\nMain Recommendation: What is the “best practice” they propose, and does it vary by objective (detect, quantify, forecast)?\nYour Critical Assessment: What is one major strength of this study’s approach? What is one potential limitation or remaining question you have?\n\nBring: Your annotated copy of the paper and your prepared notes."
  },
  {
    "objectID": "notes/tutorial_sheet_2_solutions.html#in-person-group-activity-paper-discussion-critical-review",
    "href": "notes/tutorial_sheet_2_solutions.html#in-person-group-activity-paper-discussion-critical-review",
    "title": "Tutorial Sheet 2",
    "section": "",
    "text": "Activity Goal: To collaboratively break down the paper’s core components, evaluate its methodological framework, and share your insights with the whole class.\nWork in small groups to tackle key questions about the paper’s argument and methods. Your tutor will lead the session by posing specific questions, giving your group time to discuss, and then facilitating a class-wide conversation to compare insights."
  },
  {
    "objectID": "notes/tutorial_sheet_2_solutions.html#for-tutors",
    "href": "notes/tutorial_sheet_2_solutions.html#for-tutors",
    "title": "Tutorial Sheet 2",
    "section": "",
    "text": "Ask the students to work on small groups (e.g, 4-5 students per group)\nBegin the warmo-up session with a small group discussion and ask the students about what the found most interesting about the paper - give them around 5 minutes to discuss and then ask each group to present their answers.\nFor the following part each group will discuss different sections of the paper. This is a rough plan for you to guide this discussion but it could (and probably will) be adapted based on the discussion student have. Please spend some time discussing some of the ideas with each group\n\n\n\nTotal session: 60minutes\n\nIntro & warm-up: 5-10 min\nGroup work: 25-30 min\nGroup presentations: 25 min per group (5 min per group approx)\n\nEach group should prepare:\n\n2-3 key insights to share with the class.\n1 remaining question or uncertainty\nA visual aid (sketch/diagram on whiteboard or paper) explaining their section of the paper.\n\n\n\n\nThe number of groups depends on the number of students so there could me more than one group working on a specific topic.\n\n\n\nDuring group work: - Rotate between groups, listen first before intervening - Ask probing questions: “Why do you think that?” “What evidence supports that?” - Try that all students participate\nDuring presentations: - Connect different groups’ insights: “Group 2’s point about simulation relates to what Group 4 found…” - Synthesize conflicting interpretations - Highlight particularly insightful critiques\n\n\n\n\n\n\nNote\n\n\n\n\nFor students who finish early: - You can ask the to compare this paper’s methods to another paper they’ve read or that is cited on the same paper. Ask them to sketch how they’d apply these methods if they were hired for example as statistics consultants - ask them to identify one ore more concept you’d like to learn more about.\nFor advanced groups: Challenge them to identify what’s missing from the paper\nFor struggling groups: Provide more directed questions or a summary template\nMixed-skill groups: Assign roles within groups (summarizer, questioner, connector, presenter)\n\n\n\n\n\n\n\nFor quiet groups: Use think-pair-share or assign specific speaking roles\nFor fast groups: Add extension questions about broader implications\nFor technical confusion: Clarify one key concept, then let them apply it\n\n\n\n\nYou can also consider a “jigsaw” approach if things are moving fast where:\n\nOriginal groups become experts on their section\nRegroup so new groups have one expert from each section\nExperts teach their section to their new group\nThen each group can take on a “consultancy role” scenario. E.g., tell the students : “You’re a consulting firm hired to evaluate this research for a client”\n\nThen each member of the team is assingned a role:\n\nProject Manager (oversees)\nStatistician (methods expert)\nSubject Expert (domain knowledge)\nCommunications Officer (presentation)\n\n\nThen, on the Consultation Phase each student analyze paper from their role’s perspective.\nFinally, students finish the session with a “Client Meeting” where they present their findings to the whole group.\n\nTime structure for jigsaw activity:\n\nExpert groups: 15 min approx\nTeaching groups & Consultancy role-play: 15 min approx\nClient meeting/group presentation: 25 (5 min per group approx)\n\n\n\n\nHere are some of examples of the focus- groups:\n\n\n\n\n\n\nExample: Group 1\n\n\n\nTopic: General summary of the paper, background and data.\nStudents should provide (i) an overall summary of the paper highlighting the main findings and (ii) discuss the context of the problem in detail (i.e. sections 1 & 2.1 , 2.2) . Questions to motivate the discussion can be:\n\nWhat is the purpose of the study?\nWhat is the motivation behind the case study the authors presented and is the explanation about the data collection process clear? Can you think of any potential sources of bias that the original paper has not considered?\nIs the choice of statistical methods well-justified? Would you do things differently?\n\n\n\n\n\n\n\n\n\nExample: Group 2\n\n\n\nTopic: Simulated scenarios.\nStudents should discuss the baseline simulation models and change scenarios (sections 2.3 thru 2.6) . Questions to motivate the discussion can be:\n\nIs the explanation about the baseline simulation models clear? discuss how authors have chosen the amount of observation error .\nPerformance of candidate models has been tested using “Before-After simulated datasets” - what problem you might face in a real-world data scenario? If you had unlimited resources would you propose a different design? (e.g., student can discuss BACI)\nExplain and discuss Table 1 in detail.\n\n\n\n\n\n\n\n\n\nExample: Group 3\n\n\n\nTopic: Modelling approaches\nStudents should discuss the candidate models to detect change and how are they being assessed (sections 2.7 thru 2.10) . Questions to motivate the discussion can be:\n\nAre the choice of statistical model well-justified? Discuss how the authors have presented the different candidate models, do you think is a clear manner of explaining this?\nHow would the Intervention analysis presented in section 2.8. to a non-statistician (e.g., the general public)\nExplain the power analysis that was conducted to evaluate model ability to detect change.\nSummarise the different model fit and forecast accuracy metrics used by the authors.\n\n\n\n\n\n\n\n\n\nExample: Group 4\n\n\n\nTopic: Results\nStudents should discuss the results of the paper (section 3) . Questions to motivate the discussion can be:\n\nWhat are the main findings reported on this section?\nStudents should explain to their classmates the figures presented in this section and what are the implications for the study.\nThen, ask them how would they communicate these results to the general public? Imagine they are working for a consultancy company and they have to explain this to a group of environmental scientists and conservationists.\n\n\n\n\n\n\n\n\n\nExample: Group 5\n\n\n\nTopic: Discussion and recommendations\nStudents should discuss Section 4 & 5 . Questions to motivate the discussion can be:\n\nSummarise the main points the authors have discussed? What have the authors recommended on each scenario and why? Do you agree with the authors recommendation?\nDiscuss figure 9. what do you think of it? would you presented the recommended models differently?\nHow valid are the conclusions drawn from this study? Can you think a situation in which authors recommendation would be difficult or even impossible to implement?"
  },
  {
    "objectID": "notes/notes_1.html",
    "href": "notes/notes_1.html",
    "title": "Assessing Change Over Time",
    "section": "",
    "text": "A time series is a sequence of measurements on the same object made over time. For example, we might measure the level of carbon dioxide (\\(\\text{CO}_2\\)) in a town every day for a year. The purpose of making such measurements is to understand how our variable of interest has changed over time. For example, a government would be keen to know if air pollution levels are getting better or worse, or a conservationist might want to identify declining trends in wildlife populations to better target their conservation objectives.\nWe can write our set of time series data as \\(y_1, \\ldots, y_T,\\) where \\(y_i\\) is the observation at time point \\(i\\), and \\(T\\) is the total number of observations. Time series data are typically not independent, since there will often be correlation between consecutive observations. This dependency structure must be taken into account when modelling.\n\n\n\n\n\n\n Exercise 1\n\n\n\nCan you spot any dependency structure shown in the below plot of log(flow) in the River Dee?\n\n\nLog(flow) data for River Dee.\n\n\n\nSolution\n\nThere is a clear structure to the data. It looks like there is a repeated pattern, once per year. (This is a “seasonal pattern” and we’ll go into more details about this later in this section of the course.)\n\n\n\n\n\n\n\n\n\n Example: Mauna Loa\n\n\n\nMauna Loa in Hawaii is one of the biggest and most active volcanoes in the world. \\(\\text{CO}_2\\) levels have been monitored since 1958. Mauna Loa is one of the first sites worldwide where increasing \\(\\text{CO}_2\\) levels were identified.\n\nWe can observe a clear trend, and also a seasonal pattern. It may be sensible to standardise the data and represent all observations in terms of ‘anomalies’, i.e., their deviation from the starting point (1960 mean level).\n\n\n\n\n\n\n\n\n\n\n\n\n Example: Global temperature\n\n\n\nThe plot below shows the global temperature anomaly (the current value compared to the average from 1951–1980).\n\n\n\n\nSource: https://climate.nasa.gov/vital-signs/global-temperature/\n\n\n\n\n\n\n\n\n Exercise 2\n\n\n\nHow would you describe the change in global temperature shown in the above plot?\nWe will discuss this in the lecture."
  },
  {
    "objectID": "notes/notes_1.html#introduction",
    "href": "notes/notes_1.html#introduction",
    "title": "Assessing Change Over Time",
    "section": "",
    "text": "A time series is a sequence of measurements on the same object made over time. For example, we might measure the level of carbon dioxide (\\(\\text{CO}_2\\)) in a town every day for a year. The purpose of making such measurements is to understand how our variable of interest has changed over time. For example, a government would be keen to know if air pollution levels are getting better or worse, or a conservationist might want to identify declining trends in wildlife populations to better target their conservation objectives.\nWe can write our set of time series data as \\(y_1, \\ldots, y_T,\\) where \\(y_i\\) is the observation at time point \\(i\\), and \\(T\\) is the total number of observations. Time series data are typically not independent, since there will often be correlation between consecutive observations. This dependency structure must be taken into account when modelling.\n\n\n\n\n\n\n Exercise 1\n\n\n\nCan you spot any dependency structure shown in the below plot of log(flow) in the River Dee?\n\n\nLog(flow) data for River Dee.\n\n\n\nSolution\n\nThere is a clear structure to the data. It looks like there is a repeated pattern, once per year. (This is a “seasonal pattern” and we’ll go into more details about this later in this section of the course.)\n\n\n\n\n\n\n\n\n\n Example: Mauna Loa\n\n\n\nMauna Loa in Hawaii is one of the biggest and most active volcanoes in the world. \\(\\text{CO}_2\\) levels have been monitored since 1958. Mauna Loa is one of the first sites worldwide where increasing \\(\\text{CO}_2\\) levels were identified.\n\nWe can observe a clear trend, and also a seasonal pattern. It may be sensible to standardise the data and represent all observations in terms of ‘anomalies’, i.e., their deviation from the starting point (1960 mean level).\n\n\n\n\n\n\n\n\n\n\n\n\n Example: Global temperature\n\n\n\nThe plot below shows the global temperature anomaly (the current value compared to the average from 1951–1980).\n\n\n\n\nSource: https://climate.nasa.gov/vital-signs/global-temperature/\n\n\n\n\n\n\n\n\n Exercise 2\n\n\n\nHow would you describe the change in global temperature shown in the above plot?\nWe will discuss this in the lecture."
  },
  {
    "objectID": "notes/notes_1.html#ecological-trend",
    "href": "notes/notes_1.html#ecological-trend",
    "title": "Assessing Change Over Time",
    "section": "\n2 Ecological Trend",
    "text": "2 Ecological Trend\nThe purpose of time series modelling is to identify any trends which exist in the dataset. But what exactly is a trend? It depends who you ask.\nThe Joint Nature Conservation Council (JNCC) define it as: a measurement of change derived from a comparison of the results of two or more statistics. This is often considered to be the ecological definition of trend, i.e., a change (in terms of percentage or some index) between two timepoints."
  },
  {
    "objectID": "notes/notes_1.html#statistical-trend",
    "href": "notes/notes_1.html#statistical-trend",
    "title": "Assessing Change Over Time",
    "section": "\n3 Statistical Trend",
    "text": "3 Statistical Trend\nIn statistics, the definition of a trend is often more wide-ranging:\n\nA long-term change in the mean level (Chatfield, 1996)\nLong-term movement (Kendall and Ord, 1990)\nLong-term behaviour of the process (Chandler, 2002)\nThe non-random function \\(\\mu(t) = E(Y(t))\\) (Diggle, 1990)\n\nWe may be interested in trends in mean, variance or extreme values. Trends are not limited to linear or monotonic patterns.\n\n3.1 Simple Linear Trend\nWe can represent a simple linear trend using the standard notation: \\[Y_t = \\beta_0 + \\beta_1x_t + \\epsilon_t.\\]\nHere, \\(\\beta_0\\) is an intercept and \\(\\beta_1\\) represents the slope (trend). This is just a standard linear model, with all the usual assumptions (normality, constant variance, independence). This model therefore does not account for any seasonality or autocorrelation in our data.\n\n\n\n\n\n\n Example: Chlorophyll levels in a lake\n\n\n\nWe observe monthly chlorophyll levels in a lake between 2001 and 2006.\nWe can fit a linear model of the form: \\[\\text{Log Chlorophyll} = \\beta_0 + \\beta_1 \\text{ Date} + \\text{error}\\]"
  },
  {
    "objectID": "notes/notes_1.html#seasonal-patterns",
    "href": "notes/notes_1.html#seasonal-patterns",
    "title": "Assessing Change Over Time",
    "section": "\n4 Seasonal Patterns",
    "text": "4 Seasonal Patterns\nMany environmental time series have some sort of periodicity (e.g. a monthly pattern in temperature). We can produce some form of seasonality plot to understand this better. The period is the time interval between consecutive peaks or troughs. A seasonal component of a dataset is a regular fluctuation with a period of one year or less.\n\n\n\n\n\n\n Example: Mean surface water temperature in Lake Nam\n\n\n\nLake Nam (Namtso) is a mountain lake in Tibet. The mean surface water temperature was measured monthly between 1996 and 2011.\n\n\n\n\nWe can plot the data over time, showing clear pattern in the data:\n\n\n\n\nWe should therefore plot the data by month. Doing so indicates a clear seasonal pattern. There is a peak in Month 3 and a trough in Months 9/10:"
  },
  {
    "objectID": "notes/notes_1.html#harmonic-regression",
    "href": "notes/notes_1.html#harmonic-regression",
    "title": "Assessing Change Over Time",
    "section": "\n5 Harmonic Regression",
    "text": "5 Harmonic Regression\nThe monthly pattern is very similar to a sine wave, and we can use this feature in our modelling. This is known as harmonic regression, and is suitable when we have a regular seasonal trend (as we just saw in the Lake Nam example).\n\n\n\n\nHarmonic regression is based on an equation of the form\n\\[Y_t = \\beta_0 + \\gamma \\sin\\left(\\frac{2\\pi [u_t - \\theta]}{p}\\right) + \\epsilon_t\\]\nHere, \\(\\gamma\\) is the amplitude of the wave, \\(p\\) is the period of the wave, and \\(\\theta\\) represents the ‘position’ on the wave (in radians). However, it can often be more convenient to rewrite this in the form of a simple multiple regression model, taking advantage of the double angle formula.\nGiven that \\(\\sin(a-b) = \\sin(a)\\cos(b) - \\cos(a)\\sin(b)\\), we can show that:\n\\[\n\\begin{aligned}\n\\gamma \\sin\\left(\\frac{2\\pi [u_t - \\theta]}{p}\\right) &= \\gamma \\sin\\left(\\frac{2\\pi u_t}{p} - \\frac{2\\pi\\theta}{p}\\right)\\\\\n&= \\gamma \\left[ \\sin\\left(\\frac{2\\pi u_t}{p}\\right)\\cos\\left(\\frac{2\\pi\\theta}{p}\\right) - \\cos\\left(\\frac{2\\pi u_t}{p}\\right)\\sin\\left(\\frac{2\\pi\\theta}{p}\\right)\\right]\n\\end{aligned}\n\\]\nSince \\(\\pi\\), \\(\\theta\\) and \\(p\\) are known, we can create new regression parameters \\(\\gamma_1 = \\gamma\\cos\\left(\\frac{2\\pi\\theta}{p}\\right)\\) and \\(\\gamma_2 = - \\gamma\\sin\\left(\\frac{2\\pi\\theta}{p}\\right)\\)\nThe final harmonic regression model can thus be written:\n\\[Y_t = \\beta_0 + \\gamma_1 \\sin\\left(\\frac{2\\pi u_t}{p}\\right) + \\gamma_2 \\cos\\left(\\frac{2\\pi u_t}{p}\\right) + \\epsilon_t\\]\nOur new parameters \\(\\gamma_1\\) and \\(\\gamma_2\\) control the seasonal trends, with \\(p\\) representing the period. \\(\\beta_0\\) is still the intercept term, which can also be interpreted as the overall mean. Note that this is still a linear model, since it is linear in the coefficients.\nThe standard harmonic regression assumes we have the same seasonal pattern each year, but this may not always be appropriate. There are many more sophisticated models available if this assumption does not hold. Some are still based on sine and cosine waves, while others may use autocorrelation functions or a form of semiparametric smoothing."
  },
  {
    "objectID": "notes/notes_1.html#time-series-model",
    "href": "notes/notes_1.html#time-series-model",
    "title": "Assessing Change Over Time",
    "section": "\n6 Time Series Model",
    "text": "6 Time Series Model\nThe seasonal variation can sometimes be so strong that it obscures the overall trend (or any other patterns). In most cases, we are not actually particularly interested in actually knowing about the seasonal trend. In these cases, it is simply a nuisance factor that we need to account for in our model. Our primary interest is usually in understanding the longer term trends in our data.\nWe often try to remove or separate out this seasonal pattern when analysing time series. We can therefore think of our overall time series model in the following form:\n\\[X = \\text{trend} + \\text{seasonal component} + \\text{error}\\]\nIn terms of mathematical notation, we can write this as\n\\[X_t = m_t + s_t + \\epsilon_t.\\] Our error, \\(\\epsilon_t\\) is assumed to be random, and follows the distribution \\(\\epsilon_t \\sim \\text{Normal}(0, \\sigma^2)\\)."
  },
  {
    "objectID": "notes/notes_1.html#estimating-trend",
    "href": "notes/notes_1.html#estimating-trend",
    "title": "Assessing Change Over Time",
    "section": "\n7 Estimating Trend",
    "text": "7 Estimating Trend\nWe have now identified a method for isolating the trend in our model. However, we still have to work out the best way to explore and understand this trend. We want to know the size of the trend, but also have to assess whether it is linear, and also test for statistical significance.\nA variety of models and techniques exist for exploring our trend.\n\n\n\n\n\n\n Example: Bird populations\n\n\n\nWe have collected annual data on the population of two birds between 1975 and 2005. What are the trends? Are they significantly different from zero?\n\n\n\n\nWe have fitted two models to attempt to assess the trends for each bird. The blue line is a linear regression, while the red line is a more flexible additive model.\n\n\n\n\n\n\n\n\n\n\n\n\n Exercise 3\n\n\n\nWhich of the models are more appropriate? Have we adequately captured the patterns in the data?\n\n\nSolution\n\nNeither relationship appears to be linear, so that the additive models are more appropriate than the linear regression models here. However, both models fail to capture the peaks in the data well, so that we could consider whether other models are more appropriate. E.g., models for extremes may be more appropriate if we are interested in the peaks.\n\n\n\nIn our bird population example, both models indicate the overall trend, but they do not test for significance. We therefore cannot be sure whether the changes are ‘genuine’ or are simply down to random variation. We can use non-parametric approaches (e.g., Mann-Kendall test and the Seasonal Kendall) to assess the trend in our data.\n\n7.1 Mann-Kendall test\nThe Mann-Kendall test is commonly used to detect trends in environmental, climate, and hydrological data. It looks for a consistent increase or decrease in a trend over time (not necessarily linear). It is commonly used for short time series, where we may not have sufficient data for more sophisticated approaches.\nAssume we have an ordered dataset \\(z_1, \\ldots, z_T\\)\n\nCompute ALL possible differences \\(d = z_j - z_k\\) where \\(j&gt;k\\).\nCreate an indicator function \\(\\text{sign}(z_j - z_k)\\) such that:\n\n\\[\n\\begin{aligned}\n\\text{sign}(z_j - z_k) =\n\\begin{cases}\n1 & \\text{if } z_j - z_k &gt; 0 \\\\\n0 & \\text{if } z_j - z_k = 0 \\\\\n-1 & \\text{if } z_j - z_k &lt; 0\n\\end{cases}\n\\end{aligned}\n\\]\n\nThe Mann-Kendall statistic, \\(S\\), is given by \\[S = \\sum_{k=1}^{n-1}\\sum_{j=k+1}^n \\text{sign}(z_j - z_k)\\]\n\n\nOur test statistic measures the size and direction of the trend:\n\nA positive value of \\(S\\) suggests the data are increasing over time (an upward trend).\nA negative value of \\(S\\) suggests a downward trend.\n\n\\(S=0\\) implies no trend.\n\nWe can carry out a hypothesis test to assess whether \\(S\\) is significantly different from zero:\n\\[\n\\begin{aligned}\nH_0&: \\text{our data are independent random realisations (no trend)} \\\\\nH_1&: \\text{there is a significant trend in our data}\n\\end{aligned}\n\\]\nWe compare the test statistic to a standard normal distribution \\(Z_{(1-\\alpha/2)}\\).\nWe can use the mk.test function in the trend R package, to carry out the Mann-Kendall test.\n\n\n\n\n\n\n Exercise\n\n\n\nBelow, we have plotted the average discharge (m\\(^3\\)) from the River Rhine over many years (black line), with the overall trend line added (red line).\n\n\n\n\nWe carry out the Mann-Kandall test in R as follows, where the vector Q contains the average discharge data:\n\nCodemk.test(Q)\n\n\nMann-Kendall Test two-sided homogeneity test\nStatistics for total series\n\nH0: S = 0 (no trend)\nHA: S != 0 (monotonic trend)\n\nStatistics for total series\n      S  varS    Z    tau  pvalue\n 1 -144 10450 -1.4 -0.145 0.16185\nGiven these results, what can we say in terms of the hypotheses of the test?\n\n\nSolution\n\nHere we see a p-value of 0.16, which means that there is no evidence to reject \\(H_0\\) and therefore we believe that it is possible that there is no trend present.\n\n\n\n\n7.2 Kendall rank correlation coefficient\nWe can also compute a rank correlation coefficient, \\(\\tau\\), which measures the strength of our trend, \\[\\tau = \\frac{S}{D}.\\] Here, \\(D = \\frac{n(n-1)}{2}\\), the number of pairwise comparisons used in the calculation of \\(S\\). \\(\\tau\\) has a range \\((-1, 1)\\), similar to the standard correlation used in regression modelling.\n\n\n\n\n\n\n Exercise\n\n\n\nChlorophyll levels in a lake have been measured over 36 years, as shown in the plot below.\n\n\n\n\nGiven that \\(S=384\\), compute \\(\\tau\\) to measure the strength of the trend.\nAnswer (to 2 decimal places): \n\n\nSolution\n\n\\[\n\\begin{aligned}\nD &= \\frac{n(n-1)}{2} \\\\\n&= \\frac{35 \\times 36}{2} \\\\\n&= 680\\\\\n\\\\\n\\tau = \\frac{S}{D} &= \\frac{384}{680} = 0.61\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "notes/notes_1.html#seasonal-kendall-test",
    "href": "notes/notes_1.html#seasonal-kendall-test",
    "title": "Assessing Change Over Time",
    "section": "\n8 Seasonal Kendall test",
    "text": "8 Seasonal Kendall test\nThe seasonal Kendall test accounts for seasonality by computing \\(S\\) for each of \\(M\\) seasons separately, then combining the results. For example, if we had monthly data, we might compute \\(S\\) separately for each month. Let \\(S_j\\) be the Kendall statistic for season \\(j\\), then the overall statistic is given by: \\[S_k = \\sum_{j=1}^M S_j\\]\nAgain, this can be compared to a standard normal distribution \\(Z_{(1-\\alpha/2)}\\)."
  },
  {
    "objectID": "notes/notes_1.html#smoothing-in-time-series",
    "href": "notes/notes_1.html#smoothing-in-time-series",
    "title": "Assessing Change Over Time",
    "section": "\n9 Smoothing in Time Series",
    "text": "9 Smoothing in Time Series\nEnvironmental time series data are often complex and traditional parametric methods are difficult to implement. The relationship between our parameter of interest and time may not follow a linear pattern. We could simply keep adding polynomial functions, but this may become inefficient and lead to a model with too many parameters. It is often more elegant to consider an approach which uses smoothing.\nWe can express the relationship between any response and explanatory variable as \\[y = f(x).\\]\nHere \\(y\\) is the response, \\(x\\) is our explanatory variable and \\(f()\\) is a function which describes their relationship. Smoothing techniques are used to model \\(f()\\) without specifying any specific statistical form of the underlying function.\nThere is a whole course on smoothing methods (Flexible Regression), and many of you will already have taken this. Therefore we will simply focus briefly on a couple of key methods which are used for environmental data. We will look at one method mainly used for descriptive purposes (LOWESS) and one which is used for estimation (penalised splines).\n\n9.1 LOWESS\nLOWESS (LOcally WEighted Scatterplot Smoothing) is an approach which is often used to obtain a graphical illustration of our data. It involves carrying out a series of polynomial regressions on small regions of the data, and then combining them. The more datapoints we have in a region, the smoother our curve will be. This can be somewhat computationally intensive compared to simple moving average methods, but generally produces a smoother function.\nLOWESS involves carrying out the following steps:\n\nIdentify a target point, \\(x\\).\nConstruct a `window’ containing its \\(k\\) nearest neighbours.\nFit a weighted polynomial to these \\(k\\) datapoints.\nWe then choose a new target point and repeat until we have covered all timepoints.\n\n\n\n\n\nWe have to decide on the size of the window. In R, the default is that each window contains two-thirds of the data. We can fit these models in R using the scatter.smooth or loess functions. The different colours in the plot below show different sizes of windows. The wider the window, the smoother the function (green narrowest, red widest).\n\n\n\n\n\n\n\n\n\n\n Example: SO\\(_2\\) levels\n\n\n\nAir quality \\(\\text{SO}_2\\) levels are measured daily over 30 years. The right plot with a wider window (higher bandwidth) is smoother (maybe too smooth?). The narrower bandwidth on the left leads to a gap where there are missing values.\n\n\n\n\n\n\n\n9.2 Splines\nSplines are an alternative approach to constructing a smooth function. This approach uses piecewise polynomials to estimate the function \\(f(x)\\). Spline functions are polynomial segments which are joined together smoothly at predefined subintervals. The points where the functions join together are known as knots.\n\n\n\n\nOur model takes the form: \\[Y_i = f(x_i) + \\epsilon_i\\]\nWe estimate the function \\(f()\\) as \\[\\hat{f}(x_i) = \\sum_{k=0}^p\\beta_k b_k(x_i)\\]\nHere, \\(b_k()\\) is a set of polynomial functions known as basis functions and \\(\\beta_k\\) are their coefficients. We must decide in advance the value of \\(k\\), which defines the number of basis functions used.\nIncreasing the number of basis functions leads to a more “wiggly” line. Too few basis functions might make the line too smooth, too many might lead to overfitting.\n\n\n\n\nChoosing the correct number of basis functions can be difficult. Penalised splines (p-splines) avoid this issue. Using penalised splines, we can set a large number of basis functions, but then penalise the coefficients to encourage smoothness. This is a modified form of a standard linear regression, with a parameter \\(\\lambda\\) which controls the smoothness of the estimator.\n\n9.3 Additive Models\nDeveloping methods for estimating smooth functions is only one part of the process. We must also work out how to include these in our models. Additive models are a general form of statistical model which allows us to incorporate smooth functions alongside linear terms. \\[y_i = \\alpha + \\sum_{j=1}^k g_j (x_{ij}) + \\epsilon_{ij}\\]\nHere \\(g_j()\\) is a smooth function for the \\(j\\)th explanatory variable and \\(\\alpha\\) is the overall mean. Note that \\(g_j()\\) could simply be a linear function for one or more variables.\nSuppose that we have a variable that exhibits a long-term trend and a seasonal pattern (like we saw in the Mauna Loa example). There are two main ways that we can incorporate this: via a separable structure, or via a non-separable structure.\n\n9.3.1 Separable trend and seasonality\nWe could fit a model with smooth terms for both year (top plot) and month (bottom plot). We assume that the seasonal pattern does not change from year to year (i.e., no interaction). This can be written in the form \\[y = f_1(x_1) + f_2(x_2) + \\epsilon\\]\nWe can observe a roughly linear increasing trend, but with a seasonal pattern featuring a peak in the winter.\n\n\n9.3.2 Non-separable trend and seasonality\nSuppose we decided there was a month by year interaction (i.e., the seasonal pattern may differ by year, or the long-tern trend over the years may differ by month). We would incorporate this via a bivariate term. This can be written in the form \\[y = f(x_1, x_2) + \\epsilon\\]\nThis can be harder to interpret visually, but we can still see a similar pattern.\n\n\n\n\nNote that this non-separable structure also introduces additional computation complexity (i.e., we have lots more parameters to estimate), so that it may be much slower to fit such a model, or it may require additional computational resources, compared to fitting a model with a separable structure."
  },
  {
    "objectID": "notes/notes_3.html",
    "href": "notes/notes_3.html",
    "title": "Modelling Environmental Extremes",
    "section": "",
    "text": "We might want to know more about the maxima and minima of the environmental system we are modelling. For example, we might want to know how strong to make buildings to deal with earthquakes, how high to build flood defences, or how to prepare for extreme low temperatures.\nWe are trying to model rare events — by their very definition we won’t have a lot of data on these. The bulk of the data in any statistical distribution will be in the centre. Standard density estimation techniques (eg the normal distribution) work well where the data have the greatest density, but that’s not what we need here. We need to use a statistical model which is good at estimating the tails of our distribution."
  },
  {
    "objectID": "notes/notes_3.html#example-venice-sea-levels",
    "href": "notes/notes_3.html#example-venice-sea-levels",
    "title": "Modelling Environmental Extremes",
    "section": "Example — Venice Sea Levels",
    "text": "Example — Venice Sea Levels\nSea levels in Venice are rising and the city floods on a regular basis. What sea level can we expect in the next 5, 10, 100 years?\n\n\n\n\n\nWe have daily sea level measurements from 1931-1981 (data available on the ismev R package). The plot below shows the 10 highest sea level measurements from each year.\n\n\n\n\n\n\n\n\n\nWe can apply a block maxima approach, treating each year as a block. This requires us to identify and model the maximum values every year — highlighted in red.\n\n\n\n\n\n\n\n\n\nWe use the extRemes package to fit a GEV distribution in R. The fevd() simply takes a data vector of the annual maxima and provides parameter estimates using maximum likelihood estimation.\n\nlibrary(extRemes)\nMn &lt;- apply(venice[,-1],1,function(x)max(x,na.rm = T))\nfit_gev &lt;- fevd(Mn,method=\"MLE\")\nfit_gev$results$par\n\n    location        scale        shape \n111.09726863  17.17910050  -0.07678211 \n\n\nFrom the output we can see that \\(\\mu = 111.1\\), \\(\\sigma = 17.2\\) and \\(\\xi = -0.077\\).\nWe can assess the suitability of the GEV distribution using a return level plot. This involves commuting the return level at various return periods, and comparing it to the theoretical result under the GEV.\n\nplot(fit_gev, type = \"rl\")\n\n\n\n\n\n\n\n\nThe observed points lie along the theoretical line. Our proposed GEV distribution is appropriate. The Venice maxima follow a Gumbel distribution."
  },
  {
    "objectID": "notes/notes_3.html#extending-block-maxima-k-largest",
    "href": "notes/notes_3.html#extending-block-maxima-k-largest",
    "title": "Modelling Environmental Extremes",
    "section": "Extending Block Maxima — \\(k\\)-largest",
    "text": "Extending Block Maxima — \\(k\\)-largest\nLooking at just the maxima means we throw away a lot of data, making it harder to accurately estimate parameters. We could instead follow an approach which models the \\(k\\) largest values within a block.\n\n\n\n\n\n\n\n\n\nWe have to make difficult subjective choices when fitting a block maxima model. What do we choose as our block? Week? Month? Year? Decade? Bigger blocks means we have fewer data points, but smaller blocks mean our ‘extremes’ might not be extreme at all, violating the assumptions of the GEV distribution. If we use a \\(k\\)-largest approach, we have a similar decision on what value of \\(k\\) to choose.\nBlock maxima methods work well in many situations, and take advantage of natural blocks in the data. However, it does not work well if there is a lot of variability between blocks. In that scenario, some blocks may have many more large counts than others, and much of the data will be discarded. We can overcome this by using a threshold approach, which models all observations which exceed some pre-specified value."
  },
  {
    "objectID": "notes/notes_3.html#example-fort-collins-colorado",
    "href": "notes/notes_3.html#example-fort-collins-colorado",
    "title": "Modelling Environmental Extremes",
    "section": "Example — Fort Collins, Colorado",
    "text": "Example — Fort Collins, Colorado\nWe have daily precipitation data from 1900-1999, obtained from a rain gauge in Fort Collins, Colorado, taken from Katz et al, 2002.\n\n\n\n\n\nWe compare three different choices of threshold below (\\(u = 0.5, 1.0, 1.5\\)) to show the importance of getting the choice right.\n\n\n\nu\n\\(\\% &gt; u\\)\n\\(n &gt; u\\)\n\n\n\n\n0.5\n2.08\n759\n\n\n1.0\n0.58\n213\n\n\n1.5\n0.25\n91\n\n\n\nWe can fit a mean residual plot to identify a sensible choice of threshold. It appears that a value of \\(u\\) somewhere between 1.2 and 1.8 would be an appropriate choice here — this is where the plot appears to be linear.\n\n\n\n\n\nWe can also carry out a sensitivity analysis to see the effect of choosing different threshold values on the estimated model parameters. The plot below shows the parameter estimates at different thresholds — they seem fairly robust.\n\n\n\n\n\nWe can use the extRemes package to fit a Generalised Pareto distribution in R. The function fevd allows several extreme value distributions (including GEV and GPD) to be fitted, and can also provide return levels.\n\nfitGP &lt;- fevd(Fort, threshold=1.5, type=\"GP\",\n                    time.units=\"365/year\")\n\nreturn.level(fitGP, c(10,20,100), type=\"GP\")\n\n[1] \"Return Levels for period units in years\"\n 10-year level  20-year level 100-year level\n      2.857184       3.340219       4.581339\nThreshold exceedances are not always independent due to temporal correlation. If we have high temperatures today, it’s likely we might also have high temperatures tomorrow. We have to account for this dependence within our model, for example by using autoregressive model outlined in the time series section. Alternatively, we could use a “declustering” approach which identifies these temporal clusters and simply uses the cluster maxima."
  }
]