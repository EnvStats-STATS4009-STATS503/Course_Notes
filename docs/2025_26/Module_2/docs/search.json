[
  {
    "objectID": "notes/notes_3.html",
    "href": "notes/notes_3.html",
    "title": "Modelling Environmental Extremes",
    "section": "",
    "text": "Overview\nWe might want to know more about the maxima and minima of the environmental system we are modelling. For example, we might want to know how strong to make buildings to deal with earthquakes, how high to build flood defences, or how to prepare for extreme low temperatures.\nWe are trying to model rare events — by their very definition we won’t have a lot of data on these. The bulk of the data in any statistical distribution will be in the centre. Standard density estimation techniques (eg the normal distribution) work well where the data have the greatest density, but that’s not what we need here. We need to use a statistical model which is good at estimating the tails of our distribution.\n\n\nExtreme Value Distributions\nThe first thing we have to consider is what actually represents an extreme observation. This will vary depending on the context of the dataset.\nA lot of investigation of environmental extremes will focus on time series data. Within time series data, we typically have natural groupings or blocks of observations (days, months, years etc) Therefore a common approach for modelling extremes focuses on the idea of block maxima — identifying the maximum (or minimum) value in each block. For example, if we have daily temperature data measured over 100 years, we could look at the highest temperature in each year.\nSuppose we have a series of random variables \\(X_1, \\ldots, X_n\\), each with cumulative distribution function \\(F\\), where \\(F(x) = \\text{P}(X \\leq x)\\). We can define the maximum of this set of random variables as \\(M_n = \\max{\\{ X_1, \\ldots, X_n\\}}\\). Then we can show that \\(\\text{P}(M_n \\leq x) = P(X_1 \\leq x, \\ldots X_x \\leq x) = \\{F(x)\\}^n = F^n(x)\\).\nThis suggests that to understand block maxima (the maximum value in blocks of observations), we might focus on estimating \\(F^n(x)\\). However, this approach faces two fundamental problems:\n\nThe Estimation Problem - The true cumulative distribution \\(F(x)\\) is unknown. While we could estimate it from data, small errors in estimating \\(F(x)\\) - especially in the tail where \\(F(x)\\) is close to 1 - are magnified exponentially when raised to the power \\(n\\). A tiny underestimation of the tail probability can lead to dramatically underestimating \\(F^n(x)\\).\nThe Degeneracy Problem - Note that as \\(n \\to \\infty\\) then the value of \\(F^n(x)\\) will be 0 or 1 depending on whether \\(F(x) &lt;1\\) or \\(F(x)=1\\). This means that the limit distribution of the maxima is a degenerate distribution. In simple terms, if you keep taking more and more samples, the maximum just keeps increasing until it effectively becomes a fixed extreme value, with no interesting variation left to model.\n\nInstead of trying to estimate \\(F\\) or \\(F^n\\) directly, Extreme Value Theory shifts focus to the limiting distribution of the normalized maximum. We introduce sequences \\(a_n &gt; 0\\) and \\(b_n\\) to stabilize the maximum as \\(n\\) grows. This is a distribution \\(G(x)\\) such that, for constants, \\(a_n &gt; 0\\) and \\(b_n\\),\n\\[\\text{P}\\left(\\frac{M_n - b_n}{a_n} \\leq x\\right) = F^n(a_n x + b_n) \\to G(x) \\text{ as } n \\to \\infty\\]\nThere are three main families of extreme value distribution which have our desired properties as limiting distributions of \\(F^n(x)\\).\n\nGumbel: \\(G(x) = \\exp \\left( -\\exp [\\frac{-(x-a)}{b} ]\\right)\\)\nFrechet: \\(G(x) = \\exp \\left( -[\\frac{(x-a)}{b} ]^{-\\alpha}\\right)\\) for \\(z&gt;b\\), 0 otherwise.\nWeibull: \\(G(x) = \\exp \\left( -[\\frac{-(x-a)}{b} ]^{\\alpha}\\right)\\) for \\(z&gt;b\\), 1 otherwise.\n\nHere, \\(a\\) is a location parameter, \\(b\\) is a scale parameter and \\(\\alpha\\) is a shape parameter.\nMore generally, we can model the maxima using the Generalised Extreme Value (GEV) distribution \\[G(x) = \\exp \\left( - \\left[ 1 + \\frac{\\xi (z - \\mu)}{\\sigma} \\right]^{-\\frac{1}{\\xi}} \\right)\\]\nHere, \\(\\mu\\) is the location parameter, \\(\\sigma\\) is the scale parameter and \\(\\xi\\) is the shape parameter. The Gumbel, Frechet and Weibull distributions are all special cases depending on the value of \\(\\xi\\).\n\n\n\n\n\n\nIf \\(\\xi &lt; 0\\) then we have the Weibull distribution.\nIf \\(\\xi &gt; 0\\) then we have the Frechet distribution.\nAs \\(\\xi \\to 0\\) then we have the Gumbel distribution.\n\n\n\n\n\n\n\n Task\n\n\n\nLet a random vector \\(\\mathbf{X} = X_1,\\ldots,X_n\\) such that \\(\\mathbf{X}\\overset{iid}{\\sim}\\mathrm{Uniform}(0,1)\\). Setting the normalizing constants \\(a_n = 1/n\\) and \\(b_n =1\\), show that the limiting distribution of the maxima \\(M_n =\\mathrm{max}(\\mathbf{X})\\) as \\(n\\to \\infty\\) is a GEV with \\(\\mu= -1\\) ,\\(\\sigma = 1\\) and \\(\\xi = -1\\)\n\n\nTake Hint\n\nRecall that the CDF of an U(0,1) is given by \\(F(x) = x ~(\\text{for } 0 \\leq x \\leq 1)\\), also note that:\n\\[\n\\lim_{n \\to \\infty}\\left( 1 + \\frac{1}{n} x\\right)^n  = \\exp{x}\n\\]\n\n\n\nSolution\n\n\\[\n\\begin{aligned}\n\\mathbb{Pr}\\left(\\dfrac{M_n -b_n}{a_n}\\leq x \\right) & = \\mathbb{Pr}(M_n \\leq b_n +a_n x)\\\\\n& = F\\left(b_n +a_n x\\right)^n\\\\\n&= F\\left(1 + \\frac{1}{n}x\\right)^n\\\\\n& = \\left(1 + \\frac{1}{n}x\\right)^n ~~~\\text{for } 0 \\leq 1 + x/n   \\leq 1 \\\\\n&\\rightarrow \\exp x~ \\text{as }  n \\to \\infty\n\\end{aligned}\n\\] Let \\(G(x) = \\exp \\left\\{- (1 + \\xi \\frac{x -\\mu}{\\sigma})^{-1/xi}\\right\\}\\), setting \\(\\mu= -1\\) ,\\(\\sigma = 1\\) and \\(\\xi = -1\\)\n\\[\nG(x) =  \\exp \\left\\{- (1 -x + 1 )\\right\\} = \\exp x\n\\]\n\n\n\nThe following app illustrates EVT how the block maxima of a random variable fits a GEV distribution as the sample size increases. The app will generate some samples (blocks) of size \\(n\\) drawn from a specified distribution (Uniform(0,1) or Exp(1)) and plot the distribution (histograms) of the maximum of each sample. Then, it compares the empirical histograms against the theoretical GEV (solid red line). For example, if \\(X\\sim\\text{Uniform(0,1)}\\) and we standardise the maxima using \\(b_n=1\\) and \\(a_n=1/n\\) the asymptotic distribution of the maxima is a GEV(\\(\\mu=-1,\\sigma=1,\\xi=-1\\)) (see the previous task). Likewise, if \\(X\\sim \\exp(1)\\) and we let \\(a_n=1\\) and \\(b_n= \\log n\\), the its maxima will converge to a GEV(\\(\\mu=0,\\sigma=1,\\xi=0\\)), i.e., a Gumbell distribution (see the proof in the tutorial material).\n\n\n\n\n\n\n Task\n\n\n\nPlay around with the app settings to see the impact that the number of samples and the size of blocks have on the approximation of the GEV for the different distributions.\n\n\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 500\n\nlibrary(shiny)\n\n\n\n# Function to generate block maxima\ngenerate_max &lt;- function(N, n, rdist, param, a, b, seed=pi){\n  set.seed(seed)\n  Mn &lt;- numeric(N)\n  for(i in 1:N){\n    sample &lt;- rdist(n, param[1], param[2])\n    Mn[i] &lt;- (max(sample) - b) / a\n  }\n  return(Mn)\n}\n\n# UI\niu &lt;- fluidPage(\n  titlePanel(\"Block Maxima Simulation\"),\n  sidebarLayout(\n    sidebarPanel(\n      selectInput(\"dist\", \"Choose Distribution:\", choices = c(\"Uniform(0,1)\" = \"uniform\", \"Exponential(1)\" = \"exponential\")),\n      sliderInput(\"n\", \"Observations per Block (n):\", min = 5, max = 100, value = 5, step = 5),\n      sliderInput(\"N\", \"Number of Blocks (N):\", min = 10000, max = 50000, value = 50000, step = 10000)\n    ),\n    mainPanel(\n      plotOutput(\"histogram\")\n    )\n  )\n)\n\n# Server\nserver &lt;- function(input, output) {\n  output$histogram &lt;- renderPlot({\n    if (input$dist == \"uniform\") {\n      a &lt;- 1 / input$n\n      b &lt;- 1\n      Mn &lt;- generate_max(input$N, input$n, runif, c(0,1), a, b)\n      true_gev &lt;- function(x) evd::dgev(x, loc = -1, scale = 1, shape = -1)\n    } else {\n      a &lt;- 1\n      b &lt;- log(input$n)\n      Mn &lt;- generate_max(input$N, input$n, rgamma, c(1,1), a, b)\n      true_gev &lt;- function(x) evd::dgev(x, loc = 0, scale = 1, shape = 0)\n    }\n    \n    hist(Mn, breaks = \"FD\", col = \"lightblue\", prob = TRUE, main = \"Histogram of Block Maxima\", xlab = \"Maxima\")\n    curve(true_gev, col = \"red\", lwd = 2, add = TRUE)\n  })\n}\n\n# Run App\nshinyApp(ui = iu, server = server)"
  },
  {
    "objectID": "notes/notes_1.html",
    "href": "notes/notes_1.html",
    "title": "Assessing Change Over Time",
    "section": "",
    "text": "A time series is a sequence of measurements on the same object made over time. For example, we might measure the level of carbon dioxide (\\(\\text{CO}_2\\)) in a town every day for a year. The purpose of making such measurements is to understand how our variable of interest has changed over time. For example, a government would be keen to know if air pollution levels are getting better or worse, or a conservationist might want to identify declining trends in wildlife populations to better target their conservation objectives.\nWe can write our set of time series data as \\(y_1, \\ldots, y_T,\\) where \\(y_i\\) is the observation at time point \\(i\\), and \\(T\\) is the total number of observations. Time series data are typically not independent, since there will often be correlation between consecutive observations. This dependency structure must be taken into account when modelling.\n\n\n\n\n\n\n Exercise 1\n\n\n\nCan you spot any dependency structure shown in the below plot of log(flow) in the River Dee?\n\n\nLog(flow) data for River Dee.\n\n\n\nSolution\n\nThere is a clear structure to the data. It looks like there is a repeated pattern, once per year. (This is a “seasonal pattern” and we’ll go into more details about this later in this section of the course.)\n\n\n\n\n\n\n\n\n\n Example: Mauna Loa\n\n\n\nMauna Loa in Hawaii is one of the biggest and most active volcanoes in the world. \\(\\text{CO}_2\\) levels have been monitored since 1958. Mauna Loa is one of the first sites worldwide where increasing \\(\\text{CO}_2\\) levels were identified.\n\nWe can observe a clear trend, and also a seasonal pattern. It may be sensible to standardise the data and represent all observations in terms of ‘anomalies’, i.e., their deviation from the starting point (1960 mean level).\n\n\n\n\n\n\n\n\n\n\n\n\n Example: Global temperature\n\n\n\nThe plot below shows the global temperature anomaly (the current value compared to the average from 1951–1980).\n\n\n\n\nSource: https://climate.nasa.gov/vital-signs/global-temperature/\n\n\n\n\n\n\n\n\n Exercise 2\n\n\n\nHow would you describe the change in global temperature shown in the above plot?\nWe will discuss this in the lecture."
  },
  {
    "objectID": "notes/notes_1.html#introduction",
    "href": "notes/notes_1.html#introduction",
    "title": "Assessing Change Over Time",
    "section": "",
    "text": "A time series is a sequence of measurements on the same object made over time. For example, we might measure the level of carbon dioxide (\\(\\text{CO}_2\\)) in a town every day for a year. The purpose of making such measurements is to understand how our variable of interest has changed over time. For example, a government would be keen to know if air pollution levels are getting better or worse, or a conservationist might want to identify declining trends in wildlife populations to better target their conservation objectives.\nWe can write our set of time series data as \\(y_1, \\ldots, y_T,\\) where \\(y_i\\) is the observation at time point \\(i\\), and \\(T\\) is the total number of observations. Time series data are typically not independent, since there will often be correlation between consecutive observations. This dependency structure must be taken into account when modelling.\n\n\n\n\n\n\n Exercise 1\n\n\n\nCan you spot any dependency structure shown in the below plot of log(flow) in the River Dee?\n\n\nLog(flow) data for River Dee.\n\n\n\nSolution\n\nThere is a clear structure to the data. It looks like there is a repeated pattern, once per year. (This is a “seasonal pattern” and we’ll go into more details about this later in this section of the course.)\n\n\n\n\n\n\n\n\n\n Example: Mauna Loa\n\n\n\nMauna Loa in Hawaii is one of the biggest and most active volcanoes in the world. \\(\\text{CO}_2\\) levels have been monitored since 1958. Mauna Loa is one of the first sites worldwide where increasing \\(\\text{CO}_2\\) levels were identified.\n\nWe can observe a clear trend, and also a seasonal pattern. It may be sensible to standardise the data and represent all observations in terms of ‘anomalies’, i.e., their deviation from the starting point (1960 mean level).\n\n\n\n\n\n\n\n\n\n\n\n\n Example: Global temperature\n\n\n\nThe plot below shows the global temperature anomaly (the current value compared to the average from 1951–1980).\n\n\n\n\nSource: https://climate.nasa.gov/vital-signs/global-temperature/\n\n\n\n\n\n\n\n\n Exercise 2\n\n\n\nHow would you describe the change in global temperature shown in the above plot?\nWe will discuss this in the lecture."
  },
  {
    "objectID": "notes/notes_1.html#ecological-trend",
    "href": "notes/notes_1.html#ecological-trend",
    "title": "Assessing Change Over Time",
    "section": "\n2 Ecological Trend",
    "text": "2 Ecological Trend\nThe purpose of time series modelling is to identify any trends which exist in the dataset. But what exactly is a trend? It depends who you ask.\nThe Joint Nature Conservation Council (JNCC) define it as: a measurement of change derived from a comparison of the results of two or more statistics. This is often considered to be the ecological definition of trend, i.e., a change (in terms of percentage or some index) between two timepoints."
  },
  {
    "objectID": "notes/notes_1.html#statistical-trend",
    "href": "notes/notes_1.html#statistical-trend",
    "title": "Assessing Change Over Time",
    "section": "\n3 Statistical Trend",
    "text": "3 Statistical Trend\nIn statistics, the definition of a trend is often more wide-ranging:\n\nA long-term change in the mean level (Chatfield, 1996)\nLong-term movement (Kendall and Ord, 1990)\nLong-term behaviour of the process (Chandler, 2002)\nThe non-random function \\(\\mu(t) = E(Y(t))\\) (Diggle, 1990)\n\nWe may be interested in trends in mean, variance or extreme values. Trends are not limited to linear or monotonic patterns.\n\n3.1 Simple Linear Trend\nWe can represent a simple linear trend using the standard notation: \\[Y_t = \\beta_0 + \\beta_1x_t + \\epsilon_t.\\]\nHere, \\(\\beta_0\\) is an intercept and \\(\\beta_1\\) represents the slope (trend). This is just a standard linear model, with all the usual assumptions (normality, constant variance, independence). This model therefore does not account for any seasonality or autocorrelation in our data.\n\n\n\n\n\n\n Example: Chlorophyll levels in a lake\n\n\n\nWe observe monthly chlorophyll levels in a lake between 2001 and 2006.\nWe can fit a linear model of the form: \\[\\text{Log Chlorophyll} = \\beta_0 + \\beta_1 \\text{ Date} + \\text{error}\\]"
  },
  {
    "objectID": "notes/notes_1.html#seasonal-patterns",
    "href": "notes/notes_1.html#seasonal-patterns",
    "title": "Assessing Change Over Time",
    "section": "\n4 Seasonal Patterns",
    "text": "4 Seasonal Patterns\nMany environmental time series have some sort of periodicity (e.g. a monthly pattern in temperature). We can produce some form of seasonality plot to understand this better. The period is the time interval between consecutive peaks or troughs. A seasonal component of a dataset is a regular fluctuation with a period of one year or less.\n\n\n\n\n\n\n Example: Mean surface water temperature in Lake Nam\n\n\n\nLake Nam (Namtso) is a mountain lake in Tibet. The mean surface water temperature was measured monthly between 1996 and 2011.\n\n\n\n\nWe can plot the data over time, showing clear pattern in the data:\n\n\n\n\nWe should therefore plot the data by month. Doing so indicates a clear seasonal pattern. There is a peak in Month 3 and a trough in Months 9/10:"
  },
  {
    "objectID": "notes/notes_1.html#harmonic-regression",
    "href": "notes/notes_1.html#harmonic-regression",
    "title": "Assessing Change Over Time",
    "section": "\n5 Harmonic Regression",
    "text": "5 Harmonic Regression\nThe monthly pattern is very similar to a sine wave, and we can use this feature in our modelling. This is known as harmonic regression, and is suitable when we have a regular seasonal trend (as we just saw in the Lake Nam example).\n\n\n\n\nHarmonic regression is based on an equation of the form\n\\[Y_t = \\beta_0 + \\gamma \\sin\\left(\\frac{2\\pi [u_t - \\theta]}{p}\\right) + \\epsilon_t\\]\nHere, \\(\\gamma\\) is the amplitude of the wave, \\(p\\) is the period of the wave, and \\(\\theta\\) represents the ‘position’ on the wave (in radians). However, it can often be more convenient to rewrite this in the form of a simple multiple regression model, taking advantage of the double angle formula.\nGiven that \\(\\sin(a-b) = \\sin(a)\\cos(b) - \\cos(a)\\sin(b)\\), we can show that:\n\\[\n\\begin{aligned}\n\\gamma \\sin\\left(\\frac{2\\pi [u_t - \\theta]}{p}\\right) &= \\gamma \\sin\\left(\\frac{2\\pi u_t}{p} - \\frac{2\\pi\\theta}{p}\\right)\\\\\n&= \\gamma \\left[ \\sin\\left(\\frac{2\\pi u_t}{p}\\right)\\cos\\left(\\frac{2\\pi\\theta}{p}\\right) - \\cos\\left(\\frac{2\\pi u_t}{p}\\right)\\sin\\left(\\frac{2\\pi\\theta}{p}\\right)\\right]\n\\end{aligned}\n\\]\nSince \\(\\pi\\), \\(\\theta\\) and \\(p\\) are known, we can create new regression parameters \\(\\gamma_1 = \\gamma\\cos\\left(\\frac{2\\pi\\theta}{p}\\right)\\) and \\(\\gamma_2 = - \\gamma\\sin\\left(\\frac{2\\pi\\theta}{p}\\right)\\)\nThe final harmonic regression model can thus be written:\n\\[Y_t = \\beta_0 + \\gamma_1 \\sin\\left(\\frac{2\\pi u_t}{p}\\right) + \\gamma_2 \\cos\\left(\\frac{2\\pi u_t}{p}\\right) + \\epsilon_t\\]\nOur new parameters \\(\\gamma_1\\) and \\(\\gamma_2\\) control the seasonal trends, with \\(p\\) representing the period. \\(\\beta_0\\) is still the intercept term, which can also be interpreted as the overall mean. Note that this is still a linear model, since it is linear in the coefficients.\nThe standard harmonic regression assumes we have the same seasonal pattern each year, but this may not always be appropriate. There are many more sophisticated models available if this assumption does not hold. Some are still based on sine and cosine waves, while others may use autocorrelation functions or a form of semiparametric smoothing."
  },
  {
    "objectID": "notes/notes_1.html#time-series-model",
    "href": "notes/notes_1.html#time-series-model",
    "title": "Assessing Change Over Time",
    "section": "\n6 Time Series Model",
    "text": "6 Time Series Model\nThe seasonal variation can sometimes be so strong that it obscures the overall trend (or any other patterns). In most cases, we are not actually particularly interested in actually knowing about the seasonal trend. In these cases, it is simply a nuisance factor that we need to account for in our model. Our primary interest is usually in understanding the longer term trends in our data.\nWe often try to remove or separate out this seasonal pattern when analysing time series. We can therefore think of our overall time series model in the following form:\n\\[X = \\text{trend} + \\text{seasonal component} + \\text{error}\\]\nIn terms of mathematical notation, we can write this as\n\\[X_t = m_t + s_t + \\epsilon_t.\\] Our error, \\(\\epsilon_t\\) is assumed to be random, and follows the distribution \\(\\epsilon_t \\sim \\text{Normal}(0, \\sigma^2)\\)."
  },
  {
    "objectID": "notes/notes_1.html#estimating-trend",
    "href": "notes/notes_1.html#estimating-trend",
    "title": "Assessing Change Over Time",
    "section": "\n7 Estimating Trend",
    "text": "7 Estimating Trend\nWe have now identified a method for isolating the trend in our model. However, we still have to work out the best way to explore and understand this trend. We want to know the size of the trend, but also have to assess whether it is linear, and also test for statistical significance.\nA variety of models and techniques exist for exploring our trend.\n\n\n\n\n\n\n Example: Bird populations\n\n\n\nWe have collected annual data on the population of two birds between 1975 and 2005. What are the trends? Are they significantly different from zero?\n\n\n\n\nWe have fitted two models to attempt to assess the trends for each bird. The blue line is a linear regression, while the red line is a more flexible additive model.\n\n\n\n\n\n\n\n\n\n\n\n\n Exercise 3\n\n\n\nWhich of the models are more appropriate? Have we adequately captured the patterns in the data?\n\n\nSolution\n\nNeither relationship appears to be linear, so that the additive models are more appropriate than the linear regression models here. However, both models fail to capture the peaks in the data well, so that we could consider whether other models are more appropriate. E.g., models for extremes may be more appropriate if we are interested in the peaks.\n\n\n\nIn our bird population example, both models indicate the overall trend, but they do not test for significance. We therefore cannot be sure whether the changes are ‘genuine’ or are simply down to random variation. We can use non-parametric approaches (e.g., Mann-Kendall test and the Seasonal Kendall) to assess the trend in our data.\n\n7.1 Mann-Kendall test\nThe Mann-Kendall test is commonly used to detect trends in environmental, climate, and hydrological data. It looks for a consistent increase or decrease in a trend over time (not necessarily linear). It is commonly used for short time series, where we may not have sufficient data for more sophisticated approaches.\nAssume we have an ordered dataset \\(z_1, \\ldots, z_T\\)\n\nCompute ALL possible differences \\(d = z_j - z_k\\) where \\(j&gt;k\\).\nCreate an indicator function \\(\\text{sign}(z_j - z_k)\\) such that:\n\n\\[\n\\begin{aligned}\n\\text{sign}(z_j - z_k) =\n\\begin{cases}\n1 & \\text{if } z_j - z_k &gt; 0 \\\\\n0 & \\text{if } z_j - z_k = 0 \\\\\n-1 & \\text{if } z_j - z_k &lt; 0\n\\end{cases}\n\\end{aligned}\n\\]\n\nThe Mann-Kendall statistic, \\(S\\), is given by \\[S = \\sum_{k=1}^{n-1}\\sum_{j=k+1}^n \\text{sign}(z_j - z_k)\\]\n\n\nOur test statistic measures the size and direction of the trend:\n\nA positive value of \\(S\\) suggests the data are increasing over time (an upward trend).\nA negative value of \\(S\\) suggests a downward trend.\n\n\\(S=0\\) implies no trend.\n\nWe can carry out a hypothesis test to assess whether \\(S\\) is significantly different from zero:\n\\[\n\\begin{aligned}\nH_0&: \\text{our data are independent random realisations (no trend)} \\\\\nH_1&: \\text{there is a significant trend in our data}\n\\end{aligned}\n\\]\nWe compare the test statistic to a standard normal distribution \\(Z_{(1-\\alpha/2)}\\).\nWe can use the mk.test function in the trend R package, to carry out the Mann-Kendall test.\n\n\n\n\n\n\n Exercise\n\n\n\nBelow, we have plotted the average discharge (m\\(^3\\)) from the River Rhine over many years (black line), with the overall trend line added (red line).\n\n\n\n\nWe carry out the Mann-Kandall test in R as follows, where the vector Q contains the average discharge data:\n\nCodemk.test(Q)\n\n\nMann-Kendall Test two-sided homogeneity test\nStatistics for total series\n\nH0: S = 0 (no trend)\nHA: S != 0 (monotonic trend)\n\nStatistics for total series\n      S  varS    Z    tau  pvalue\n 1 -144 10450 -1.4 -0.145 0.16185\nGiven these results, what can we say in terms of the hypotheses of the test?\n\n\nSolution\n\nHere we see a p-value of 0.16, which means that there is no evidence to reject \\(H_0\\) and therefore we believe that it is possible that there is no trend present.\n\n\n\n\n7.2 Kendall rank correlation coefficient\nWe can also compute a rank correlation coefficient, \\(\\tau\\), which measures the strength of our trend, \\[\\tau = \\frac{S}{D}.\\] Here, \\(D = \\frac{n(n-1)}{2}\\), the number of pairwise comparisons used in the calculation of \\(S\\). \\(\\tau\\) has a range \\((-1, 1)\\), similar to the standard correlation used in regression modelling.\n\n\n\n\n\n\n Exercise\n\n\n\nChlorophyll levels in a lake have been measured over 36 years, as shown in the plot below.\n\n\n\n\nGiven that \\(S=384\\), compute \\(\\tau\\) to measure the strength of the trend.\nAnswer (to 2 decimal places): \n\n\nSolution\n\n\\[\n\\begin{aligned}\nD &= \\frac{n(n-1)}{2} \\\\\n&= \\frac{35 \\times 36}{2} \\\\\n&= 680\\\\\n\\\\\n\\tau = \\frac{S}{D} &= \\frac{384}{680} = 0.61\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "notes/notes_1.html#seasonal-kendall-test",
    "href": "notes/notes_1.html#seasonal-kendall-test",
    "title": "Assessing Change Over Time",
    "section": "\n8 Seasonal Kendall test",
    "text": "8 Seasonal Kendall test\nThe seasonal Kendall test accounts for seasonality by computing \\(S\\) for each of \\(M\\) seasons separately, then combining the results. For example, if we had monthly data, we might compute \\(S\\) separately for each month. Let \\(S_j\\) be the Kendall statistic for season \\(j\\), then the overall statistic is given by: \\[S_k = \\sum_{j=1}^M S_j\\]\nAgain, this can be compared to a standard normal distribution \\(Z_{(1-\\alpha/2)}\\)."
  },
  {
    "objectID": "notes/notes_1.html#smoothing-in-time-series",
    "href": "notes/notes_1.html#smoothing-in-time-series",
    "title": "Assessing Change Over Time",
    "section": "\n9 Smoothing in Time Series",
    "text": "9 Smoothing in Time Series\nEnvironmental time series data are often complex and traditional parametric methods are difficult to implement. The relationship between our parameter of interest and time may not follow a linear pattern. We could simply keep adding polynomial functions, but this may become inefficient and lead to a model with too many parameters. It is often more elegant to consider an approach which uses smoothing.\nWe can express the relationship between any response and explanatory variable as \\[y = f(x).\\]\nHere \\(y\\) is the response, \\(x\\) is our explanatory variable and \\(f()\\) is a function which describes their relationship. Smoothing techniques are used to model \\(f()\\) without specifying any specific statistical form of the underlying function.\nThere is a whole course on smoothing methods (Flexible Regression), and many of you will already have taken this. Therefore we will simply focus briefly on a couple of key methods which are used for environmental data. We will look at one method mainly used for descriptive purposes (LOWESS) and one which is used for estimation (penalised splines).\n\n9.1 LOWESS\nLOWESS (LOcally WEighted Scatterplot Smoothing) is an approach which is often used to obtain a graphical illustration of our data. It involves carrying out a series of polynomial regressions on small regions of the data, and then combining them. The more datapoints we have in a region, the smoother our curve will be. This can be somewhat computationally intensive compared to simple moving average methods, but generally produces a smoother function.\nLOWESS involves carrying out the following steps:\n\nIdentify a target point, \\(x\\).\nConstruct a `window’ containing its \\(k\\) nearest neighbours.\nFit a weighted polynomial to these \\(k\\) datapoints.\nWe then choose a new target point and repeat until we have covered all timepoints.\n\n\n\n\n\nWe have to decide on the size of the window. In R, the default is that each window contains two-thirds of the data. We can fit these models in R using the scatter.smooth or loess functions. The different colours in the plot below show different sizes of windows. The wider the window, the smoother the function (green narrowest, red widest).\n\n\n\n\n\n\n\n\n\n\n Example: SO\\(_2\\) levels\n\n\n\nAir quality \\(\\text{SO}_2\\) levels are measured daily over 30 years. The right plot with a wider window (higher bandwidth) is smoother (maybe too smooth?). The narrower bandwidth on the left leads to a gap where there are missing values.\n\n\n\n\n\n\n\n9.2 Splines\nSplines are an alternative approach to constructing a smooth function. This approach uses piecewise polynomials to estimate the function \\(f(x)\\). Spline functions are polynomial segments which are joined together smoothly at predefined subintervals. The points where the functions join together are known as knots.\n\n\n\n\nOur model takes the form: \\[Y_i = f(x_i) + \\epsilon_i\\]\nWe estimate the function \\(f()\\) as \\[\\hat{f}(x_i) = \\sum_{k=0}^p\\beta_k b_k(x_i)\\]\nHere, \\(b_k()\\) is a set of polynomial functions known as basis functions and \\(\\beta_k\\) are their coefficients. We must decide in advance the value of \\(k\\), which defines the number of basis functions used.\nIncreasing the number of basis functions leads to a more “wiggly” line. Too few basis functions might make the line too smooth, too many might lead to overfitting.\n\n\n\n\nChoosing the correct number of basis functions can be difficult. Penalised splines (p-splines) avoid this issue. Using penalised splines, we can set a large number of basis functions, but then penalise the coefficients to encourage smoothness. This is a modified form of a standard linear regression, with a parameter \\(\\lambda\\) which controls the smoothness of the estimator.\n\n9.3 Additive Models\nDeveloping methods for estimating smooth functions is only one part of the process. We must also work out how to include these in our models. Additive models are a general form of statistical model which allows us to incorporate smooth functions alongside linear terms. \\[y_i = \\alpha + \\sum_{j=1}^k g_j (x_{ij}) + \\epsilon_{ij}\\]\nHere \\(g_j()\\) is a smooth function for the \\(j\\)th explanatory variable and \\(\\alpha\\) is the overall mean. Note that \\(g_j()\\) could simply be a linear function for one or more variables.\nSuppose that we have a variable that exhibits a long-term trend and a seasonal pattern (like we saw in the Mauna Loa example). There are two main ways that we can incorporate this: via a separable structure, or via a non-separable structure.\n\n9.3.1 Separable trend and seasonality\nWe could fit a model with smooth terms for both year (top plot) and month (bottom plot). We assume that the seasonal pattern does not change from year to year (i.e., no interaction). This can be written in the form \\[y = f_1(x_1) + f_2(x_2) + \\epsilon\\]\nWe can observe a roughly linear increasing trend, but with a seasonal pattern featuring a peak in the winter.\n\n\n9.3.2 Non-separable trend and seasonality\nSuppose we decided there was a month by year interaction (i.e., the seasonal pattern may differ by year, or the long-tern trend over the years may differ by month). We would incorporate this via a bivariate term. This can be written in the form \\[y = f(x_1, x_2) + \\epsilon\\]\nThis can be harder to interpret visually, but we can still see a similar pattern.\n\n\n\n\nNote that this non-separable structure also introduces additional computation complexity (i.e., we have lots more parameters to estimate), so that it may be much slower to fit such a model, or it may require additional computational resources, compared to fitting a model with a separable structure."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Module 2",
    "section": "",
    "text": "#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 500\nlibrary(shiny)\n\n\n# Function to generate block maxima\ngenerate_max &lt;- function(N, n, rdist, param, a, b, seed=pi){\n  set.seed(seed)\n  Mn &lt;- numeric(N)\n  for(i in 1:N){\n    sample &lt;- rdist(n, param[1], param[2])\n    Mn[i] &lt;- (max(sample) - b) / a\n  }\n  return(Mn)\n}\n\n# UI\niu &lt;- fluidPage(\n  titlePanel(\"Block Maxima Simulation\"),\n  sidebarLayout(\n    sidebarPanel(\n      selectInput(\"dist\", \"Choose Distribution:\", choices = c(\"Uniform(0,1)\" = \"uniform\", \"Exponential(1)\" = \"exponential\")),\n      sliderInput(\"n\", \"Observations per Block (n):\", min = 5, max = 100, value = 5, step = 5),\n      sliderInput(\"N\", \"Number of Blocks (N):\", min = 10000, max = 50000, value = 50000, step = 10000)\n    ),\n    mainPanel(\n      plotOutput(\"histogram\")\n    )\n  )\n)\n\n# Server\nserver &lt;- function(input, output) {\n  output$histogram &lt;- renderPlot({\n    if (input$dist == \"uniform\") {\n      a &lt;- 1 / input$n\n      b &lt;- 1\n      Mn &lt;- generate_max(input$N, input$n, runif, c(0,1), a, b)\n      true_gev &lt;- function(x) evd::dgev(x, loc = -1, scale = 1, shape = -1)\n    } else {\n      a &lt;- 1\n      b &lt;- log(input$n)\n      Mn &lt;- generate_max(input$N, input$n, rgamma, c(1,1), a, b)\n      true_gev &lt;- function(x) evd::dgev(x, loc = 0, scale = 1, shape = 0)\n    }\n    \n    hist(Mn, breaks = \"FD\", col = \"lightblue\", prob = TRUE, main = \"Histogram of Block Maxima\", xlab = \"Maxima\")\n    curve(true_gev, col = \"red\", lwd = 2, add = TRUE)\n  })\n}\n\n# Run App\nshinyApp(ui = iu, server = server)\n\nThis module provides the foundational principles and practical skills for processing, evaluating, and interpreting environmental and ecological data. It begins by contextualizing the role of statistics in environmental science, using contemporary news and research to illustrate how quantitative evidence informs public discourse and policy. The core of the module focuses on characterizing and managing the inherent uncertainty, variability, and common issues found in environmental and ecological data — such as censored data, outliers, and missing values—found in empirical datasets. We will also revise some core statistical concepts and look further into how environmental data is sourced, from different sampling strategies to monitoring network design."
  },
  {
    "objectID": "index.html#lectures",
    "href": "index.html#lectures",
    "title": "Module 2",
    "section": "2.1 Lectures",
    "text": "2.1 Lectures\nThere will be two - 1 hr lectures per week\n\nTuesday 12 noon (Wolfson Medical School:253 Seminar 1-Yudo)\nWednesday 9am (Maths and Stats:116 Lecture Theatre).\n\n\n\n\n\n\n\nNote\n\n\n\nLectures will be recorded if the room’s technology allows them to be."
  },
  {
    "objectID": "index.html#tutorials",
    "href": "index.html#tutorials",
    "title": "Module 2",
    "section": "2.2 Tutorials",
    "text": "2.2 Tutorials\nIn addition, there will be four tutorials for this course. There are two tutorial groups - please check on MyCampus which one you are in.\n\nTutorial Group 1 - Monday 10amTutorial Group 2- Wednesday 12 noon\n\n\nTutorial groups:\n\nSTATS 4009 - TU01 (23738)\nSTATS 5031 - TU01 (24174)\n\nVenue:\nAdam Smith: 281\nTutorial dates:\n\n26-Jan-2026\n09-Feb-2026\n23-Feb-2026\n09-Mar-2026\n\n\n\nTutorial groups:\n\nSTATS 4009 - TU02 (23739)\nSTATS 5031 - TU02 (24175)\n\nVenue:\nJoseph Black Building:C407 Agricultm\nTutorial dates:\n\n28 -Jan-2026\n11-Feb-2026\n25-Feb-2026\n11-Mar-2026\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nYou are expected to have attempted the exercise sheets before the tutorial - they will be available in advance."
  },
  {
    "objectID": "index.html#labs",
    "href": "index.html#labs",
    "title": "Module 2",
    "section": "2.3 Labs",
    "text": "2.3 Labs\nThere will be three labs taking place in Boyd Orr Building:418 Lab from 15:00-17:00pm on the following dates (clicking on the date will direct you to the lab material):\n\nLab session 1 - Jan 30th\nFeb 27th\nMarch 13th"
  },
  {
    "objectID": "notes/notes_2.html",
    "href": "notes/notes_2.html",
    "title": "Temporal Correlation and Changepoints",
    "section": "",
    "text": "We begin this week by finishing our notes on additive models in R, and then move onto temporal correlation and changepoints."
  },
  {
    "objectID": "notes/notes_2.html#fitting-additive-models-in-r",
    "href": "notes/notes_2.html#fitting-additive-models-in-r",
    "title": "Temporal Correlation and Changepoints",
    "section": "\n1.1 Fitting Additive Models in R",
    "text": "1.1 Fitting Additive Models in R\nWe can fit GAMs in R using the package mgcv, which was designed to allow extensions of generalised linear models (GLMs). The generalised aspect means that we can also extend the standard additive model to situations where we have non-normal responses, but we will not focus on these in this course.\nWe use the function gam() to fit our model. This works in a very similar manner to the lm() function. The smooth functions are represented by s(). These use the penalised splines approach described last week. Any linear terms can be included additively as normal.\nThe model will take the form below, where you can include as many smooth or linear terms as you wish.\n\nCodelibrary(mgcv)\n\nmod &lt;- gam(response ~ s(smooth1) + s(smooth2) + linear)\n\n\n\n\n\n\n\n\n Example: River Tweed nitrate level\n\n\n\nThe nitrate levels in the River Tweed were measured monthly between 1997 and 2007. The red line is a simple LOWESS curve.\n\n\nLog(Nitrate) by time for River Tweed. Black points are data. Red line is LOWESS curve.\n\n\nCodem1 &lt;- gam(log_nitrate ~ s(Date))\n\n\nFamily: Gaussian\nLink function: identity\n\nParametric coefficients:\n             Estimate Std. Error  t value    Pr(&gt;|t|)\n(Intercept)   2.04454   0.03965     51.56     &lt;2e-16\n\nApproximate significance of smooth terms:\n          edf   Ref.df     F   p-value\ns(Date) 6.183   7.336   4.37   0.000272\n\nR-sq.(adj) = 0.242 Deviance explained = 29.3%\n\nGCV = 0.15847 Scale est. = 0.14623 n = 93\nWe are mainly interested in the output related to smooth terms.\nApproximate significance of smooth terms:\n          edf   Ref.df     F   p-value\ns(Date) 6.183   7.336   4.37   0.000272\nThe p-value tells us the significance of the term, i.e., whether the smooth term is significantly different from a flat (horizontal) line. (The p-value doesn’t tell us whether the smooth term is different from a linear term!)\nThe effective degrees of freedom (EDF) tells us how nonlinear the relationship is:\n\nHigher EDF means a more nonlinear relationship.\nAn EDF of 1 indicates a linear relationship.\n\nIn our example, the p-value is very small (&lt;&lt;0.05), and therefore we have evidence that this smooth term is necessary in our model.\nThe EDF for this term is 6.183, suggesting that this is far from linear and that a smooth term may be appropriate.\nWe can assess the significance of our smooth term using the anova function. We fit a simple linear regression and compare it to the additive model we have already fitted:\n\nCodem1 &lt;- gam(log_nitrate ~ s(Date))\nm2 &lt;- lm(log_nitrate ~ Date)\n\nanova(m2, m1)\n\n\nAnalysis of Variance Table\nModel 1: log_nitrate ~ Date\nModel 2: log_nitrate ~ s(Date)\n\n    Res.Df      RSS    Df    SS      F   Pr(&gt;F)\n1     91.000   14.883\n2     85.817   12.549 5.183 2.334 3.0794 0.01228\nThe p-value confirms that the smooth term is necessary instead of a linear term. (Note that we don’t really need to test for nonlinearity, since the model should penalise excess wiggliness, effectively fitting a linear term where appropriate.)"
  },
  {
    "objectID": "notes/notes_2.html#visualising-additive-models-in-r",
    "href": "notes/notes_2.html#visualising-additive-models-in-r",
    "title": "Temporal Correlation and Changepoints",
    "section": "\n1.2 Visualising additive models in R",
    "text": "1.2 Visualising additive models in R\nUnlike linear terms, we can’t simply report parameter estimates for smooth terms in additive models. We can instead simply use the plot function to visualise our smooth functions.\n\n\n\n\n\n\n Example: River Tweed nitrate level (continued)\n\n\n\nHere, we observe that we may have a bimodal shape, with peaks in 1999 and 2005:\n\nCodeplot(m1)\n\n\n\n\nPlot of fitted smooth of time for River Tweed log(Nitrate) data. The black points are partial residuals, the black line is the estimated smooth, and the dashed lines are the 95% interval bounds for the estimated smooth.\n\nIn reality, we might spot that there is a seasonal pattern evident in the plot, which our smooth does not capture. We could capture this by either:\n\nfitting a smooth to the months plus a smooth to years (to capture the seasonal pattern plus long term smooth trend), as we saw at the end of last week’s notes, or\nincreasing the basis dimension for our smooth, using the k argument of the s() function. (See the help file of the s function in R for details, by running ?mgcv::s.)"
  },
  {
    "objectID": "notes/notes_2.html#autocorrelation-function",
    "href": "notes/notes_2.html#autocorrelation-function",
    "title": "Temporal Correlation and Changepoints",
    "section": "\n2.1 Autocorrelation function",
    "text": "2.1 Autocorrelation function\nWe can estimate the strength of temporal dependence using a sample autocorrelation function (ACF). This function represents the autocorrelation of the data at a series of different lags in time. Assuming we have a regularly spaced time series, we compute the sample ACF at lag \\(k\\) as:\n\\[r(k) = \\frac{\\sum_{t=k+1}^n(x_t - \\bar{x})(x_{t-k} - \\bar{x})}{\\sum_{t=1}^n(x_t - \\bar{x})^2}\\]\nWe compute this for values from \\(k=1,\\ldots,K\\) where \\(K\\) is some sensible maximum lag.\nOur sample ACF is an estimate of the overall ACF, and as such we have to consider uncertainty. Typically we will compute a simple confidence interval around our point estimate at each lag as\n\\[r(k) \\pm 1.96\\sqrt{\\frac{1}{n}}\\]\nwhere \\(n\\) is the number of observations in the time series.\nWe plot the ACF function with a separate vertical line representing the size correlation at each lag, and dashed lines for the confidence intervals. If the lines lie within the confidence intervals, no autocorrelation is present.\nHere, we have several lines outside the confidence intervals, so autocorrelation exists in this dataset.\n\n\n\n\nEach of the three ACFs below are examples of cases where suggest autocorrelation is present.\n\nThe first has a repeating pattern — suggests seasonality:\n\n\n\n\n\n\nThe second has a decreasing pattern — likely caused by trend:\n\n\n\n\n\n\nThe third has both patterns — probably seasonality AND trend:\n\n\n\n\n\nIf we have identified autocorrelation in our data, we have to find a way to account for it in our model. In some cases we may choose to simply treat it as a nuisance, and make adjustments to our standard errors to reflect the reduced effective sample size.\nThe alternative is to explicitly account for the autocorrelation in our model. For seasonal patterns, we may be able to eliminate it using methods discussed previously, such as harmonics. For other types of autocorrelation, we may use approaches such as autoregressive integrated moving average (ARIMA)."
  },
  {
    "objectID": "notes/notes_2.html#autocorrelation-models",
    "href": "notes/notes_2.html#autocorrelation-models",
    "title": "Temporal Correlation and Changepoints",
    "section": "\n2.2 Autocorrelation models",
    "text": "2.2 Autocorrelation models\nAutoregressive integrated moving average (ARIMA) models are a general class of models which account for autocorrelation. These models combine aspects of two main classes of model: autoregressive (AR) and moving average (MA).\nBroadly speaking, AR(\\(p\\)) models assume that the current value is a function of the previous \\(p\\) observations. In contrast, MA(\\(q\\)) models assume that the current value can be computed by a linear regression on the \\(q\\) previous random error terms. These models are covered in more detail in the Time Series course, but will be addressed briefly here.\n\n2.2.1 AR model\nAn autoregressive (AR) model accounts for correlation by describing each value as a function of the previous values. The AR(\\(p\\)) process can be written as\n\\[X_t = \\sum_{i=1}^p \\phi_i X_{t-i} + \\epsilon_t.\\]\nHere \\(\\phi_i\\) is the “autoregressive parameter” which measures the strength of the autocorrelation. \\(\\epsilon_t \\sim \\mbox{N}(0, \\sigma^2)\\) is simply random error, often referred to as noise.\n\n2.2.2 MA model\nA moving average (MA) model accounts for correlation by describing each value as a function of the previous set of error terms. The MA(\\(q\\)) process can be written as\n\\[X_t = \\mu + \\sum_{i=1}^q \\theta_i \\epsilon_{t-i} + \\epsilon_t.\\]\nHere \\(\\mu\\) is the mean of the series and \\(\\theta_i\\) is the regression parameter associated with the \\(i\\)th lag.\n\n2.2.3 ARIMA\nThe ARIMA is a combination of AR and MA processes. The I stands for Integrated, which relates to “differencing”, i.e., replacing a value with the difference between itself and the previous value. We write this model as ARIMA(\\(p,d,q\\)), where \\(p\\) is the order of the AR process, \\(d\\) is the degree of differencing and \\(q\\) is the order of the MA.\nFor example, ARIMA(1,0,0) would be equivalent to an AR(1) model and ARIMA(0,0,1) is an MA(1) model.\nWe can use the sample ACF to suggest the appropriate model to account for our autocorrelation. A smooth decay suggests that we have AR components. A less structured ACF might suggest an MA is more appropriate. In practice, AR processes are less complex than MA and tend to be used more frequently as a result.\n\n\n\n\n\n\n Examples of ACF plots\n\n\n\n\nPlot A has a clear seasonal pattern, meaning that harmonics may be more appropriate than an ARIMA.\nIn Plot B, the value at lag 1 is outside the interval, and thus an AR(1) may be most suitable. (Note that we can likely ignore the spike at lag 12 as just random error).\nPlot C does not appear to have any correlations outside of the error bars, and so we can conclude that there is no evidence of autocorrelation. Note that the bars are wider. This is likely because we had less data available.\n\n\nARIMA methods are all based on regularly spaced data (measurements equally spaced in time). However, in some cases we may have irregularly spaced data. If the data are roughly regular (just a small deviation here and there), we may be able to treat them as though they are regular. If we have missing data, we may be able to impute or interpolate without too many issues. In cases where we have completely irregular data, we may need to use more complex statistical methods (which will not be covered in this course).\n\n2.2.4 ARIMA in R\nWe can use the arima() function in R to explore autocorrelation. We must first fit a linear model, and then extract the design matrix to use as an input to this function. For example, to fit an AR(1) model we would use the following code:\n\nCodetrend.model0 &lt;- lm(response ~ decimal.date)\n\nX &lt;- model.matrix(trend.model0)\n\ntrend.model1 &lt;- arima(y, order = c(1, 0, 0), xreg = X, \n                      include.mean = FALSE)"
  },
  {
    "objectID": "notes/notes_2.html#known-changepoint",
    "href": "notes/notes_2.html#known-changepoint",
    "title": "Temporal Correlation and Changepoints",
    "section": "\n3.1 Known changepoint",
    "text": "3.1 Known changepoint\nSometimes it is known that a change occurred at a specific timepoint, but the magnitude or shape of this change are not known.\n\n3.1.1 Known changepoint — mean shift\nSuppose that we have a series of data \\(Y_i\\) collected at a set of timepoints \\(t_i\\) with \\(i=1,\\ldots, n\\). If our known changepoint is at time \\(c\\), then we can construct an indicator function \\[\\mathcal{I}_{t_i} =\n\\begin{cases}\n0\\hspace{0.5cm} \\text{if } \\hspace{0.1cm} t_i &lt; c\\\\\n1\\hspace{0.5cm} \\text{if } \\hspace{0.1cm} t_i \\geq c\n\\end{cases}\\] This can then be included as a parameter in our regression model \\[Y_i = \\beta_0 + \\varphi\\mathcal{I}_{t_i} + \\epsilon_i\\]\nHere, \\(\\varphi\\), the coefficient of the indicator function ,can be described as the intervention effect. If this parameter is significant in our model, that implies that we have a significant change in mean at timepoint \\(c\\).\n\n3.1.2 Known changepoint — change in slope\nWe also need to consider examples where we observe a change in slope at a known timepoint.\n\n\nPlot of value by time, with data as black dots, and changepoint at time \\(c\\) illustrated by vertical dashed red line.\n\nIt would be possible to fit two separate regressions. However, this seems quite simplistic, and it would be better to have a single continuous model.\n\n\nPlot of value by time, with data as black dots, and changepoint at time \\(c\\) illustrated by vertical dashed red line. Blue regression lines represent the fitted separate regression models.\n\nWe want our regression to be continuous at \\(c\\) such that we have \\[\\alpha_1 + \\beta_1 c =  \\alpha_2 + \\beta_2 c\\] This can be rewritten in terms of a single model parameter, as \\[\\alpha_2 = \\alpha_1 + c(\\beta_1 - \\beta_2)\\] We can thus update our equations to the following, which is known as piecewise regression (or segmented regression):\n\\[\\begin{align*}\nY_i &= \\alpha_1 + \\beta_1 x_i + \\epsilon_i\\hspace{16mm} &\\mbox{ for } x&lt;c\\\\\nY_i &= \\alpha_1 +  (\\beta_1 - \\beta_2)c + \\beta_2 x_i + \\epsilon_i\\hspace{2mm} &\\mbox{ for } x\\geq c\n\\end{align*}\\]\n(Note that this could be expressed as a single model using our indicator function.)\nThe two linear parts of our model now meet at \\(c\\). Note that our piecewise model is more efficient than two separate regressions, since it uses one fewer parameter.\n\n\nPlot of value by time, with data as black dots, and changepoint at time \\(c\\) illustrated by vertical dashed red line. Grey regression lines represent the fitted piecewise regression model.\n\nIn many cases, we may have more complex changes to our trend. There are a variety of more advanced models for known changepoints, but these are all based on the same underlying principles. For example, the bent cable model allows for an extended “transition phase” between the two slopes, often represented by a smooth curve.\n\n\n\n\n\n\n Example: Chlorofluorocarbons (CFCs)\n\n\n\nChlorofluorocarbons (CFCs) are pollutants which were often used in aerosols. Their use was phased out in the 1990s as a result of environmental policy. We can see this “phasing out” period represented in the model.\n\n\nDiagram illustrating the bent cable model as fitted to CFC-11 (ppt) values between 1988 and 2000."
  },
  {
    "objectID": "notes/notes_2.html#unknown-changepoint",
    "href": "notes/notes_2.html#unknown-changepoint",
    "title": "Temporal Correlation and Changepoints",
    "section": "\n3.2 Unknown changepoint",
    "text": "3.2 Unknown changepoint\nIt can be more challenging to fit a changepoint model when you don’t clearly know exactly when the change occurred. One of the most popular methods is an iterative approach which searches across the entire range of our data for possible changepoints.\n\n\n\n\n\n\n Example: River Nile flow data\n\n\n\nWe have historic data on the levels of the River Nile around the city of Aswan, Egypt. Is there any evidence of a change in water volume? If so, when did it occur?\n\n\nRiver Nile.\n\n\n\nPlot of River Nile volume by year, Aswan.\n\nWe can examine the data by fitting a LOWESS curve. There does appear to be a change around 1900. However, we need to explore this further via a model.\n\n\nFitted LOWESS curve for River Nile data.\n\nWe use the segmented() function in R (in the package also called segmented) to fit an unknown changepoint model, using the following steps:\n\nFirst, fit a standard regression using lm().\nWe then pass the linear model into our segmented() function along with an initial estimate of the changepoint.\nThis initial estimate (psi = 1900) is used as a starting point for our iterative algorithm.\n\nWe run this in R:\n\nCodeout.lm &lt;- lm(Volume ~ Year)\nmod &lt;- segmented(out.lm, seg.Z = ~Year, psi = 1900)\n\n\npsi1.x  \n  1913  \n  \nslope(mod)\n$x\n          Est. St.Err. t value\nslope1 -8.1820   1.759  -4.650\nslope2  0.7458   1.084   0.688\nThe final model output suggests that the changepoint occurred in 1913. Prior to 1913, the volume was decreasing by 8.18 units per year. Afterwards, it was increasing by 0.75 units per year.\nThe Aswan Low Dam was constructed between 1899–1902, massively impacting river levels in the area. Therefore it is more sensible to fit a model which introduces a mean shift, rather than a change of slope. Subject matter expertise is key!\n\nIn this case, given there is a clear reason why the time series will change either side of the dam’s construction, we need to fit two separate models. The plot below shows two separate penalised spline models for the before and after periods.\n\n\nFitted LOWESS curves for River Nile data (separate for before/after 1900)."
  }
]