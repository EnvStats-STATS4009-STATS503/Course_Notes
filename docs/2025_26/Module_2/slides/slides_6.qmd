---
title: "Modelling Environmental Extremes"
embed-resources: false
format:
  revealjs:
    footer: '<a href="https://envstats-stats4009-stats503.github.io/Course_Notes/2025_26/Module_2/docs/" style="color:#ccc; padding:20px 30px; display:inline-block; margin:-20px -30px;">Home</a>'
    margin: 0
    logo:  UofG.png
    theme: uofg_theme.scss
    chalkboard: true
    header-includes: |
      <script src="custom.js" type="application/javascript"></script>
title-slide-attributes: 
  data-background-image: uog_cloistures2.jpg
  data-background-color: "#FFFFFF"
slide-number: true
include-in-header: 
  text: |
    <style>
      .custom-small table {
        font-size: .8em
      }
      .custom-tiny table {
        font-size: .6em
      }
    </style>
author:
  - name: Jafet Belmont 
    email: jafet.BelmontOsuna@glasgow.ac.uk 
    affiliations: School of Mathematics and Statistics
editor_options: 
  chunk_output_type: console
execute: 
  freeze: auto
  echo: true
filters:
  - shinylive
---

##  {background-image="river.png"}

::: {.blockquote style="color: #FFFFFF; background-color:rgb(38, 38, 38,0.8); font-size: 1.25em; padding: 20px; border-radius: 5px;"}
> "*On the subject of climate change, the data clearly show that Earth is warming up, and that humans are contributing to the warming. That’s the fairest assessment of the evidence.*"
>
> Neil deGrasse Tyson
:::

## Recap

-   For the last two weeks, we have looked at time series, and how appropriate models can be fitted that deal with the temporal structures in the data.
-   In some cases, we want to specifically look at the maxima or minima of a particular environmental system.
-   This week, we will look at methods for modelling these statistical **extremes**.

## Extremes

::: incremental
-   We are trying to model rare events - by their very definition we won't have a lot of data on these.
-   The bulk of the data in any statistical distribution will be in the centre.
-   Standard density estimation techniques (e.g., the normal distribution) work well where the data have the greatest density, but that's not what we need here.
-   We need to use a statistical model which is good at estimating the **tails** of our distribution.
:::

## Motivation

Statistical modelling of extreme environmental phenomena has a very practical motivation: reliability - anything we build needs to have a good chance of surviving the weather/environment for the whole of its working life.

This has obvious implications for civil engineers and planners. They need to know:

-   how strong to make buildings;
-   how high to build sea walls;
-   how tall to build reservoir dams;
-   how much fuel to stockpile.

## Motivation (continued)

This motivates the need to estimate what the:

-   strongest wind;
-   highest tide;
-   heaviest rainfall;
-   most severe cold-spell;

etc. will be over some fixed period of future time.

The only sensible way to do this is to use data on the variable of interest (wind, rain etc.) and fit an appropriate statistical model.

The models themselves are motivated by asymptotic theory, and this is our starting point.

## What are your thoughts?

**Join mentimeter**

![](images/clipboard-2550031199.png)

## Environmental extremes in our life

Environmental extreme events are closer than you think...

![](figures/eowyn_example.png){fig-align="center" width="100%"}

## Environmental extremes in our life

![](figures/extreme_examples.png){fig-align="center" width="100%"}

## Defining an extreme {.smaller auto-animate="true" background-color="#FFFFFF"}

-   The first thing we have to consider is what actually represents an *extreme* observation.
-   This will vary depending on the context of the dataset.

::::: columns
::: {.column width="50%"}
```{r}
#| echo: false
#| warning: false
#| message: false
#| fig-width: 4
#| fig-align: center
#| fig-height: 4

library(ismev)
library(dplyr)
library(tidyr)
library(ggplot2)

data("venice")

# Reshape to long format
venice_long <- venice %>%
  pivot_longer(
    cols = -Year,
    names_to = "period",
    values_to = "value"
  )

# Scatter plot
ggplot(venice_long, aes(x = Year, y = value)) +
  geom_point() +
  labs(
    x = "Year",
    y = "Sea level (mm)") + theme_classic()

```
:::

::: {.column width="50%"}
-   We could just look at the biggest (or smallest) values.
-   If so, over what time period?
:::
:::::

## Defining an extreme {.smaller auto-animate="true" background-color="#FFFFFF"}

-   The first thing we have to consider is what actually represents an *extreme* observation.
-   This will vary depending on the context of the dataset.

::::: columns
::: {.column width="50%"}
```{r}
#| echo: false
#| warning: false
#| message: false
#| fig-width: 4
#| fig-align: center
#| fig-height: 4


# Scatter plot
ggplot(venice_long, aes(x = Year, y = value)) +
  geom_point() +
  labs(
    x = "Year",
    y = "Sea level (mm)") + theme_classic()+
  geom_vline(xintercept = unique(venice_long$Year), 
             linetype = "dashed", alpha = 0.3, color = "tomato")+
    stat_summary(
    aes(x = Year, y = value), 
    fun = max,
    geom = "point",
    color = "red",
  )

```
:::

::: {.column width="50%"}
-   We could just look at the biggest (or smallest) values.
-   If so, over what time period? Annual maximum? Over the whole dataset? Both?
:::
:::::

## Defining an extreme {.smaller auto-animate="true" background-color="#FFFFFF"}

-   The first thing we have to consider is what actually represents an *extreme* observation.
-   This will vary depending on the context of the dataset.

::::: columns
::: {.column width="50%"}
```{r}
#| echo: false
#| warning: false
#| message: false
#| fig-width: 4
#| fig-align: center
#| fig-height: 4

highlight_years <- seq(min(venice_long$Year), 
                       max(venice_long$Year), 
                       by = 5)
highlight_data <- venice_long %>%
  filter(Year %in% highlight_years)

ggplot(venice_long, aes(x = Year, y = value)) +
  geom_point() +
  geom_vline(xintercept = highlight_years, 
             linetype = "dashed", color = "tomato", alpha = 0.5) +
  labs(x = "Year", y = "Sea level (mm)") + 
  theme_classic()+
stat_summary(
    data = highlight_data,
    aes(group = Year),
    fun = max,
    geom = "point",
    color = "red"
  ) 

```
:::

::: {.column width="50%"}
-   We could just look at the biggest (or smallest) values.
-   If so, over what time period? Annual maximum? Over the whole dataset? Both?
:::
:::::

## Defining an extreme {.smaller auto-animate="true" background-color="#FFFFFF"}

-   The first thing we have to consider is what actually represents an *extreme* observation.
-   This will vary depending on the context of the dataset.

::::: columns
::: {.column width="50%"}
```{r}
#| echo: false
#| warning: false
#| message: false
#| fig-width: 4
#| fig-align: center
#| fig-height: 4

ggplot(venice_long, aes(x = Year, y = value)) +
  geom_point() +
  geom_point(data=venice_long%>%filter(value > 120),color="tomato")+
  geom_hline(yintercept = 120, 
             linetype = "dashed", color = "tomato", alpha = 0.5) +
  labs(x = "Year", y = "Sea level (mm)") + 
  theme_classic()

```
:::

::: {.column width="50%"}
-   We could just look at the biggest (or smallest) values.
-   If so, over what time period? Annual maximum? Over the whole dataset? Both?
-   Other times, it may be all observations above a certain threshold.
:::
:::::

## Defining an extreme {.smaller auto-animate="true" background-color="#FFFFFF"}

-   The first thing we have to consider is what actually represents an *extreme* observation.
-   This will vary depending on the context of the dataset.

::::: columns
::: {.column width="50%"}
![](figures/sea_level_thresholds.gif){fig-align="center"}

```{r}
#| echo: false
#| warning: false
#| message: false
#| fig-width: 4
#| fig-align: center
#| fig-height: 4
#| eval: false

library(gganimate)
library(dplyr)

# Define thresholds
thresholds <- c(80, 100, 120, 140)

# Prepare data
venice_animated <- venice_long %>%
  crossing(threshold = thresholds) %>%
  mutate(above_threshold = value > threshold)

# Create the plot
p <- ggplot(venice_animated, aes(x = Year, y = value)) +
  geom_point() +
  geom_point(data = ~filter(.x, above_threshold), 
             color = "tomato") +
  geom_hline(aes(yintercept = threshold), 
             linetype = "dashed", color = "tomato") +
  labs(
    x = "Year", 
    y = "Sea level (mm)",
    title = "Threshold: {closest_state} mm"
  ) + 
  theme_classic() +
  theme(
    # Increase axis text size
    axis.text = element_text(size = 14),      # Both axes
    axis.title = element_text(size = 16),     # Axis titles
    plot.title = element_text(size = 18)      # Plot title
  )+
  transition_states(
    threshold,
    transition_length = 2,
    state_length = 2
  ) +
  ease_aes('cubic-in-out')

# Animate and save as GIF
anim <- animate(p, 
                nframes = 100,  # Total frames
                fps = 10,       # Frames per second
                width = 800,
                height = 600,
                renderer = gifski_renderer())  # Use gifski for better quality

# Save to file
anim_save("sea_level_thresholds.gif", anim)

```
:::

::: {.column width="50%"}
-   We could just look at the biggest (or smallest) values.
-   If so, over what time period? Annual maximum? Over the whole dataset? Both?
-   Other times, it may be all observations above a certain threshold.
-   But then how to select the threshold?
:::
:::::

## Extreme Value theory {.smaller}

::: {style="height: 50px;"}
:::

Extreme value modelling has a central theoretical result, analogous to the Central Limit Theorem...

-   Suppose we have a series of random variables $X_1, \ldots, X_n$, each with cumulative distribution function $F$, where $F(x) = \text{P}(X \leq x).$
-   We can define the maximum of this set of random variables as $M_n = \max{\{ X_1, \ldots, X_n\}}$.
-   Then we can show that $\text{P}(M_n \leq x) = P(X_1 \leq x, \ldots X_n \leq x) = \{F(x)\}^n = F^n(x)$
-   The block maxima approach focuses on understanding and estimating this function $F(x)$

## Extreme Value Theory {.smaller}

::: incremental
-   The true cumulative distribution function, $F(x)$, is unknown.

-   We could replace it with an estimate, but:

    -   Any small differences between the estimate and the truth for $F(x)$ could lead to large differences in $F^n(x)$.
    -   As $n \to \infty$ then the value of $F^n(x)$ will be 0 or 1 depending on whether $F(x) <1$ or $F(x)=1$. This means that the limit distribution of the maxima is a degenerate distribution.

-   Instead, we tend to focus on the limiting distribution of $F^n(x)$ as $n \to \infty$, .i.e., the *limiting distribution of the normalized maximum* by introducing some constants to stabilize the maximum as $n$ grows.

-   This is a distribution $G(x)$ such that, for constants $a_n > 0$ and $b_n$,

    $$\text{P}\left(\frac{M_n - b_n}{a_n} \leq x\right) = F^n(a_n x + b_n) \to G(x) \text{ as } n \to \infty$$

    $G$ belongs to the type of one of three distribution functions.
:::

## Extreme Value Distributions

There are three main families of extreme value distribution which have our desired properties as limiting distributions of $F^n(x)$.

**Gumbel**: $G(x) = \exp \left( -\exp [\frac{-(x-a_n)}{b_n} ]\right)$

**Fréchet**: $G(x) = \exp \left( -[\frac{(x-a_n)}{b_n} ]^{-\xi}\right)$ for $z>b_n$, 0 otherwise.

**Weibull**: $G(x) = \exp \left( -[\frac{-(x-a_n)}{b_n} ]^{\xi}\right)$ for $z>b_n$, 1 otherwise.

Here, $a_n$ is a location parameter, $b_n$ is a scale parameter and $\xi$ is a shape parameter.

# Block Maxima

## Block Maxima

-   A lot of investigation of environmental extremes will focus on time series data.
-   Within time series data, we typically have natural groupings or blocks of observations (days, months, years etc).
-   Therefore a common approach for modelling extremes focuses on the idea of **block maxima** - identifying the maximum (or minimum) value in each block.
-   For example, if we have daily temperature data measured over 100 years, we could look at the highest temperature in each year.

## Block Maxima Approach

::: incremental
-   Break up our sequence $X_1,X_2,\ldots$ into blocks of size $n$ (with $n$ reasonably large), and extract only the maximum observation from each block.
-   Now we fit $G(x)$ to the sequence of extracted maxima $M_{(1)},M_{(2)},\ldots , M_{(N)}$ and use this as the basis for statistical inference.
-   For example, consider the annual maxima of daily rainfall. Here our blocks have $n=365$ observations, which is reasonably large, so we fit our model to $N$ annual maxima (where $N$ is the number of years).
-   This rough and ready approach has shown itself to be surprisingly robust.
:::

## The GEV Distribution

-   More generally, we can model the maxima using the Generalised Extreme Value (GEV) distribution: $$G(x) = \exp \left( - \left[ 1 + \frac{\xi (x - \mu)}{\sigma} \right]^{-\frac{1}{\xi}} \right)$$
-   Here, $\mu$ is the location parameter, $\sigma$ is the scale parameter and $\xi$ is the shape parameter.
-   The Gumbel, Fréchet and Weibull distributions are all special cases depending on the value of $\xi$.

## The GEV Distribution {background-color="#FFFFFF"}

::::: columns
::: {.column width="50%"}
$$G(x) = \exp \left( - \left[ 1 + \frac{\xi (x - \mu)}{\sigma} \right]^{-\frac{1}{\xi}} \right)$$

-   If $\xi < 0$ then we have the Weibull distribution.
-   If $\xi > 0$ then we have the Fréchet distribution.
-   As $\xi \to 0$ then we have the Gumbel distribution.
:::

::: {.column width="50%"}
![](figures/GEV.png){width="100%" fig-align="center"}
:::
:::::

## Example

In this course we will focus on the statistical inference of real data for which the underlying distribution $F(\cdot)$ is unknown.

Thus, the following example just illustrates how the choice of normalizing constants lead to a limit distribution within the GEV family.

::: {.callout-note icon="false"}
## Example

Let $X_1,\ldots,X_n \overset{iid}{\sim} F$, where $F$ is Uniform(0,1). If we set the normalizing constants as:

$$
a_n = \frac{1}{n} ~~~~~b_n=1,
$$

the limiting distribution of the maxima $M_n$ as $n\to \infty$ is a GEV with $\mu= -1$ ,$\sigma = 1$ and $\xi = -1$
:::

## Example: Solution {.smaller}

Recall that if $X_1,X_2,\ldots$ are a sequence of independent uniform U(0, 1) then, $F(x) = x ~(\text{for } 0 \leq x \leq 1)$

$$
\begin{aligned}
\mathbb{Pr}\left(\dfrac{M_n -b_n}{a_n}\leq x \right) & = \mathbb{Pr}(M_n \leq b_n +a_n x)\\
&= F\left(b_n +a_n x\right)^n\\
& \class{fragment}{= F\left(1 + \frac{1}{n}x\right)^n}\\
& \class{fragment}{= \left(1 + \frac{1}{n}x\right)^n ~~~\text{for } 0 \leq 1 + x/n   \leq 1}\\
\end{aligned}
$$

## Example: Solution {.smaller}

Recall that if $X_1,X_2,\ldots$ are a sequence of independent uniform U(0, 1) then, $F(x) = x ~(\text{for } 0 \leq x \leq 1)$

$$
\begin{aligned}
\mathbb{Pr}\left(\dfrac{M_n -b_n}{a_n}\leq x \right) & = \mathbb{Pr}(M_n \leq b_n +a_n x)\\
&= F\left(b_n +a_n x\right)^n\\
&= F\left(1 + \frac{1}{n}x\right)^n\\
&= \left(1 + \frac{1}{n}x\right)^n ~~~\text{for } 0 \leq 1 + x/n   \leq 1\\
& \class{fragment}{ \rightarrow \exp x~ \text{as }  n \to \infty}
\end{aligned}
$$

::: fragment
Using the fact that

$$
\lim_{n \to \infty}\left( 1 + \frac{1}{n} x\right)^n  = \exp{x}
$$
:::

## Example: Solution {.smaller}

Recall that if $X_1,X_2,\ldots$ are a sequence of independent uniform U(0, 1) then, $F(x) = x ~(\text{for } 0 \leq x \leq 1)$

$$
\begin{aligned}
\mathbb{Pr}\left(\dfrac{M_n -b_n}{a_n}\leq x \right) & = \mathbb{Pr}(M_n \leq b_n +a_n x)\\
&= F\left(b_n +a_n x\right)^n\\
&= F\left(1 + \frac{1}{n}x\right)^n\\
&= \left(1 + \frac{1}{n}x\right)^n ~~~\text{for } 0 \leq 1 + x/n   \leq 1\\
&  \rightarrow \exp x~ \text{as }  n \to \infty
\end{aligned}
$$

::: fragment
Let $G(x) = \exp \left\{- (1 + \xi \frac{x -\mu}{\sigma})^{-1/xi}\right\}$, setting $\mu= -1$ ,$\sigma = 1$ and $\xi = -1$ leads to:
:::

::: fragment
$$
G(x) =  \exp \left\{- (1 -x + 1 )\right\} = \exp x
$$
:::

## Generating samples from Block Maxima in R {background-color="#FFFFFF"}

::: {style="font-size: 80%;"}
```{r}
#| output-location: column
#| fig-align: center
#| fig-width: 4
#| fig-height: 4 
#| echo: true


generate.max <- function(N = 50,n=5,
                         rdist,param=c(0,1),
                         a=1,b=0,seed=pi){
  set.seed(seed)
  Mn <- c()
  for(i in 1:N){
    sample <- rdist(n,param[1],param[2])
    Mn[i]<- (max(sample)-b)/a
  }
  hist(Mn,breaks = "FD",
       col="lightblue",proba=T,
       main="Histogram of maximums",xlab='')
}

generate.max(N=50000,n=5,rdist=runif,
             param=c(0,1),a=1/5,b=1)
curve(ExtremalDep::dGEV(x,-1,1,-1),type="l",
      n=500,add=T,lwd=3,lty=1,col=2)

```
:::

## Generating samples from Block Maxima in R

::: {style="height: 50px;"}
:::

::::: {style="font-size: 80%;"}
::: {.callout-note icon="false"}
For $X_1,\ldots,X_n \overset{iid}{\sim} F$, where $F$ is Uniform(0,1), if we normalise the maximum using $b_n=1$ and $a_n=1/n$, we know that the asymptotic distribution of $\dfrac{M_n -b_n}{a_n}$ is a GEV($\mu$=−1,$\sigma$=1,$\xi$=−1).
:::

::: {.callout-note icon="false"}
Likewise, if $X_1, X_2 , \ldots$ is a sequence of independent standard exponential $Exp(1)$ variables, using $b_n=1$ and $a_n=\log n$ the asymptotic distribution of the normalised maxima is a GEV($\mu=0,\sigma=1,\xi=0$) i.e., a Gumbell distribution.
:::

-   Let’s see what this looks like, for a few different values of n
:::::

## Generating samples from Block Maxima in R {background-color="#FFFFFF"}

```{shinylive-r}
#| standalone: true
#| viewerHeight: 500

library(shiny)

# Function to generate block maxima
generate_max <- function(N, n, rdist, param, a, b, seed=pi){
  set.seed(seed)
  Mn <- numeric(N)
  for(i in 1:N){
    sample <- rdist(n, param[1], param[2])
    Mn[i] <- (max(sample) - b) / a
  }
  return(Mn)
}

# UI
iu <- fluidPage(
  titlePanel("Block Maxima Simulation"),
  sidebarLayout(
    sidebarPanel(
      selectInput("dist", "Choose Distribution:", choices = c("Uniform(0,1)" = "uniform", "Exponential(1)" = "exponential")),
      sliderInput("n", "Observations per Block (n):", min = 5, max = 100, value = 5, step = 5),
      sliderInput("N", "Number of Blocks (N):", min = 10000, max = 50000, value = 50000, step = 10000)
    ),
    mainPanel(
      plotOutput("histogram")
    )
  )
)

# Server
server <- function(input, output) {
  output$histogram <- renderPlot({
    if (input$dist == "uniform") {
      a <- 1 / input$n
      b <- 1
      Mn <- generate_max(input$N, input$n, runif, c(0,1), a, b)
      true_gev <- function(x) evd::dgev(x, loc = -1, scale = 1, shape = -1)
    } else {
      a <- 1
      b <- log(input$n)
      Mn <- generate_max(input$N, input$n, rgamma, c(1,1), a, b)
      true_gev <- function(x) evd::dgev(x, loc = 0, scale = 1, shape = 0)
    }
    
    hist(Mn, breaks = "FD", col = "lightblue", prob = TRUE, main = "Histogram of Block Maxima", xlab = "Maxima")
    curve(true_gev, col = "red", lwd = 2, add = TRUE)
  })
}

# Run App
shinyApp(ui = iu, server = server)

```

## Maxima and Minima {.smaller background-color="#FFFFFF"}

-   Communication of extremes typically focuses on maxima (or minima).
-   Environmental or climate events are often described as the "worst/highest/lowest in X years".

::::: columns
::: {.column width="50%"}
![](figures/EuropeDrought.png){width="100%"}
:::

::: {.column width="50%"}
![](figures/BangladeshFloods.png){width="100%"}
:::
:::::

## Return Levels

::: incremental
-   In statistics, this idea of the "highest in X years" can be related to the idea of a **return level** and **return period**.
-   The return level $z_p$ is the value we would expect to be exceeded once every $p$ years, where $\frac{1}{p}$ is the return period.
-   The return level can also be thought of as the value which has probability $\frac{1}{p}$ of being exceeded in a given year.
-   Now consider the statement: *"The temperature in Glasgow will reach 20 degrees once every 50 years"*.
-   Here, we have return period $\frac{1}{50}=0.02$ and return level $z_p=20$.
:::

## Return Levels and the GEV

-   The return level $z_p$ is the $(1-\frac{1}{p})$ quantile of the GEV distribution, since we have a probability $\frac{1}{p}$ of the maximum exceeding that value, i.e., $P(M_n > z_p) = \frac{1}{p}$.
-   Recall that the GEV takes the form $$
    \begin{aligned}
    G(z_p) &= \begin{cases} 
      \exp \left( - \left[ 1 + \frac{\xi (z_p - \mu)}{\sigma} \right]^{-\frac{1}{\xi}} \right) &, \xi \neq 0 \\
      \exp\left(-\exp\left(-\frac{z_p-\mu}{\sigma}\right)\right) &, \xi = 0 
    \end{cases}\\
    G(z_p) &= P(M_n \leq z_p) = 1 - P(M_n > z_p)
    \end{aligned}
    $$
-   Therefore the return level can be obtained by inverting this distribution to obtain $$
    z_p = \begin{cases}
      \mu - \frac{\sigma}{\xi}\left[ 1 - \{ -\log(1-\frac{1}{p}) \}^\xi \right] & \xi \neq 0\\
      \mu - \sigma \log\{ -\log(1-\frac{1}{p})\} & \xi = 0
    \end{cases}
    $$

## Example - Venice Sea Levels {background-color="#FFFFFF"}

::::: columns
::: {.column width="45%"}
-   Sea levels in Venice are rising.
-   The city floods on a regular basis.
-   What sea level can we expect in the next 5, 10, 100 years?
:::

::: {.column width="55%"}
![](figures/GuardianVenice.png){width="100%"}
:::
:::::

## Example - Venice Sea Levels {background-color="#FFFFFF"}

-   We have daily sea level measurements from 1931-1981.
-   The plot below shows the 10 highest sea level measurements from each year.

```{r}
#| echo: false
#| warning: false
#| message: false
#| fig-align: center
#| fig-width: 4
#| fig-height: 4

library(ismev)
library(dplyr)
library(ggplot2)
library(tidyr)

data("venice")

# Reshape to long format
venice_long <- venice %>%
  pivot_longer(
    cols = -Year,
    names_to = "period",
    values_to = "value"
  )

# Scatter plot
ggplot(venice_long, aes(x = Year, y = value)) +
  geom_point() +
  labs(
    x = "Year",
    y = "Sea level")
```

## Example - Venice Sea Levels {background-color="#FFFFFF"}

-   We can apply a block maxima approach, treating each year as a block.
-   This requires us to identify and model the yearly maximum.

```{r}
#| echo: false
#| warning: false
#| message: false
#| fig-align: center
#| fig-width: 4
#| fig-height: 4

ggplot(venice_long, aes(x = Year, y = value)) +
  geom_point() +
  stat_summary(
    fun = max,
    geom = "point",
    color = "red"
  ) +
  labs(x = "Year", y = "Sea level") 
```

## Fitting the GEV in R {background-color="#FFFFFF"}

-   We use the `extRemes` package to fit a GEV distribution in R
-   The `fevd()` simply takes a data vector of the annual maxima and provides parameter estimates using maximum likelihood estimation.

```{r}
#| warning: false
#| message: false

library(extRemes)
Mn <- apply(venice[,-1],1,function(x)max(x,na.rm = T))
fit_gev <- fevd(Mn,method="MLE")
fit_gev$results$par
```

-   From the output we can see that $\mu = 111.1$, $\sigma = 17.2$ and $\xi = -0.077$.

## Return Level Plot {.smaller background-color="#FFFFFF"}

-   We can assess the suitability of the GEV distribution using a return level plot.
-   This involves commuting the return level at various return periods, and comparing it to the theoretical result under the GEV.

:::::: columns
:::: {.column width="45%"}
::: incremental
-   The observed points lie along the theoretical line.
-   Our proposed GEV distribution is appropriate.
-   The Venice maxima follow a Gumbel distribution.
:::
::::

::: {.column width="55%"}
```{r}
#| fig-align: center
#| fig-width: 4.5
#| fig-height: 4.5
plot(fit_gev, type = "rl")
```
:::
::::::

## Extending Block Maxima - $k$-largest {background-color="#FFFFFF"}

-   Looking at just the maxima means we throw away a lot of data, making it harder to accurately estimate parameters.
-   We could instead follow an approach which models the $k$ largest values within a block.

```{r}
#| echo: false
#| warning: false
#| message: false
#| fig-align: center
#| fig-width: 6
#| fig-height: 4.5

nth_largest <- function(x, n) {
  sort(x, decreasing = TRUE)[n]
}

ggplot(venice_long, aes(x = Year, y = value)) +
  geom_point() +
  stat_summary(fun = function(x) nth_largest(x, 1), geom = "point",
                aes(color = "1st max")) +
  stat_summary(fun = function(x) nth_largest(x, 2), geom = "point",
               aes(color = "2nd max")) +
  stat_summary(fun = function(x) nth_largest(x, 3), geom = "point",
                aes(color = "3rd max")) +
  labs(x = "Year", y = "Sea level") +
  scale_color_manual(name="",values=c("tomato","purple","turquoise"))


```

## Choice of Block Size

::: incremental
-   We have to make difficult subjective choices when fitting a block maxima model.
-   What do we choose as our block? Week? Month? Year? Decade?
-   Bigger blocks means we have fewer data points, but smaller blocks mean our 'extremes' might not be extreme at all, violating the assumptions of the GEV distribution.
-   If we use a $k$-largest approach, we have a similar decision on what value of $k$ to choose.
:::

# Peak Over Threshold

## Drawbacks of Block Maxima

-   Block maxima methods work well in many situations, and take advantage of natural blocks in the data.
-   However, it does not work well if there is a lot of variability between blocks.
-   In that scenario, some blocks may have many more large counts than others, and much of the data will be discarded.
-   We can overcome this by using a threshold approach, which models all observations which exceed some pre-specified value.

## Peak Over Threshold

-   This approach is known as **peak over threshold (POT)** modelling.
-   Again, we assume we have data represented by a time series, and some threshold $u$.
-   We need a statistical model for the values which are above $u$, known as **exceedances**.
-   Sometimes we may also wish to model the *number* of exceedances.

## Peak Over Threshold {background-color="#FFFFFF"}

-   Again, let $X_1, \ldots, X_n$ be a sequence of independent random variables with a common distribution function $F$.
-   We can consider our extreme values in terms of their **threshold excess** (how much they exceed the threshold by).
-   For an extreme value $X > u$, its threshold excess is given as $y = X - u$.

![](figures/POT.png){width="100%"}

## Peak Over Threshold

The probability of threshold excess of size $y$ is given by

$$
\begin{aligned}
P(X > u+y~|~X > u) &= \frac{P(X>u+y)}{P(X>u)} \quad\text{where } y > 0\\
 &=  \frac{1 - F(u + y)}{1 - F(u)}
\end{aligned}
$$

![](figures/POT2.png){width="50%" fig-align="center"}

## Generalised Pareto

-   The function $F$ is still unknown, but the distribution of all threshold excesses can be approximated by a **Generalised Pareto distribution (GPD)**.
-   The cdf of the Generalised Pareto distribution is given by $$
    P(X \leq y~|~X>u) = G(y) = 
    \begin{cases}
    1 - \left( 1 + \frac{\xi (y-u)}{\sigma} \right)^{-\frac{1}{\xi}} & \xi \neq 0\\
    1 - \exp \left( - \frac{y-u}{\sigma} \right) & \xi = 0
    \end{cases}
    $$
-   Again, $u$ is the location parameter, $\sigma$ is the scale parameter and $\xi$ is the shape parameter.

## Return Levels for POT Models

-   We can define a return level for POT models in a roughly similar way to block maxima models.
-   The $m$-observation return level, $x_m$ is defined as the level expected to be exceeded once every $m$ observations, with $$
    x_m = 
    \begin{cases}
    u + \frac{\sigma}{\xi} \left[ \left(m P(X>u)\right)^\xi - 1 \right] & \xi \neq 0\\
    u + \sigma \log\left(m P(X>u)\right) & \xi = 0
    \end{cases}
    $$
-   For any given observation, the probability of exceeding $x_m$ is simply $\frac{1}{m}$.

## Derivation of the return level when $\xi=0$ {.smaller}

:::::: columns
::: {.column width="40%"}
Recall that

1.  the probability of exceeding $x_m$ is given by

$$
\begin{aligned}
\mathbb{Pr}(x > x_m) &= \mathbb{Pr}(x > x_m|x > u)\mathbb{Pr}(x>u) \\
&= 1/m
\end{aligned}
$$

2.  the GEV with $\xi = 0$ is

$$\underbrace{G(x_m)}_{\mathbb{Pr}(x\leq x_m | x >u)} = 1- \underbrace{\exp \left(-\dfrac{x_m -u}{\sigma}\right)}_{\mathbb{Pr}(x>x_m|x>u)}$$
:::

:::: {.column width="60%"}
::: {style="height: 50px;"}
:::

$$
\begin{aligned}
\mathbb{Pr}(x > x_m) &= \mathbb{Pr}(x > x_m|x > u)\mathbb{Pr}(x>u)\\
& \class{fragment}{ = \exp \left(-\dfrac{x_m -u}{\sigma}\right) \mathbb{Pr}(x>u) = \frac{1}{m}}\\
& \class{fragment}{ \Rightarrow ~ \exp \left(-\dfrac{x_m -u}{\sigma}\right) = \dfrac{1}{m\mathbb{Pr}(x>u)}}\\
& \class{fragment}{-\dfrac{x_m -u}{\sigma}= \log (1) -\log (m\mathbb{Pr}(x>u))} \\
  &~~~~~~~~~~~~~~~~~~~\class{fragment}{\vdots} \\
 &~~~~~~~~~~~~ \class{fragment}{x_m =  u + \sigma \log (m\mathbb{Pr}(x>u))}
\end{aligned}
$$
::::
::::::

## Choosing a Threshold {background-color="#FFFFFF"}

-   We need a threshold low enough that we have sufficient data, but high enough that values above it are genuinely extreme.

![](figures/Thresholds.png){width="65%" fig-align="center"}

## Choosing a Threshold

::: incremental
-   Occasionally there is a natural choice of threshold (e.g., a legal limit for a pollutant), but generally we need to choose it.
-   One approach is to use a **mean residual life plot**, which plots the sample mean excess (mean of $x>u$) at a variety of thresholds $u$.
-   If the GPD is appropriate, the mean excess should be linearly related to the threshold.
-   Therefore, we can identify a suitable threshold as one which lies within an area of linearity on this plot.
:::

## Example - Fort Collins, Colorado {background-color="#FFFFFF"}

-   We have daily precipitation data from 1900-1999, obtained from a rain gauge in Fort Collins, Colorado, taken from Katz et al, 2002.

![](figures/FortCollinsData.png){fig-align="center"}

## Example - Fort Collins, Colorado {background-color="#FFFFFF"}

We compare three different choices of threshold below ($u = 0.5, 1.0, 1.5$) to show the importance of getting the choice right.

::::: columns
::: {.column width="70%"}
![](figures/FortCollinsThresholds.png){width="100%" fig-aligna="center"}
:::

::: {.column width="30%"}
| $u$ | $\% > u$ | $n > u$ |
|-----|----------|---------|
| 0.5 | 2.08     | 759     |
| 1.0 | 0.58     | 213     |
| 1.5 | 0.25     | 91      |
:::
:::::

## Mean Residual Life Plot {background-color="#FFFFFF"}

-   We can fit a mean residual life plot to identify a sensible choice of threshold.
-   It appears that a value of $u$ somewhere between 1.2 and 1.8 would be an appropriate choice here - this is where the plot appears to be linear.

![](figures/MeanResidualPlot.png){width="100%" fig-align="center"}

## Sensitivity Analysis {background-color="#FFFFFF"}

-   We can also carry out a sensitivity analysis to see the effect of choosing different threshold values on the estimated model parameters.
-   The plot below shows the parameter estimates at different thresholds - they seem fairly robust.

![](figures/Sensitivity.png){width="60%" fig-align="center"}

## POT Models in R

-   We can use the `extRemes` package to fit a Generalised Pareto distribution in R.
-   The function `fevd` allows several extreme value distributions (including GEV and GPD) to be fitted, and can also provide return levels.

```{r}
#| echo: true
#| eval: false
fitGP <- fevd(Fort, threshold=1.5, type="GP",
                    time.units="365/year")

return.level(fitGP, c(10,20,100), type="GP")
```

```         
[1] "Return Levels for period units in years"
 10-year level  20-year level 100-year level
      2.857184       3.340219       4.581339
```

## Correlation

::: incremental
-   Threshold exceedances are not always independent due to temporal correlation.

-   If we have high temperatures today, it's likely we might also have high temperatures tomorrow.

-   We have to account for this dependence within our model, for example by using the autoregressive approaches outlined in the time series section.

-   Alternatively, we could use a 'declustering' approach which identifies these temporal clusters and simply uses the cluster maxima.
:::

## Summary

::: incremental
-   Estimating extremes is challenging and the results can be unreliable even with large datasets.

-   We have identified two main approaches for dealing with these data - block maxima and point over threshold.

-   The block maxima approach uses the Generalised Extreme Value (GEV) distribution to model the highest value(s) within a specific block of time.

-   The point over threshold approach uses the Generalised Pareto distribution (GPD) to model all observations which exceed a certain value.
:::
