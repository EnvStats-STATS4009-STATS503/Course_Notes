{
  "hash": "632d7288fa474534ebf184639b1fb9fc",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Modelling Environmental Extremes\"\nformat:\n  html:    \n    fontsize: \"12\"\n    embed-resources: false\n    mainfont: serif\n    code-link: true\n    code-fold: true\n    code-tools:\n      source: false\n      toggle: true\n    toc: true\n    toc-location: left\n    toc-title: Contents\n    number-sections: true\n  PrettyPDF-pdf:\n    number-sections: true\nexecute: \n  freeze: auto\neditor: visual\nembed-resources: false\neditor_options: \n  chunk_output_type: console\nbibliography: references.bib\nfilters: \n  - shinylive\n---\n\n::: {.cell}\n\n:::\n\n\n\n\n\n\n# Overview\n\nWe might want to know more about the maxima and minima of the environmental system we are modelling. For example, we might want to know how strong to make buildings to deal with earthquakes, how high to build flood defences, or how to prepare for extreme low temperatures.\n\nWe are trying to model rare events --- by their very definition we won't have a lot of data on these. The bulk of the data in any statistical distribution will be in the centre. Standard density estimation techniques (eg the normal distribution) work well where the data have the greatest density, but that's not what we need here. We need to use a statistical model which is good at estimating the tails of our distribution.\n\n# Extreme Value Distributions\n\nThe first thing we have to consider is what actually represents an *extreme* observation. This will vary depending on the context of the dataset.\n\nA lot of investigation of environmental extremes will focus on time series data. Within time series data, we typically have natural groupings or blocks of observations (days, months, years etc) Therefore a common approach for modelling extremes focuses on the idea of **block maxima** --- identifying the maximum (or minimum) value in each block. For example, if we have daily temperature data measured over 100 years, we could look at the highest temperature in each year.\n\nSuppose we have a series of random variables $X_1, \\ldots, X_n$, each with cumulative distribution function $F$, where $F(x) = \\text{P}(X \\leq x)$. We can define the maximum of this set of random variables as $M_n = \\max{\\{ X_1, \\ldots, X_n\\}}$. Then we can show that $\\text{P}(M_n \\leq x) = P(X_1 \\leq x, \\ldots X_x \\leq x) = \\{F(x)\\}^n = F^n(x)$.\n\nThis suggests that to understand block maxima (the maximum value in blocks of observations), we might focus on estimating $F^n(x)$. However, this approach faces two fundamental problems:\n\n1.  **The Estimation Problem** - The true cumulative distribution $F(x)$ is unknown. While we could estimate it from data, small errors in estimating $F(x)$ - especially in the tail where $F(x)$ is close to 1 - are magnified exponentially when raised to the power $n$. A tiny underestimation of the tail probability can lead to dramatically underestimating $F^n(x)$.\n\n2.  **The Degeneracy Problem** - Note that as $n \\to \\infty$ then the value of $F^n(x)$ will be 0 or 1 depending on whether $F(x) <1$ or $F(x)=1$. This means that the limit distribution of the maxima is a degenerate distribution. In simple terms, if you keep taking more and more samples, the maximum just keeps increasing until it effectively becomes a fixed extreme value, with no interesting variation left to model.\n\nInstead of trying to estimate $F$ or $F^n$ directly, **Extreme Value Theory** shifts focus to the **limiting distribution of the normalized maximum**. We introduce sequences $a_n > 0$ and $b_n$ to stabilize the maximum as $n$ grows. This is a distribution $G(x)$ such that, for constants, $a_n > 0$ and $b_n$,\n\n$$\\text{P}\\left(\\frac{M_n - b_n}{a_n} \\leq x\\right) = F^n(a_n x + b_n) \\to G(x) \\text{ as } n \\to \\infty$$\n\nThere are three main families of extreme value distribution which have our desired properties as limiting distributions of $F^n(x)$.\n\n-   **Gumbel**: $G(x) = \\exp \\left( -\\exp [\\frac{-(x-a)}{b} ]\\right)$\n\n-   **Frechet**: $G(x) = \\exp \\left( -[\\frac{(x-a)}{b} ]^{-\\alpha}\\right)$ for $z>b$, 0 otherwise.\n\n-   **Weibull**: $G(x) = \\exp \\left( -[\\frac{-(x-a)}{b} ]^{\\alpha}\\right)$ for $z>b$, 1 otherwise.\n\nHere, $a$ is a location parameter, $b$ is a scale parameter and $\\alpha$ is a shape parameter.\n\nMore generally, we can model the maxima using the Generalised Extreme Value (GEV) distribution $$G(x) = \\exp \\left( - \\left[ 1 + \\frac{\\xi (z - \\mu)}{\\sigma} \\right]^{-\\frac{1}{\\xi}} \\right)$$\n\nHere, $\\mu$ is the location parameter, $\\sigma$ is the scale parameter and $\\xi$ is the shape parameter. The Gumbel, Frechet and Weibull distributions are all special cases depending on the value of $\\xi$.\n\n![](images/GEV.png){fig-align=\"center\" width=\"427\"}\n\n-   If $\\xi < 0$ then we have the Weibull distribution.\n-   If $\\xi > 0$ then we have the Frechet distribution.\n-   As $\\xi \\to 0$ then we have the Gumbel distribution.\n\n::: {.callout-important icon=\"false\"}\n## {{< bi box color=#005C8A >}} Task\n\nLet a random vector $\\mathbf{X} = X_1,\\ldots,X_n$ such that $\\mathbf{X}\\overset{iid}{\\sim}\\mathrm{Uniform}(0,1)$. Setting the normalizing constants $a_n = 1/n$ and $b_n =1$, show that the limiting distribution of the maxima $M_n =\\mathrm{max}(\\mathbf{X})$ as $n\\to \\infty$ is a GEV with $\\mu= -1$ ,$\\sigma = 1$ and $\\xi = -1$\n\n\n<div class='webex-solution'><button>Take Hint</button>\n\n\nRecall that the CDF of an U(0,1) is given by $F(x) = x ~(\\text{for } 0 \\leq x \\leq 1)$, also note that:\n\n$$\n\\lim_{n \\to \\infty}\\left( 1 + \\frac{1}{n} x\\right)^n  = \\exp{x}\n$$\n\n\n</div>\n\n\n\n\n<div class='webex-solution'><button>Solution</button>\n\n\n$$\n\\begin{aligned}\n\\mathbb{Pr}\\left(\\dfrac{M_n -b_n}{a_n}\\leq x \\right) & = \\mathbb{Pr}(M_n \\leq b_n +a_n x)\\\\\n& = F\\left(b_n +a_n x\\right)^n\\\\\n&= F\\left(1 + \\frac{1}{n}x\\right)^n\\\\\n& = \\left(1 + \\frac{1}{n}x\\right)^n ~~~\\text{for } 0 \\leq 1 + x/n   \\leq 1 \\\\\n&\\rightarrow \\exp x~ \\text{as }  n \\to \\infty\n\\end{aligned}\n$$\nLet $G(x) = \\exp \\left\\{- (1 + \\xi \\frac{x -\\mu}{\\sigma})^{-1/xi}\\right\\}$, setting  $\\mu= -1$ ,$\\sigma = 1$ and $\\xi = -1$\n\n$$\nG(x) =  \\exp \\left\\{- (1 -x + 1 )\\right\\} = \\exp x\n$$\n\n\n\n</div>\n\n:::\n\n\nThe following app illustrates EVT how the block maxima of a random variable fits a GEV distribution as the sample size increases. The app will generate some samples (blocks) of size $n$ drawn from a specified distribution (Uniform(0,1) or Exp(1)) and plot the distribution (histograms) of the maximum of each sample. Then, it compares the empirical histograms against the theoretical GEV (solid red line). For example, if $X\\sim\\text{Uniform(0,1)}$ and we standardise the maxima using $b_n=1$ and $a_n=1/n$ the asymptotic distribution of the maxima is a GEV($\\mu=-1,\\sigma=1,\\xi=-1$) (see the previous task). Likewise, if $X\\sim \\exp(1)$ and we let $a_n=1$ and $b_n= \\log n$, the its maxima will converge to a GEV($\\mu=0,\\sigma=1,\\xi=0$), i.e., a Gumbell distribution (see the proof in the tutorial material).\n\n::: {.callout-important icon=\"false\"}\n## {{< bi box color=#005C8A >}} Task\nPlay around with the app settings to see the impact that the number of samples and the size of blocks have on the approximation of the GEV for the different distributions.\n:::\n\n \n```{shinylive-r}\n#| standalone: true\n#| viewerHeight: 800\n\nlibrary(shiny)\nlibrary(ExtremalDep)\n\n# Function to generate block maxima\ngenerate_max <- function(N, n, rdist, param, a, b, seed=pi){\n  set.seed(seed)\n  Mn <- numeric(N)\n  for(i in 1:N){\n    sample <- rdist(n, param[1], param[2])\n    Mn[i] <- (max(sample) - b) / a\n  }\n  return(Mn)\n}\n\n# UI\niu <- fluidPage(\n  titlePanel(\"Block Maxima Simulation\"),\n  sidebarLayout(\n    sidebarPanel(\n      selectInput(\"dist\", \"Choose Distribution:\", choices = c(\"Uniform(0,1)\" = \"uniform\", \"Exponential(1)\" = \"exponential\")),\n      sliderInput(\"n\", \"Observations per Block (n):\", min = 5, max = 100, value = 5, step = 5),\n      sliderInput(\"N\", \"Number of Blocks (N):\", min = 10000, max = 50000, value = 50000, step = 10000)\n    ),\n    mainPanel(\n      plotOutput(\"histogram\")\n    )\n  )\n)\n\n# Server\nserver <- function(input, output) {\n  output$histogram <- renderPlot({\n    if (input$dist == \"uniform\") {\n      a <- 1 / input$n\n      b <- 1\n      Mn <- generate_max(input$N, input$n, runif, c(0,1), a, b)\n      true_gev <- function(x) dGEV(x, -1, 1, -1)\n    } else {\n      a <- 1\n      b <- log(input$n)\n      Mn <- generate_max(input$N, input$n, rgamma, c(1,1), a, b)\n      true_gev <- function(x) dGEV(x, 0, 1, 0)\n    }\n    \n    hist(Mn, breaks = \"FD\", col = \"lightblue\", prob = TRUE, main = \"Histogram of Block Maxima\", xlab = \"Maxima\")\n    curve(true_gev, col = \"red\", lwd = 2, add = TRUE)\n  })\n}\n\n# Run App\nshinyApp(ui = iu, server = server)\n\n\n```\n\n\n\n# Return Levels\n\nCommunication of extremes typically focuses on maxima (or minima). Environmental or climate events are often described as the \"worst/highest/lowest in X years\".\n\n![](images/EuropeDrought.png){height=\"250\"} ![](images/BangladeshFloods.png){height=\"250\"}\n\nIn statistics, this idea of the \"highest in X years\" can be related to the idea of a **return level** and **return period**. The return level $z_p$ is the value we would expect to be exceeded once every $p$ years, where $\\frac{1}{p}$ is the return period. The return level can also be thought of as the value which has probability $\\frac{1}{p}$ of being exceeded in a given year.\n\nNow consider the statement \"The temperature in Glasgow will reach 20 degrees once every 50 years\".\n\nHere, we have return period $\\frac{1}{p}=0.02$ and return level $z_p=20$.\n\nThe return level $z_p$ is the $(1-\\frac{1}{p})$ quantile of the GEV distribution, since we have a probability $\\frac{1}{p}$ of the maximum exceeding that value.\n\nRecall that the GEV takes the form $$G(x) = \\exp \\left( - \\left[ 1 + \\frac{\\xi (z - \\mu)}{\\sigma} \\right]^{-\\frac{1}{\\xi}} \\right)$$\n\nTherefore the return level can be obtained by inverting this distribution to obtain\n\n$$z_p = \\begin{cases}\n  \\mu -  \\frac{\\sigma}{\\xi}\\left[ 1 -  \\{ - \\log(1-\\frac{1}{p}) \\}^\\xi  \\right] &  \\xi \\neq 0\\\\\n  \\mu - \\sigma \\log\\{ - \\log(1-\\frac{1}{p})\\}  &  \\xi = 0\n  \\end{cases}$$\n\n## Example --- Venice Sea Levels\n\nSea levels in Venice are rising and the city floods on a regular basis. What sea level can we expect in the next 5, 10, 100 years?\n\n![](images/GuardianVenice.png){width=\"500\"}\n\nWe have daily sea level measurements from 1931-1981. The plot below shows the 10 highest sea level measurements from each year.\n\n![](images/VeniceData.png){width=\"500\"}\n\nWe can apply a block maxima approach, treating each year as a block. This requires us to identify and model the maximum values every year --- highlighted in red.\n\n![](images/VeniceMax.png){width=\"500\"}\n\nWe use the `ismev` package to fit a GEV distribution in R. The `gev.fit()` simply takes a data vector and provides parameter estimates using maximum likelihood estimation.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmod1 <- gev.fit(venice)\n\nmod1$mle\n```\n:::\n\n\n\n\n\n\n```         \n[1] 111.09925486  17.17548761  -0.07673265\n```\n\nFrom the output we can see that $\\mu = 111.1$, $\\sigma = 17.2$ and $\\xi = -0.077$.\n\nWe can assess the suitability of the GEV distribution using a return level plot. This involves commuting the return level at various return periods, and comparing it to the theoretical result under the GEV.\n\n![](images/ReturnLevelPlot.png){width=\"500\"}\n\nThe observed points lie along the theoretical line. Our proposed GEV distribution is appropriate. The Venice maxima follow a Gumbel distribution.\n\n## Extending Block Maxima --- $k$-largest\n\nLooking at just the maxima means we throw away a lot of data, making it harder to accurately estimate parameters. We could instead follow an approach which models the $k$ largest values within a block.\n\n![](images/BlockMaximaK.png){width=\"500\"}\n\nWe have to make difficult subjective choices when fitting a block maxima model. What do we choose as our block? Week? Month? Year? Decade? Bigger blocks means we have fewer data points, but smaller blocks mean our 'extremes' might not be extreme at all, violating the assumptions of the GEV distribution. If we use a $k$-largest approach, we have a similar decision on what value of $k$ to choose.\n\nBlock maxima methods work well in many situations, and take advantage of natural blocks in the data. However, it does not work well if there is a lot of variability between blocks. In that scenario, some blocks may have many more large counts than others, and much of the data will be discarded. We can overcome this by using a threshold approach, which models all observations which exceed some pre-specified value.\n\n# Peak Over Threshold\n\nThis approach is known as *peak over threshold (POT)* modelling. Again, we assume we have data represented by a time series, and some threshold $u$. We need a statistical model for the values which are above $u$, known as *exceedances*. Sometimes we may also wish to model the *number* of exceedances.\n\nAgain, let $X_1, \\ldots, X_n$ be a sequence of independent random variables with a common distribution function $F$. We can consider our extreme values in terms of their **threshold excess** (how much they exceed the threshold by). For an extreme value $X > u$, its threshold excess is given as $y = X - u$. The probability of threshold excess of size $y$ is given by\n\n$$\\text{P}(X > u+y|X > u) = \\frac{1 - F(u + y)}{1 - F(u)} \\hspace{3mm}\\text{ where } y > 0.$$\n\nThe function $F$ is still unknown, but the distribution of all threshold excesses can be approximated by a **Generalised Pareto distribution (GPD)**. The cdf of the Generalised Pareto distribution is given by\n\n$$\nG(y) = \n\\begin{cases}\n1 - \\left( 1 + \\frac{\\xi (y-\\mu)}{\\sigma} \\right)^{-\\frac{1}{\\xi}} &  \\xi \\neq 0\\\\\n1 - \\exp \\left( - \\frac{y-\\mu}{\\sigma} \\right)  &  \\xi = 0\n\\end{cases}\n$$\n\nAgain, $\\mu$ is the location parameter, $\\sigma$ is the scale parameter and $\\xi$ is the shape parameter. We can define a return level for POT models in a roughly similar way to block maxima models. The $m$-observation return level, $x_m$ is defined as the level expected to be exceeded once every $m$ observations, with\n\n$$\nx_m = \n\\begin{cases}\nu + \\frac{\\sigma}{\\xi} \\left[ \\left(m \\text{P}(X>u)\\right)^\\xi -1 \\right] &  \\xi \\neq 0\\\\\nu + \\sigma \\log\\left(m \\text{P}(X>u)\\right) &  \\xi = 0\n\\end{cases}\n$$\n\nFor any given observation, the probability of exceeding $x_m$ is simply $\\frac{1}{m}$.\n\nChoosing a threshold is challenging. We need a threshold low enough that we have sufficient data, but high enough that values above it are genuinely extreme.\n\n![](images/Thresholds.png){width=\"500\"}\n\nOccasionally there is a natural choice of threshold (eg a legal limit for a pollutant), but generally we need to choose it. One approach is to use a **mean residual life plot**, which plots the sample mean excess (mean of $x>u$) at a variety of thresholds $u$. If the GPD is appropriate, the mean excess should be linearly related to the threshold. Therefore, we can identify a suitable threshold as one which lies with an area of linearity on this plot.\n\n## Example --- Fort Collins, Colorado\n\nWe have daily precipitation data from 1900-1999, obtained from a rain gauge in Fort Collins, Colorado, taken from Katz et al, 2002.\n\n![](images/FortCollinsData.png){width=\"500\"}\n\nWe compare three different choices of threshold below ($u = 0.5, 1.0, 1.5$) to show the importance of getting the choice right.\n\n| u   | $\\% > u$ | $n > u$ |\n|:----|:---------|:--------|\n| 0.5 | 2.08     | 759     |\n| 1.0 | 0.58     | 213     |\n| 1.5 | 0.25     | 91      |\n\nWe can fit a mean residual plot to identify a sensible choice of threshold. It appears that a value of $u$ somewhere between 1.2 and 1.8 would be an appropriate choice here --- this is where the plot appears to be linear.\n\n![](images/MeanResidualPlot.png){width=\"500\"}\n\nWe can also carry out a sensitivity analysis to see the effect of choosing different threshold values on the estimated model parameters. The plot below shows the parameter estimates at different thresholds --- they seem fairly robust.\n\n![](images/Sensitivity.png){width=\"500\"}\n\nWe can use the `extRemes` package to fit a Generalised Pareto distribution in R. The function `fevd` allows several extreme value distributions (including GEV and GPD) to be fitted, and can also provide return levels.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfitGP <- fevd(Fort, threshold=1.5, type=\"GP\", \n                    time.units=\"365/year\")\n\nreturn.level(fitGP, c(10,20,100), type=\"GP\")\n```\n:::\n\n\n\n\n\n\n```         \n[1] \"Return Levels for period units in years\"\n 10-year level  20-year level 100-year level \n      2.857184       3.340219       4.581339 \n```\n\nThreshold exceedances are not always independent due to temporal correlation. If we have high temperatures today, it's likely we might also have high temperatures tomorrow. We have to account for this dependence within our model, for example by using the ARIMA approaches outlined in the time series section. Alternatively, we could use a \"declustering\" approach which identifies these temporal clusters and simply uses the cluster maxima.\n",
    "supporting": [
      "notes_3_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}