{
  "hash": "6c2ba720954cdc97169f4b434d3d4f65",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Lab session 2\"\nformat: \n  html:\n    number-sections: true\n    toc: true\n    embed-resources: true\n  PrettyPDF-pdf:\n    number-sections: true\nembed-resources: true\neditor_options: \n  chunk_output_type: console\nbibliography: references.bib\nexecute: \n  freeze: auto\n---\n\n::: {.cell}\n\n:::\n\n\n\n\n<font size=\"5\"> **Aim of this practical session:** </font>\n\nIn this practical session we are going\n\n-   To continue the work from Lab 1 in interpreting statistical plots and model output.\n-   To provide seasonal and trend analysis for environmental time series (@sec-time).\n-   To examine and interpret some models for extremes (@sec-ext).\n\n\n\n\n{{< downloadthis lab_2.R dname=\"lab_2\" label = \"Download Lab 2 R script\" icon=\"database-fill-down\" type=\"success\" >}}\n\n\n\n\n\n\n# Part 1: Seasonal and trend analysis {#sec-time}\n\n## Sulphur dioxide in the air {.unnumbered}\n\nSulphur dioxide is an atmospheric pollutant that is monitored in most cities. The data here are weekly concentrations from 1989, in the file `SO2.csv`.\n\n\n\n\n{{< downloadthis datasets/SO2.csv dname=\"SO2\" label=\"Download SO2 data set\" icon=\"database-fill-down\" type=\"info\" >}}\n\n\n\n\n\n\nThe dataset contains (amongst others) the following variables of interest:\n\n| Variable  | Meaning                               |\n|-----------|---------------------------------------|\n| `Years`   | Year of observation                   |\n| `Weeks`   | Week of observation (within the year) |\n| `ln.SO2.` | Log(SO$_\\text{2}$)                    |\n\nA natural logarithm transformation has been applied to the SO$_\\text{2}$ data.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load required R packages:\n\nlibrary(ggplot2)    # For visualizing our data\nlibrary(broom)      # For model predictions\n\n\n# Read in data (making sure to set the working directory to the \n# appropriate location):\nSO2 <- read.csv(\"datasets/SO2.csv\", header = TRUE)\n\n# Examine structure of the dataset:\nstr(SO2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n'data.frame':\t516 obs. of  7 variables:\n $ Years         : int  1989 1989 1989 1989 1989 1989 1989 1989 1989 1989 ...\n $ Weeks         : int  2 5 7 8 9 10 11 12 13 14 ...\n $ Log.Rain      : num  2.001 -0.916 3.19 2.51 2.398 ...\n $ Mean.Temp     : num  2.157 -0.929 0.986 5.271 3.243 ...\n $ Humidity      : num  87.9 92.9 84.4 78 76.9 ...\n $ Wind.Dir.Speed: num  -111 -168 -117 -171 -134 ...\n $ ln.SO2.       : num  0.3086 0.3552 0.4732 0.0487 -0.4865 ...\n```\n\n\n:::\n\n```{.r .cell-code}\n# Add year as a decimal (continuous variable):\nSO2$year_num <- SO2$Years + SO2$Weeks / 53\n```\n:::\n\n\n\n\n## Visualizing our data\n\nLets create some exploratory plots to visualize the relationship between log SO$^2$ and the week while adding some loess curves with different spans (to examine possible seasonal patterns):\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nggplot(data = SO2,\n       aes(y = ln.SO2., x = Weeks ))+\n  geom_point()+\n  geom_smooth(method = \"loess\",span=0.75,aes(color=\"0.75\"))+\n  geom_smooth(method = \"loess\",span=0.5,aes(color=\"0.5\"))+\n    geom_smooth(method = \"loess\",span=0.3,aes(color=\"0.3\"))+\n  labs(y=expression(log~SO^2))\n```\n\n::: {.cell-output-display}\n![](lab_2_files/figure-html/unnamed-chunk-3-1.png){fig-align='center' width=432}\n:::\n:::\n\n\n\n\nThere seems to be some evidence of a seasonal pattern in log(SO$_2$), with higher values towards the starts and ends of the week within years.\n\n::: {.callout-warning icon=\"false\"}\n## {{< bi pencil-square color=#c8793c >}} Task 1\n\nAnother way of exploring potential seasonality is by using boxplots. Try to produce some boxplots that show the Log(SO$_\\text{2}$) against the weeks. Recall that boxplots are meant to summarise the distribution of a continuous variable against categorical ones -thus, you need to declare the `Weeks` variable as a factor (this can be done directly on `ggplot`)\n\n\n<div class='webex-solution'><button>Take hint</button>\n\n\nWe can achieve this by adding a `geom_boxplot()` layer to a `ggplot` object.\n\n\n</div>\n\n\n\n\n\n::: {.cell layout-align=\"center\" webex.hide='Click here to see the solution'}\n\n<div class='webex-solution'><button>Click here to see the solution</button>\n\n```{.r .cell-code  code-fold=\"show\"}\nggplot(data = SO2,\n       aes(y = ln.SO2., x = as.factor(Weeks) ))+\n  geom_boxplot()+\n   labs(y=expression(log~SO^2),x=\"Weeks\")\n```\n\n::: {.cell-output-display}\n![](lab_2_files/figure-html/unnamed-chunk-4-1.png){fig-align='center' width=432}\n:::\n\n\n</div>\n:::\n\n\n\n:::\n\nNow to examine the trend we can plot our data over years as follows:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nggplot(data = SO2,\n       aes(y = ln.SO2., x = year_num ))+\n  geom_point()+\n  geom_smooth(method = \"loess\",aes(color=\"0.75\"))\n```\n\n::: {.cell-output-display}\n![](lab_2_files/figure-html/unnamed-chunk-5-1.png){fig-align='center' width=432}\n:::\n:::\n\n\n\n\nThere is an overall decreasing (not necessarily linear) trend in log(SO$_2$) over time. However, we need to account for the seasonal pattern when trying to assess the trend, so we fit a harmonic model to the data, with a long-term trend also in the model:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a model with seasonal pattern (harmonic model) plus trend:\nharmonic_model <- lm(ln.SO2. ~ sin(2 * pi * (Weeks - 53) / 53) + \n              cos(2 * pi * (Weeks - 53) / 53) + year_num,\n            data = SO2)\n```\n:::\n\n\n\n\nLets look at the summary of the model:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(harmonic_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = ln.SO2. ~ sin(2 * pi * (Weeks - 53)/53) + cos(2 * \n    pi * (Weeks - 53)/53) + year_num, data = SO2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.44795 -0.42321  0.03846  0.42933  1.64858 \n\nCoefficients:\n                                Estimate Std. Error t value Pr(>|t|)    \n(Intercept)                   331.667008  16.390999  20.235  < 2e-16 ***\nsin(2 * pi * (Weeks - 53)/53)   0.246362   0.039360   6.259 8.19e-10 ***\ncos(2 * pi * (Weeks - 53)/53)   0.326653   0.040119   8.142 2.98e-15 ***\nyear_num                       -0.166260   0.008215 -20.239  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6365 on 512 degrees of freedom\nMultiple R-squared:  0.5082,\tAdjusted R-squared:  0.5053 \nF-statistic: 176.4 on 3 and 512 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\n\n\nWe can also compute the predicted values by first generating a grid of values where we want to predict:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a prediction grid over the weeks of the years 1989 to 2000:\npred_data <- expand.grid(Weeks = 1:53, year = 1989:2000)\npred_data$year_num <- pred_data$year + pred_data$Weeks / 53\n```\n:::\n\n\n\n\nThen we can use the `augment` function from the `broom` library to compute the model predictions for (i) the mean (i.e., $E(\\mu_i|Y_i) = \\beta_0 + \\beta_1 \\text{year} + \\gamma_1 \\sin\\left(\\frac{2\\pi \\text{week}}{p}\\right) + \\gamma_2 \\cos\\left(\\frac{2\\pi \\text{week}}{p}\\right)$ ) or (ii) for new observations (i.e., including the uncertainty associated with the observational error $\\epsilon_i$) as follows:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Model predictions with confidence interval for the mean\n\npred_mean = broom::augment(x=harmonic_model,\n                     newdata = pred_data,\n                     interval = \"confidence\",\n                     type.predict = \"response\")\n\n# Model predictions with prediction interval for the new observations\n\npred_obs = broom::augment(x=harmonic_model,\n                     newdata = pred_data,\n                     interval = \"prediction\",\n                     type.predict = \"response\")\n```\n:::\n\n\n\n\nWe can visualize the model predictions as follows:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nggplot() +\n  geom_line(data=SO2,aes(y=ln.SO2.,x=year_num),alpha=0.25)+\n  geom_ribbon(data= pred_obs,\n                aes(x=year_num,\n                    ymin = .lower,\n                    ymax = .upper,\n                    fill=\"Prediction Interval\"),\n                alpha = 0.15)+\n    geom_ribbon(data= pred_mean,\n                aes(x=year_num,\n                    ymin = .lower,\n                    ymax = .upper,\n                    fill=\"Confidence Interval\"),\n                alpha = 0.5)+\n  geom_line(data=pred_mean,\n            aes(y=.fitted,x=year_num),\n            color=\"tomato\")+\n  scale_fill_discrete(name=\"\")+\n  labs(y=expression(log~SO^2),x=\"Time\")\n```\n\n::: {.cell-output-display}\n![](lab_2_files/figure-html/unnamed-chunk-10-1.png){fig-align='center' width=432}\n:::\n:::\n\n\n\n\nThe plot shows the data (grey lines) and the fitted model predictions (as red lines). Then, the blue and red shaded regions indicate the prediction and confidence intervals respectively (notice how the prediction intervals is wider because it takes the observational error into account).\n\n**How well does this model fit the data?** It seems to capture the trend well (following the general pattern in the data) and it seems to fit the seasonal pattern fairly well (having peaks and troughs at the same locations as in the data). However, we note that the model is not perfect: it does not, e.g., capture the higher peak in year 1996. Perhaps, a more flexible model like an additive model (a type of GAM) might be more appropriate?\n\nWe should always check our diagnostic plots when assessing a model:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlayout(matrix(1:4, nrow = 2, byrow = TRUE))\nplot(harmonic_model)\n```\n\n::: {.cell-output-display}\n![](lab_2_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\n\n\n-   The plot of residuals versus fitted values (top left) looks OK, since we don't see any clear patterns here.\n-   The Normal quantile-quantile (Q-Q) plot (top right) also looks OK, since most of the data points follow the line.\n\nWhat about temporal autocorrelation?\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nacf(resid(harmonic_model))\n```\n\n::: {.cell-output-display}\n![](lab_2_files/figure-html/unnamed-chunk-12-1.png){fig-align='center' width=432}\n:::\n:::\n\n\n\n\n-   The plot of the autocorrelation function (ACF; bottom) shows that many of the lower lags have values lying outwith the intervals (i.e. lying outwith the blue dashed lines). Therefore, there is autocorrelation in the residuals, and the assumptions of the model are not satisfied. Therefore, this model is not appropriate for the data. We could consider fitted an AR(1) model here to try to address this. (Our more flexible additive model proposed above may also be worth trying.)\n\n\n## Trend detection in Haddock stocks\n\nWe will study data relating to the annual estimates of biomass for North Sea Haddock from 1963--2000.\n\nThe data are stored in `haddock.dat`, which contains the columns `Year` and `Biomass`. \n\n\n\n\n{{< downloadthis datasets/haddock.dat dname=\"haddock\" label=\"Download haddock data set\" icon=\"database-fill-down\" type=\"info\" >}}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Read in data:\nhaddock.all <- read.table(\"datasets/haddock.dat\",header=TRUE)\n# Remove the data from year 2001 onwards:\nhaddock.data <- haddock.all[haddock.all$Year <= 2000,]\n```\n:::\n\n\n\n\n\n::: {.callout-warning icon=\"false\"}\n## {{< bi pencil-square color=#c8793c >}} Task 2\n\nUsing `ggplot` produce a line plot that shows biomass (measured in thousands of tonnes) over the years\n\n\n<div class='webex-solution'><button>Take hint</button>\n\n\nWe can add a `geom_line()` layer to produce a line graph in ggplot.\n\n\n</div>\n\n\n\n\n\n::: {.cell layout-align=\"center\" webex.hide='Click here to see the solution'}\n\n<div class='webex-solution'><button>Click here to see the solution</button>\n\n```{.r .cell-code  code-fold=\"show\"}\nggplot(data=haddock.data,aes(x=Year,y=Biomass))+\n  geom_line()+\n  labs(y=\"Thousands of tonnes\")\n```\n\n::: {.cell-output-display}\n![](lab_2_files/figure-html/unnamed-chunk-14-1.png){fig-align='center' width=432}\n:::\n\n\n</div>\n:::\n\n\n\n\n:::\n\n\n::: {.callout-tip icon=\"false\"}\n## {{< bi question-octagon color=#6dc83c >}} Question\n\nWhat does the plot produced in the previous task tells you?\n\n\n<div class='webex-solution'><button>See Solution</button>\n\n\nThis plot shows that there is some kind of nonlinear trend in the data, with some higher values earlier on (around 1970) and then generally lower values later on. There seems to be higher variability for the time when there were some higher values. \n\n\n</div>\n\n:::\n\n\n\n\n::: {.callout-warning icon=\"false\"}\n## {{< bi pencil-square color=#c8793c >}} Task 3\n\nNotice that a log-transformation may be appropriate to address the large variability observed in the data. Plot the $\\log_{10}$ of the biomass agianst years using `ggplot`, what do you notice?\n\n\n<div class='webex-solution'><button>Take hint</button>\n\n\nWe can use apply the `log10` function directly to the variable(s) in the ggplot `aes` argument.\n\n\n</div>\n\n\n\n\n\n::: {.cell layout-align=\"center\" webex.hide='Click here to see the solution'}\n\n<div class='webex-solution'><button>Click here to see the solution</button>\n\n```{.r .cell-code  code-fold=\"show\"}\nggplot(data=haddock.data,aes(x=Year,y=log10(Biomass)))+\n  geom_line()+\n  labs(y=expression(paste(log[10], \"(1000 tonnes)\")))\n```\n\n::: {.cell-output-display}\n![](lab_2_files/figure-html/unnamed-chunk-15-1.png){fig-align='center' width=432}\n:::\n\n\n</div>\n:::\n\n\n\n:::\n\n\nWe can fit a simple model to the log-transformed biomass (see previous task) below:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Linear trend model:\n\n## Set up data by creating a Time index 1:38 years\nhaddock.data$Time <- haddock.data$Year-1962\n## Fit model:\ntrend.model0 <- lm(log10(Biomass) ~ Time, data = haddock.data)\nsummary(trend.model0)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = log10(Biomass) ~ Time, data = haddock.data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.46891 -0.06912  0.01352  0.09904  0.45118 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  2.631107   0.063576  41.385  < 2e-16 ***\nTime        -0.016065   0.002842  -5.653 2.02e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1921 on 36 degrees of freedom\nMultiple R-squared:  0.4703,\tAdjusted R-squared:  0.4556 \nF-statistic: 31.96 on 1 and 36 DF,  p-value: 2.021e-06\n```\n\n\n:::\n:::\n\n\n\n\nThe p-value for Time is $<0.05$, so there is a statistically significant trend. Since the coefficient of this is $<0$ (and the p-value is $<0.05$), this is a statistically significant decreasing trend.\n\nLet's predict into the future:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Predict for years 2001 to 2010, i.e., time index 39:48\npred.t <- data.frame(Time = 39:48)\t\n```\n:::\n\n\n\n\nNow we use the `augment` function from `broom` to get the model predictions:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrend_preds = broom::augment(x=trend.model0,\n                     newdata = pred.t,\n                     interval = \"prediction\",\n                     type.predict = \"response\")\n\n# This will allow a plot labelled with \"Year\"\ntrend_preds$Year <- 1962 + trend_preds$Time \n```\n:::\n\n\n\n\nLastly, we use `ggplot` to plot our predictions:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nggplot()+\n  geom_line(data=haddock.data,\n            aes(x=Year,y=log10(Biomass)),\n            alpha=0.25)+\n   geom_ribbon(data= trend_preds,\n                aes(x=Year,\n                    ymin = .lower,\n                    ymax = .upper),alpha=0.25)+\n  geom_line(data=trend_preds,aes(x=Year,y=.fitted),color=\"tomato\")\n```\n\n::: {.cell-output-display}\n![](lab_2_files/figure-html/unnamed-chunk-19-1.png){fig-align='center' width=432}\n:::\n:::\n\n\n\n\n\nSince we're predicting for 1 to 10 years ahead, we can extract the 6th and 10th values:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrend_preds[c(6,10),]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 5\n   Time .fitted .lower .upper  Year\n  <int>   <dbl>  <dbl>  <dbl> <dbl>\n1    44    1.92   1.51   2.34  2006\n2    48    1.86   1.43   2.29  2010\n```\n\n\n:::\n:::\n\n\n\n\nso we have:\n\n- The predicted log(biomass) in 2006 is 1.92, with a 95% prediction interval of (1.51, 2.34).\n- The predicted log(biomass) in 2010 is 1.86, with a 95% prediction interval of (1.43, 2.29).\n\n\nSince we are on the log scale, we can exponentiate to get predictions on the original scale:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nexp(trend_preds[c(6,10),2:4])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 3\n  .fitted .lower .upper\n    <dbl>  <dbl>  <dbl>\n1    6.85   4.50  10.4 \n2    6.42   4.19   9.85\n```\n\n\n:::\n:::\n\n\n\n\n\n- The predicted biomass in 2006 is 6.85, with a 95% prediction interval of (4.50, 10.42).\n- The predicted biomass in 2010 is 6.42, with a 95% prediction interval of (4.19, 9.85).\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlayout(matrix(1:4, nrow = 2, byrow = TRUE))\n# Check model diagnostic plots:\nplot(trend.model0, which = 1:4)\n```\n\n::: {.cell-output-display}\n![](lab_2_files/figure-html/unnamed-chunk-22-1.png){width=672}\n:::\n:::\n\n\n\n\n\nThe plot of residuals versus fitted values (top left) shows a clear pattern. This means that the deterministic part of the model does not capture the patterns in the data (a problem!).\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlayout(matrix(1:3, nrow = 1, byrow = TRUE))\ne0 <- resid(trend.model0)\n\nplot(haddock.data$Year, e0, type = \"l\", xlab = \"Year\", \n     ylab = \"Residual\", main = \"Residual time series\")\nabline(h = 0, lty=2)\nacf(e0,main=\"Residual ACF\")\npacf(e0,main=\"Residual PACF\")\n```\n\n::: {.cell-output-display}\n![](lab_2_files/figure-html/unnamed-chunk-23-1.png){width=672}\n:::\n:::\n\n\n\n\nThe lag 1 coefficient is outwith the intervals, so we have statistically significant evidence of residual autocorrelation in the data. This shows that the model is not appropriate for the data. Let's try a time series regression approach.\n\n**Time series model**\n\n\nLets begin with a simple AR(1) model:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(forecast)\n\nar1_model <- Arima(log10(haddock.data$Biomass), order=c(1,0,0),\n                   xreg = haddock.data$Time,\n                   include.constant = TRUE) # Include intercept\n\nsummary(ar1_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSeries: log10(haddock.data$Biomass) \nRegression with ARIMA(1,0,0) errors \n\nCoefficients:\n         ar1  intercept     xreg\n      0.6141     2.5611  -0.0134\ns.e.  0.1405     0.1240   0.0054\n\nsigma^2 = 0.02539:  log likelihood = 17.2\nAIC=-26.4   AICc=-25.19   BIC=-19.85\n\nTraining set error measures:\n                      ME      RMSE       MAE         MPE    MAPE      MASE\nTraining set 0.008715057 0.1529264 0.1141291 -0.07817945 4.91856 0.8889311\n                  ACF1\nTraining set 0.1968254\n```\n\n\n:::\n:::\n\n\n\n\n\nNow we can forecast into the future as follows:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nforecast_result <- forecast::forecast(\n  ar1_model,\n  xreg = trend_preds$Time,\n  h = 10  # forecast horizon\n)\nautoplot(forecast_result)\n```\n\n::: {.cell-output-display}\n![](lab_2_files/figure-html/unnamed-chunk-25-1.png){fig-align='center' width=432}\n:::\n:::\n\n\n\n\n\nNotice that our predictions are added as a blue solid line, with\n\n- Darker/lighter inner region indicating 80% prediction interval\n\n- Lighter/outer region indicating 95% prediction interval\n\nWe check the time series diagnostic plots:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlayout(matrix(1:2, nrow = 1, byrow = TRUE))\n\n# Check model diagnostic plots:\ne1 <- resid(ar1_model)\nacf(e1, main = \"Residual ACF for AR(1) model\")\npacf(e1, main = \"Residual PACF for AR(1) model\")\n```\n\n::: {.cell-output-display}\n![](lab_2_files/figure-html/unnamed-chunk-26-1.png){width=672}\n:::\n:::\n\n\n\n\nThis looks much better, since the lag 1 coefficient is now within the confidence interval bounds, so we no longer have any statistically significant evidence of residual autocorrelation. Therefore, the AR(1) model appears to be appropriate for the data.\n\nWe should also check the other diagnostic plots:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlayout(matrix(1:2, nrow = 2, byrow = TRUE))\n\nplot(resid(ar1_model), type = \"o\")\nqqnorm(resid(ar1_model))\nqqline(resid(ar1_model))\n```\n\n::: {.cell-output-display}\n![](lab_2_files/figure-html/unnamed-chunk-27-1.png){width=672}\n:::\n:::\n\n\n\n\nThese do not look too bad --- there is little remaining pattern in the plot of residuals versus time (left) and the points seem to follow the line fairly well on the Q-Q plot (right). therefore, this model appears to be appropriate for the data.\n\n\n\n#  Part 2: Models for extremes {#sec-ext}\n\nLets look at extreme data, and makes use of the `extRemes` R package.\n\nThe data set `ewe_dailyflow_noleap.txt` provides daily river flows from the River Ewe.\n\n\n\n\n{{< downloadthis datasets/ewe_dailyflow_noleap.txt dname=\"ewe_dailyflow_noleap\" label=\"Download Ewe data set\" icon=\"database-fill-down\" type=\"info\" >}}\n\n\n\n\n\n\nHere we will use some tidy data manipulation & wrangling via the `tidyr` and `dplyr` R packages/ we will also make use of the `lubridate` library to handle date and time variables. Firs lets load the libraries and data:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(extRemes)  # library for GEV models\nlibrary(lubridate) # library to work with dates and time\nlibrary(tidyr)     # library for data tidying\nlibrary(dplyr)     # library for data manipulation \n\n#Read the data\newe <- read.table(\"datasets/ewe_dailyflow_noleap.txt\",header=T)\n```\n:::\n\n\n\n\nWe will work with the log-transformed flow. Here, we will use the `mutate` function fro `dplyr` to create our log-transformed variable named `log_flow`. We will also use this function to create a date variable by combining the information on the year, month and day:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\newe <- ewe %>% mutate(\n   log_flow = log(flow),  # we'll work with logged data\n   date = make_date(year, month, day))\n```\n:::\n\n\n\n\nLets visualise our daily time series data:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nggplot(data=ewe,aes(y=log_flow,x=date))+\n  geom_line(alpha=0.35)+\nlabs(y=\"log flow\")\n```\n\n::: {.cell-output-display}\n![](lab_2_files/figure-html/unnamed-chunk-30-1.png){fig-align='center' width=432}\n:::\n:::\n\n\n\n\n\nSuppouse now we want to compute the annual maximum flows for the river Ewe. We can achieve this using the `sumarise` function from `dplyr`:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\newe_max <- ewe %>% summarise(flow_max= max(log_flow),.by=year)\n```\n:::\n\n\n\n\nHere, the `.by` argument allows us to specify the grouping variable for which the max function `log_flow` of  will be summarised.\n\n::: {.callout-warning icon=\"false\"}\n## {{< bi pencil-square color=#c8793c >}} Task 4\n\nPlot the annual maximum flows for the river Ewe and comment.\n\n\n\n\n::: {.cell layout-align=\"center\" webex.hide='Click here to see the solution'}\n\n<div class='webex-solution'><button>Click here to see the solution</button>\n\n```{.r .cell-code  code-fold=\"show\"}\nggplot(data=ewe_max,aes(y=flow_max,x=year))+\n  geom_line(alpha=0.35)+\nlabs(y=\"log flow annual maximum\")\n```\n\n::: {.cell-output-display}\n![](lab_2_files/figure-html/unnamed-chunk-32-1.png){fig-align='center' width=432}\n:::\n\n\n</div>\n:::\n\n\n\n\n:::\n\n\n::: {.callout-tip icon=\"false\"}\n## {{< bi question-octagon color=#6dc83c >}} Question\n\nWhat does the plot produced in the previous task tells you?\n\n\n<div class='webex-solution'><button>See Solution</button>\n\n\nFrom this, we can see that there is a potential increasing trend in the annual maximum flows at the start of the time period, but this does not continue after around 1974. In fact, this apparent trend is only due to the data for a few timepoints, and there is high variability, so that we cannot say that there is clear evidence for a trend here. \n\n\n</div>\n\n\n:::\n\nNow,  we will use the `fevd` function from the `extRemes` library to estimate the GEV parameters using maximum likelihood. This function receives as input a numeric vector.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# GEV model fitting\nfit_gev <- fevd(ewe_max$flow_max,method=\"MLE\")\nresults <- summary(fit_gev)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nfevd(x = ewe_max$flow_max, method = \"MLE\")\n\n[1] \"Estimation Method used: MLE\"\n\n\n Negative Log-Likelihood Value:  3.380743 \n\n\n Estimated parameters:\n  location      scale      shape \n 4.6548447  0.2650445 -0.2652807 \n\n Standard Error Estimates:\n  location      scale      shape \n0.04605764 0.03128801 0.07928741 \n\n Estimated parameter covariance matrix.\n              location         scale        shape\nlocation  2.121306e-03 -1.166577e-05 -0.001252898\nscale    -1.166577e-05  9.789396e-04 -0.001335417\nshape    -1.252898e-03 -1.335417e-03  0.006286493\n\n AIC = 12.76149 \n\n BIC = 17.75217 \n```\n\n\n:::\n:::\n\n\n\n\n::: {.callout-tip icon=\"false\"}\n## {{< bi question-octagon color=#6dc83c >}} Question\n\nWhat are the standard errors for the estimated scale parameter, $\\sigma$, in your model (rounded to two decimal places)?\n\n<input class='webex-solveme nospaces' data-tol='0.001' size='4' data-answer='[\"0.03\",\".03\"]'/>\n:::\n\n\n::: {.callout-tip icon=\"false\"}\n## {{< bi question-octagon color=#6dc83c >}} Question\n\nWhat is your estimate of the GEV parameter for location, $\\mu$ (rounded to 2 decimal places)?\n\n<input class='webex-solveme nospaces' data-tol='0.001' size='4' data-answer='[\"0.05\",\".05\"]'/>\n:::\n\nRunning the following code provides us with confidence intervals for the GEV model parameters:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nci(fit_gev, alpha = 0.05, type = c(\"parameter\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nfevd(x = ewe_max$flow_max, method = \"MLE\")\n\n[1] \"Normal Approx.\"\n\n         95% lower CI   Estimate 95% upper CI\nlocation    4.5645734  4.6548447    4.7451160\nscale       0.2037211  0.2650445    0.3263678\nshape      -0.4206812 -0.2652807   -0.1098802\n```\n\n\n:::\n:::\n\n\n\n\n::: {.callout-tip icon=\"false\"}\n## {{< bi question-octagon color=#6dc83c >}} Question\n\nBased on the output of the GEV model, which of the three families of the GEV distribution best describes extreme river flow events?\n\n\n<div class='webex-solution'><button>Take hint</button>\n\n\nLooking at our course notes, we have the following statements:\n\n\"The Gumbel, Frechet and Weibull distributions are all special cases (of the GEv distribution) depending on the value of $\\xi$ (the shape parameter).\n\n* If $\\xi < 0$, then we have the Weibull distribution.\n* If $\\xi > 0$, then we have the Frechet distribution.\n* If $\\xi \\rightarrow 0$, then we have the Gumbel distribution.\"\n\n\n</div>\n\n\n<select class='webex-select'><option value='blank'></option><option value=''>Gumbel</option><option value=''>Frechet</option><option value='answer'>Weibull</option></select>\n:::\n\nRunning the following code provides us with estimates for the 10, 50 and 100 year return levels:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nreturn.level(fit_gev,return.period = c(10,50,100))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nfevd(x = ewe_max$flow_max, method = \"MLE\")\nget(paste(\"return.level.fevd.\", newcl, sep = \"\"))(x = x, return.period = return.period)\n\n GEV model fitted to  ewe_max$flow_max  \nData are assumed to be  stationary \n[1] \"Return Levels for period units in years\"\n 10-year level  50-year level 100-year level \n      5.103972       5.299083       5.359084 \n```\n\n\n:::\n:::\n\n\n\n\nWe can generate 95% confidence interval estimates for these return levels using the following code:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nci(fit_gev, alpha = 0.05, type = c(\"return.level\"),return.period = c(10,50,100))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nfevd(x = ewe_max$flow_max, method = \"MLE\")\n\n[1] \"Normal Approx.\"\n\n                      95% lower CI Estimate 95% upper CI\n10-year return level      4.996722 5.103972     5.211223\n50-year return level      5.156316 5.299083     5.441850\n100-year return level     5.193936 5.359084     5.524233\n```\n\n\n:::\n:::\n\n\n\n\n\n\n::: {.callout-tip icon=\"false\"}\n## {{< bi question-octagon color=#6dc83c >}} Question\n\nBased on the output above, state what the approximate return level would be for a 100-year return level - how would you communicate this statement to the a non-expert (e.g., the general public)?\n\n\n<div class='webex-solution'><button>Solution</button>\n\nSince the 100-year return level is 5.36, we can say that a (log) flow of 5.36 is expected to be exceeded once on average, every 100 years.\n\nThe interval bounds for the 10-year return level are (5.19, 5.52). A 100-year return level does **not** guarantee that a maximum flow will fall in a fixed range. Instead, it estimates a **threshold that is exceeded** on average once every 100 years.\n\n\n</div>\n\n:::\n\n\n\n",
    "supporting": [
      "lab_2_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}