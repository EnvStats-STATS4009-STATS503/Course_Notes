{
  "hash": "e06d93ff3d11edb8d8ff68e106cafaae",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Lab session 3\"\nformat: \n  html:\n    number-sections: true\n    toc: true\n    embed-resources: true\n  PrettyPDF-pdf:\n    number-sections: true\nembed-resources: true\neditor_options: \n  chunk_output_type: console\nexecute: \n  freeze: auto\n---\n\n::: {.cell}\n\n:::\n\n\n\n\n\n\n<font size=\"5\"> **Aim of this practical session:** </font>\n\nIn this practical session we are going\n\n-   Manipulate and visualize different types of spatial objects.\n-   Familiarize with the syntax for fitting LGMs in the `inlabru` software\n-   Learn how to fit a Bayesian areal model for disease risk mapping (@sec-areal)\n-   Learn how to fit a Geostatistical spatial model (@sec-geo).\n-   Fit point process models in `inlabru` (@sec-points).\n\n\n\n\n\n\n{{< downloadthis lab_3.R dname=\"lab_3\" label = \"Download Lab 3 R script\" icon=\"database-fill-down\" type=\"success\" >}}\n\n\n\n\n\n\n\n\n\n\nThe data for this session is available throughout different packages, if you cannot access some of these libraries you can the download the data using the link below and then load it into R using the `load()` function:\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nload(\"lab_3_DataFiles.RData\")\n```\n:::\n\n{{< downloadthis lab_3_DataFiles.RData dname=\"lab_3_DataFiles\" label = \"Download Lab 3 Data Sets\" icon=\"database-fill-down\" type=\"success\" >}}\n\n\n\n\n\n\n\n\n\n\nFirst lets load the `R` libraries we will use during this session:\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Libraries for Data manipulation\nlibrary(tidyr)\nlibrary(dplyr)\nlibrary(sf)\nlibrary(terra)\n\n# Libraries for producing maps\nlibrary(ggplot2)\nlibrary(viridis)\nlibrary(tidyterra)\nlibrary(mapview)\nlibrary(patchwork)\n\n# Libraries for the Analysis\nlibrary(INLA)\nlibrary(inlabru)\nlibrary(spdep) \nlibrary(gstat)\nlibrary(variosig)\n```\n:::\n\n\n\n\n\n\n# Disease risk modelling {#sec-areal}\n\n## The Data: Lip cancer rates in Scotland {.unnumbered}\n\nIn this example we model the number of lip cancer rates in Scotland in the years 1975–1980 at the county level in order to evaluate the presence of an association between sun exposure and lip cancer.\n\n![](images/clipboard-2435220026.png){fig-align=\"center\" width=\"409\"}\n\nThe data is available on the `SpatialEpi` R package\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(SpatialEpi)\ndata(scotland_sf)\n```\n:::\n\n\n\n\n\n\nThe `scotland_sf` data is a Simple Features (`sf`) object containing the spatial polygon information for the set of 56 counties. The `sf` package allows us to work with vector data which is used to represent points, lines, and polygons. It can also be used to read vector data stored as a shapefiles.\n\nThe dataset contains the following variables of interest:\n\n| Variable | Meaning |\n|-------------------|-----------------------------------------------------|\n| `county.names` | Scotland County name |\n| `cases` | Number of Lip Cancer cases per county |\n| `expected` | Expected number of lip cancer cases |\n| `AFF` | Proportion of the population who work in agricultural fishing and farming |\n| `geometry` | Geometric representation of counties in Scotland |\n\n## Standardized Mortality Ratios and spatial correlation\n\nIn epidemiology, disease risk is usually estimated using Standardized Mortality Ratios (SMR). The SMR for a given spatial areal unit $i$ is defined as the ratio between the observed ( $Y_i$ ) and expected ( $E_i$ ) number of cases:\n\n$$\nSMR_i = \\dfrac{Y_i}{E_i}\n$$\n\nA value $SMR > 1$ indicates that there are more observed cases than expected which corresponds to a high risk area. On the other hand, if $SMR<1$ then there are fewer observed cases than expected, suggesting a low risk area.\n\nWe can manipulate `sf` objects the same way we manipulate standard data frame objects via the `dplyr` package. Lets use the pipeline command `%>%` and the `mutate` function to calculate the yearly SMR values for each county:\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Compute the SMR and add a region index (for later modelling)\nscotland_sf <- scotland_sf %>% mutate(\n  SMR = cases/expected,\n  region_id = 1:nrow(scotland_sf))\n```\n:::\n\n\n\n\n\n\nNow we use `ggplot` to visualize our data by adding a `geom_sf` layer and coloring it according to our variable of interest (i.e., SMR) choosing an appropriate color palette using the `scale_fill_viridis` from the `viridis` package:\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Visualize the regions colored by the SMR\nggplot()+geom_sf(data=scotland_sf,aes(fill=SMR))+scale_fill_viridis(direction = -1,option = \"magma\")\n```\n\n::: {.cell-output-display}\n![](lab_3_files/figure-pdf/unnamed-chunk-6-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n\n\nWe can see that higher lip cancer risks occurs in the north of Scotland.\n\n::: {.callout-warning icon=\"false\"}\n## {{< bi pencil-square color=#c8793c >}} Task\n\nProduce a map that shows the spatial distribution of the proportion of the population who work in agricultural fishing and farming (`AFF`) for each county.\n\n\n\n\n\n\n::: {.cell layout-align=\"center\" webex.hide='Click here to see the solution'}\n\n<div class='webex-solution'><button>Click here to see the solution</button>\n\n```{.r .cell-code}\nggplot()+geom_sf(data=scotland_sf,aes(fill=AFF))+scale_fill_viridis(direction = -1,option = \"magma\")\n```\n\n::: {.cell-output-display}\n![](lab_3_files/figure-pdf/unnamed-chunk-7-1.pdf){fig-align='center' fig-pos='H'}\n:::\n\n\n</div>\n:::\n\n\n\n\n\n:::\n\n### Neighbourhood structure\n\nA key aspect of any spatial analysis is that observations closer together in space are likely to have more in common than those further apart. This can lead us towards approaches similar to those used in time series, where we consider the spatial *closeness* of our regions in terms of a *neighbourhood structure*.\n\nThe function [`poly2nb()`](https://r-spatial.github.io/spdep/reference/poly2nb.html) of the `spdep` package can be used to construct a list of neighbors based on areas with contiguous boundaries (e.g., using Queen contiguity).\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nW.nb <- poly2nb(scotland_sf,queen = TRUE)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in poly2nb(scotland_sf, queen = TRUE): some observations have no neighbours;\nif this seems unexpected, try increasing the snap argument.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in poly2nb(scotland_sf, queen = TRUE): neighbour object has 4 sub-graphs;\nif this sub-graph count seems unexpected, try increasing the snap argument.\n```\n\n\n:::\n\n```{.r .cell-code}\nW.nb\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nNeighbour list object:\nNumber of regions: 56 \nNumber of nonzero links: 234 \nPercentage nonzero weights: 7.461735 \nAverage number of links: 4.178571 \n3 regions with no links:\n3, 53, 55\n4 disjoint connected subgraphs\n```\n\n\n:::\n:::\n\n\n\n\n\n\nThe warning tell us that the neighbourhood is comprised of 4 interconnected regions. By looking at the neighbourhood graph below, we can see that these are the three separate island in the north.\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nplot(st_geometry(scotland_sf), border = \"lightgray\")\nplot.nb(W.nb, st_geometry(scotland_sf), add = TRUE)\n```\n\n::: {.cell-output-display}\n![](lab_3_files/figure-pdf/unnamed-chunk-9-1.pdf){fig-align='center' fig-pos='H'}\n:::\n:::\n\n\n\n\n\n\n### Moran's I\n\nOnce we have identified a set of neighbours using our chosen method, we can use this to account for correlation.\n\nWe can use Moran's $I$ as a measure of global spatial autocorrelation based on a neighbourhood matrix:\n\n$$ w_{ij} = \\begin{cases} 1 & \\text{if areas } (B_i, B_j) \\text{ are neighbours.}\\\\ 0 & \\text{otherwise.} \\end{cases} $$\n\nIn `R` we can use the `nb2listw` function to create $w$ while specifying `zero.policy = TRUE` as we have some regions with no neighbors.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# neighbors list \nnbw <- nb2listw(W.nb, style = \"W\",zero.policy = TRUE)\n```\n:::\n\n\n\n\n\n\nMoran's $I$ ranges between -1 and 1, and can be interpreted in a similar way to a standard correlation coefficient.\n\n-   $I=1$ implies that we have **perfect spatial correlation**.\n\n-   $I=0$ implies that we have **complete spatial randomness**.\n\n-   $I=-1$ implies that we have **perfect dispersion** (negative correlation).\n\nOur observed $I$ is a point estimate, and we may also wish to assess whether it is significantly different from zero. We can test for a statistically significant spatial correlation using a permutation test, with hypotheses:\n\n$$\n\\begin{aligned}\nH_0&: \\text{ negative or no spatial association } (I \\leq 0)\\\\\nH_1&: \\text{ positive spatial association } (I > 0)\n\\end{aligned}\n$$\n\nWe can use `moran.test()` to test this hypothesis by setting `alternative = \"greater\"`. To do so, we need to supply list containing the neighbors via the `nb2listw()` function from the `spdep` package. Lets assess now the spatial autocorrelation of the SMR in 2011:\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Global Moran's I\ngmoran <- moran.test(scotland_sf$SMR, nbw,\n                     alternative = \"greater\")\ngmoran\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tMoran I test under randomisation\n\ndata:  scotland_sf$SMR  \nweights: nbw  \nn reduced by no-neighbour observations  \n\nMoran I statistic standard deviate = 5.6068, p-value = 1.03e-08\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.497024704      -0.019230769       0.008478093 \n```\n\n\n:::\n:::\n\n\n\n\n\n\n::: {.callout-tip icon=\"false\"}\n## {{< bi question-octagon color=#6dc83c >}} Question\n\nWhat do we conclude from the Moran's I test?\n\n\n<div class='webex-solution'><button>Answer</button>\n\n\nSince have set the alternative hypothesis to be \\$ I \\> 0\\$ and have a *p*-value $<0.05$, we then reject the null hypothesis and conclude there is evidence for positive spatial autocorrelation.\n\n\n</div>\n\n:::\n\n## Fitting a BYM model\n\nAs with the other types of spatial modelling, our goal is to observe and explain spatial variation in our data. Generally, we are aiming to produce a smoothed map which summarises the spatial patterns we observe in our data.\n\n### The Model structure\n\nNow will fit a BYM model to the data where we consider a Poisson model for the observed cases. Remember that we can formulate this as a Latent Gaussian Model (LGM) on threes stages:\n\n> -   **Stage 1:** We assume the responses are Poisson distributed: $$\n>     \\begin{aligned}y_i|\\eta_i & \\sim \\text{Poisson}(E_i\\lambda_i)\\\\\\text{log}(\\lambda_i) = \\eta_i & = \\beta_0 + \\beta_1 \\mathrm{AFF} + u_i + z_i\\end{aligned}\n>     $$\n\n> -   **Stage 2:** $\\eta_i$ is linked to the disease relative risks (RR) via the log link function and is a linear function of **four components**: $\\beta_0$ the overall intercept, $\\beta_1$ the AFF effect,$\\mathbf{u} = (u_1, \\dots, u_n)$ a spatially structured model (or ICAR) with precision matrix $\\tau_u\\mathbf{Q}$ and an unstructured *iid* random effect $\\mathbf{z} = (z_1, \\dots, z_n)$ with precision $\\tau_z$\n\n> -   **Stage 3:** $\\{\\tau_{z},\\tau_u\\}$: Precision parameters for the random effects\n\nThe latent field is then given by $\\mathbf{x}= (\\beta_0, \\beta_1, u_1, u_2,\\ldots, u_n,z_1,...,z_n)$, the hyperparameters are $\\boldsymbol{\\theta} = (\\tau_u,\\tau_z)$.\n\nWe can fit this model with `inlabru` by following the next workflow and defining:\n\n1.  The model components (`cmp`)\n\n2.  The `formula` for the linear predictor\n\n3.  The observational model (via the `bru_obs` function)\n\n4.  fit the model using the `bru` function\n\n### Model components {.unnumbered}\n\nOur model has 4 compontents, (i) a global intercept, (ii) the covariate AFF effect (which is a column in our data set), (iii) a unstructured random effect $\\mathbf{z} \\sim N(0,\\tau_z^{-1})$) and (iv) a structured random effect $\\mathbf{u}$ that follows an ICAR model structure of the form:\n\n$$\nu_i|\\mathbf{u}_{-i},\\tau_u, \\sim N\\left(\\frac{1}{d_i}\\sum_{j\\sim i}u_j,\\frac{1}{d_i\\tau_u}\\right)\n$$\n\nWhere\n\n-   $\\mathbf{u}_{-i} = (u_i,\\ldots,u_{i-1},u_{i+1},\\ldots,u_n)^T$\n\n-   $\\tau_u$ is the precision parameter (inverse variance).\n\n-   $d_i$ is the number of neighbours\n\n-   $Q$ denotes the precision matrix defined as\n\n$$\nQ_{i,j} = \\begin{cases}\nd_i, & i = j \\\\\n-1, & i \\sim j \\\\\n0, &\\text{otherwise}\n\\end{cases}\n$$\n\nThus, we can define the precision matrix Q according the proximity matrix using the `nb2mat` function (this is similar to `nb2listw` we used before but it will give us a matrix rather than a list):\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"show\"}\nR <- nb2mat(W.nb, style = \"B\", zero.policy = TRUE)\ndiag = apply(R,1,sum)\nQ = -R\ndiag(Q) = diag\n```\n:::\n\n\n\n\n\n\nRecall The precision matrix $Q$ depends on the neighboring structure and $\\tau_u$ (which will be estimated). Now, to make the precision parameters of models with different intrinsic Gaussian random field comparable we add a sum-to-zero constrain $\\sum_i^n u_i = 0$ (see `scale.model = TRUE` in the code below).\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"show\"}\ncmp = ~ Intercept(1) + beta_1(AFF, model = \"linear\") + \n   z_i(region_id , model = \"iid\") +\n  u_i(region_id, model = \"besag\", graph = Q,scale.model = TRUE) \n```\n:::\n\n\n\n\n\n\nHere, `Intercept`, `beta_1`, `u_i` and `z_i` are the names of the model components (note these are just labels we could have chosen any other names for them)\n\n### The linear predictor and the observational model {.unnumbered}\n\nNow we need to specify:\n\n1.  The linear predictor formula for the model components (here we declare the labels that we used in the previous step)\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"show\"}\nformula = cases ~ Intercept + beta_1 + u_i + z_i\n```\n:::\n\n\n\n\n\n\n2.  The observational model/likelihood for our observations:\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"show\"}\nlik = bru_obs(formula = formula,\n\n              family = \"poisson\",\n\n              E = expected,\n\n              data = scotland_sf)\n```\n:::\n\n\n\n\n\n\nNotice how we add the **expected number of cases** as our offset through the `E` argument of `bru_obs` and specify the distribution of our data through the `family` argument.\n\n### Fit the Model and Extract the Results {.unnumbered}\n\nWe finally fit the model using the `bru` function and extract some posterior summaries using the `summary` function:\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"show\"}\nfit = bru(cmp, lik)\nsummary(fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ninlabru version: 2.13.0.9016 \nINLA version: 25.08.21-1 \nComponents: \nLatent components:\nIntercept: main = linear(1)\nbeta_1: main = linear(AFF)\nz_i: main = iid(region_id)\nu_i: main = besag(region_id)\nObservation models: \n  Family: 'poisson'\n    Tag: <No tag>\n    Data class: 'sf', 'data.frame'\n    Response class: 'numeric'\n    Predictor: cases ~ Intercept + beta_1 + u_i + z_i\n    Additive/Linear: TRUE/TRUE\n    Used components: effects[Intercept, beta_1, z_i, u_i], latent[] \nTime used:\n    Pre = 0.892, Running = 0.496, Post = 0.239, Total = 1.63 \nFixed effects:\n            mean    sd 0.025quant 0.5quant 0.975quant   mode kld\nIntercept -0.306 0.119     -0.538   -0.307     -0.069 -0.307   0\nbeta_1     4.317 1.272      1.758    4.337      6.761  4.337   0\n\nRandom effects:\n  Name\t  Model\n    z_i IID model\n   u_i Besags ICAR model\n\nModel hyperparameters:\n                      mean       sd 0.025quant 0.5quant 0.975quant    mode\nPrecision for z_i 22037.66 24119.96    1474.30 14478.63   86080.91 4022.36\nPrecision for u_i     4.14     1.43       2.02     3.91       7.60    3.48\n\nMarginal log-Likelihood:  -193.98 \n is computed \nPosterior summaries for the linear predictor and the fitted values are computed\n(Posterior marginals needs also 'control.compute=list(return.marginals.predictor=TRUE)')\n```\n\n\n:::\n:::\n\n\n\n\n\n\nLooking at $\\exp(\\hat{\\beta_1} \\times 0.10)$ we can see that a 10 percentage-point increase in the proportion working in agriculture/fishing/farming is associated with about a 2 increase in disease risk, after accounting for the spatial structures. In general, areas with a higher proportion of people working in agriculture, fishing, and farming tend to have substantially higher disease risk, even after accounting for spatial correlation using the ICAR effect.\n\n::: {.callout-tip icon=\"false\"}\n## {{< bi question-octagon color=#6dc83c >}} Question\n\n1.  What is the estimated value for $\\beta_0$? __________________\n\n2.  Look at the estimated values of the hyperparameters , which of the two spatial components (structured or unstructured) explains more of the variability in the counts? \n\n* (A) unstructured  \n* (B) structured  \n\n\n:::\n\n## Plot predictions\n\nWe can then create a map of the RR using the `predict` function and supply the formula of our relative risks:\n\n$$\n\\lambda_i = \\exp \\{\\beta_0 +\\beta_1 \\mathrm{AFF} + u_i +z_i\\}\n$$\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](lab_3_files/figure-pdf/unnamed-chunk-17-1.pdf){fig-align='center'}\n:::\n:::\n\n\n\n\n\n\n# Geostatistical Modelling {#sec-geo}\n\n## The Data\n\nIn this example, we will explore data on the Pacific Cod (*Gadus macrocephalus*) from a trawl survey in Queen Charlotte Sound.\n\n![](images/clipboard-3836139549.png){fig-align=\"center\" width=\"349\"}\n\nThe `pcod` dataset is available from the `sdmTMB` package and contains the presence/absence records of the Pacific Cod during each surveys along with the biomass density of Pacific cod in the area swept (kg/Km$^2$) from 2003 to 2017. In this example we **only** consider year 2003.\n\nLet's create an initial `sf` spatial object using the standard geographic coordinate system (`EPSG:4326`). This correctly defines the point locations based on latitude and longitude.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(sdmTMB)\npcod = sdmTMB::pcod \n\npcod = pcod %>% filter(year==2003)\npcod_sf =   st_as_sf(pcod, coords = c(\"lon\",\"lat\"), crs = 4326)\n```\n:::\n\n\n\n\n\n\nNow we can transform to the standard UTM Zone 9N projection (EPSG:32609) and change the spatial units to *km* to better reflect the scale of our ecological study and to make resulting distance/area values more intuitive to interpret:\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npcod_sf = st_transform(pcod_sf,\n                       crs = \"+proj=utm +zone=9 +datum=WGS84 +no_defs +type=crs +units=km\" )\n```\n:::\n\n\n\n\n\n\nLet's map the presence/absence of the Pacific Cod in 2003 using the `mapview` function:\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npcod_sf %>% \n  mutate(present = as.factor(present)) %>%\nmapview(zcol = \"present\",\n        layer.name = \"Occupancy status of Pacific Cod in 2003\")\n```\n\n::: {.cell-output-display}\n![](lab_3_files/figure-pdf/unnamed-chunk-20-1.pdf){fig-align='center' fig-pos='H'}\n:::\n:::\n\n\n\n\n\n\n::: {.callout-warning icon=\"false\"}\n## {{< bi pencil-square color=#c8793c >}} Task\n\nUse `ggplot` and the `sf` library to map the biomass density of the pacific cod\n\n\n<div class='webex-solution'><button>hint</button>\n\n\nYou can plot an`sf` object by adding a `geom_sf` layer to a ggplot object. \n</div>\n\n\n\n\n\n\n\n::: {.cell layout-align=\"center\" webex.hide='Click here to see the solution'}\n\n<div class='webex-solution'><button>Click here to see the solution</button>\n\n```{.r .cell-code}\nggplot()+ \n  geom_sf(data=pcod_sf,aes(color=density))+ \n  theme(legend.position = \"bottom\")\n```\n\n::: {.cell-output-display}\n![](lab_3_files/figure-pdf/unnamed-chunk-21-1.pdf){fig-align='center' fig-pos='H'}\n:::\n\n\n</div>\n:::\n\n\n\n\n\n:::\n\n### Raster Data\n\nThe `qcs_grid` data contain the depth values stored as $2\\times 2$ km grid for Queen Charlotte Sound. Environmental data are typically stored in raster format, which represents spatially continuous phenomena by dividing a region into a grid of equally-sized cells, each storing a value for the variable of interest. In R, the `terra` package is a modern and powerful tool for efficiently working with raster data. The function `rast()`, can be used both to read raster files from standard formats (e.g., `.tif` or `.tiff`) and to create a new raster object from a data frame. For instance, the following code creates a raster from the `qcs_grid` grid data for Queen Charlotte Sound.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nqcs_grid = sdmTMB::qcs_grid\ndepth_r <- rast(qcs_grid, type = \"xyz\")\n```\n:::\n\n\n\n\n\n\nThe raster object contains three layers corresponding to the (i) depth values, (ii) the scaled depth values and (iii) the squared depth values.\n\nNotice that there are no CRS associated with the raster. Thus, we can assign appropriate CRS using the `crs` function. Additionally, we also want the raster CRS to match the CRS in the survey data (recall that we have previously reprojected our data to utm coordinates). We can assign an appropriate CRS that matches the CRS of the `sf` object as follows:\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncrs(depth_r) <- crs(pcod_sf)\n```\n:::\n\n\n\n\n\n\nWe can use the `tidyterra` package to plot raster data using `ggplot` by adding a `geom_spatraster` function and then select an appropriate `fill` and `color` palettes:\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nggplot()+ \n  geom_spatraster(data=depth_r$depth)+\n  geom_sf(data=pcod_sf,aes(color=factor(present))) +\n    scale_color_manual(name=\"Occupancy status for the Pacific Cod\",\n                     values = c(\"black\",\"orange\"),\n                     labels= c(\"Absence\",\"Presence\"))+\n  scale_fill_viridis(name = \"Depth\",\n                   na.value = \"transparent\" )\n```\n\n::: {.cell-output-display}\n![](lab_3_files/figure-pdf/unnamed-chunk-24-1.pdf){fig-align='center' fig-pos='H'}\n:::\n:::\n\n\n\n\n\n\nWe can check the spatial extension of our study area using the `ext` function:\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\next(depth_r)[1:2] %>% diff() # difference in x coord\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nxmax \n 242 \n```\n\n\n:::\n\n```{.r .cell-code}\next(depth_r)[3:4] %>% diff()  # difference in y coord\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nymax \n 204 \n```\n\n\n:::\n:::\n\n\n\n\n\n\nNote that this is in Km since we have transformed our original data.\n\n### Autocorrelation and Variograms\n\nSpatial statistics quantifies the fundamental principle that nearby things are closely related. This spatial dependence means that observation are not independent, as assumed by most classical statistical models, but are instead correlated relative to their proximity. While this correlation can be a valuable source of information, it must be explicitly accounted for to avoid wrong inference and incorrect conclusions.\n\nThe first step is to assess whether there is any evidence of spatial dependency in our data. Spatial dependence in georeferenced data can be explored by a function known as a variogram $2\\gamma(\\cdot)$ (or semivariogram $\\gamma(\\cdot)$). The variogram is similar in many ways to the autocorrelation function used in time series modelling. In simple terms, it is a function which measures the difference in the spatial process between a pair of locations a fixed distance apart.\n\nWe can calculate the binned- empirical variogram for the data using `variogram` function from the `gstat` library. This plot shows the semi-variances for each pair of points. Lets compute a variogram for the biomass log-density of the Pacific Cod in 2003:\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nvario_binned <- gstat::variogram(log(density) ~ depth_scaled  + depth_scaled2,\n                                 data = pcod_sf %>% filter(density>0),\n                                 cloud = FALSE,\n                                 cutoff = 300)\nplot(vario_binned)\n```\n\n::: {.cell-output-display}\n![](lab_3_files/figure-pdf/unnamed-chunk-26-1.pdf){fig-align='center' fig-pos='H'}\n:::\n:::\n\n\n\n\n\n\n**Assessing spatial dependence**\n\nWe can construct null envelope based on permutations of the data values across the locations, i.e. envelopes built under the assumption of no spatial correlation. By overlapping these envelopes with the empirical variograms we can determine whether there is some spatial dependence in our data ,e.g. if our observed variograms falls outside of the envelopes constructed under spatial randomness.\n\nWe can construct permutation envelopes on the gstat empirical variogram using the `envelope` function from the `variosig` R package. Then we can visualize the results using the `envplot` function:\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nvarioEnv <- envelope(vario_binned,\n                     data = pcod_sf %>% filter(density>0),\n                     locations = st_coordinates(pcod_sf %>% filter(density>0)),\n                     formula = log(density) ~ depth_scaled  + depth_scaled2,\n                     nsim = 499)\nenvplot(varioEnv)\n```\n\n::: {.cell-output-display}\n![](lab_3_files/figure-pdf/unnamed-chunk-27-1.pdf){fig-align='center' fig-pos='H'}\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"There are 2 out of 15 variogram estimates outside the 95% envelope.\"\n```\n\n\n:::\n:::\n\n\n\n\n\n\nIt seems that the depth and squared depth covariates account for large part of the spatial variability in the log density as only 2 point fall outside the null envelopes. However, we have discarded all locations were no fish were caught so we might prefer to fit a model that explicitly accounts for this.\n\n## Hurdle Geostatistical Model\n\nHere we present a multilikelihood approach to jointly model the the log-biomass density while accounting for the presence of zeros in our data. A two-part model can be constructed to accommodate zero-inflated continuous data by combining separate likelihoods: one for the occurrence (e.g., Bernoulli) and one for the conditional positive amount (e.g., log-normal). The primary advantage of this framework is the ability to model the probability of an event and its magnitude independently.\n\n### The Model structure\n\n-   **Stage 1** Model for the response(s)$$\n    \\begin{aligned}\n    y_i|\\eta^{(1)}_i&\\sim \\text{Binomial}(1,\\pi_i)\\\\\n    \\log(z_i)|\\eta^{(2)}_i&\\sim \\text{Normal}(\\mu_i,\\tau_e^{-1})\\\\\n    \\end{aligned}\n    $$\n\n    -   We then define a likelihood for each outcome.\n\n        -   $y_i =\\begin{cases} 1 &\\text{if fishes have been caught at location } \\mathbf{s}_i  \\\\ 0 &\\text{otherwise}\\end{cases}$\n\n        -   $z_i =\\begin{cases} NA &\\text{if no fish were  caught at location } \\mathbf{s}_i  \\\\ \\text{biomass density at location } \\mathbf{s}_i &\\text{otherwise}\\end{cases}$\n\nThis structure is equivalent to a **Hurdle-log-Normal model**, where the overall expected value of log biomass is given by the product $\\pi_{i} * \\mu_{i}$, with $\\mu_{i}$ representing the conditional expectation from the log-normal component.\n\nNext we define the components of our linear predictor. Notice how we are defining this model in a LGM framework:\n\n-   **Stage 2** Latent field model $$\n    \\begin{aligned}\n    \\eta^{(1)}_i &= \\text{logit}(\\pi_i) = X'\\beta + \\xi_i\\\\\n    \\eta^{(2)}_i &= \\mu_i = X'\\alpha + \\omega_i\n    \\end{aligned}\n    $$\n    -   $\\{\\alpha,\\beta\\}$ = Intercepts + covariate effects.\n    -   $\\{\\xi,\\omega\\}$ = are the Gaussian fields with Matérn covariance (separate for each outcome).\n\nFor the occurrence of fish, the linear predictor gets mapped to the logit of the probability of the Bernoulli model while the linear predictor for the biomass density is mapped to the mean of a log normal distribution.\n\n-   **Stage 3** Hyperparameters\n\nThe hyperparameter for the model are:\n\n-   observational error (nugget) $\\tau_e$\n-   Matérn field(s) parameters $\\{\\rho^{(1)},\\rho^{(2)},\\tau_{d}^{(1)},\\tau_{d}^{(2)}\\}$\n\n### The workflow\n\nWhen fitting a geostatistical model we need to fulfill the following tasks:\n\n1.  Build the mesh\n\n2.  Define the SPDE representation of the spatial GF. This includes defining the priors for the range and sd of the spatial GF\n\n3.  Define the *components* of the linear predictor. This includes the spatial GF and all eventual covariates\n\n4.  Define the observation model using the `bru_obs()` function\n\n5.  Run the model using the `bru()` function\n\n### 1. Building the mesh {.unnumbered}\n\nThe first task, when dealing with geostatistical models is to build the mesh that covers the area of interest. For this purpose we use the function `fm_mesh_2d`.\n\nOne way to build the mesh is to start from the locations where we have observations, these are contained in the dataset `pcod_sf`.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmesh = fm_mesh_2d(loc = pcod_sf,           # Build the mesh\n                  max.edge = c(10,20),     # The largest allowed triangle edge length.\n                  offset = c(5,50))       # The automatic extension distance\nggplot() + gg(mesh) + geom_sf(data= pcod_sf, aes(color = factor(present)), size = 0.1) + xlab(\"\") + ylab(\"\")\n```\n\n::: {.cell-output-display}\n![](lab_3_files/figure-pdf/unnamed-chunk-28-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n\n\nAs you can see from the plot above, some of the locations are very close to each other, this causes some very small triangles. This can be avoided using the option `cutoff =` which collapses the locations that are closer than a cutoff (those points are collapsed in the mesh construction but, of course, not when it come to estimaation.)\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmesh = fm_mesh_2d(loc = pcod_sf,           # Build the mesh\n                  cutoff = 2,\n                  max.edge = c(10,20),     # The largest allowed triangle edge length.\n                  offset = c(5,50))       # The automatic extension distance\nggplot() + gg(mesh) + geom_sf(data= pcod_sf, aes(color = factor(present)), size = 0.1) + xlab(\"\") + ylab(\"\")\n```\n\n::: {.cell-output-display}\n![](lab_3_files/figure-pdf/unnamed-chunk-29-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n\n\n::: {.callout-warning icon=\"false\"}\n## {{< bi pencil-square color=#c8793c >}} Task\n\nLook at the documentation for the `fm_mesh_2d` function typing\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n?fm_mesh_2d\n```\n:::\n\n\n\n\n\n\nplay around with the different options and create different meshes.\n\nThe *rule of thumb* is that your mesh should be:\n\n-   fine enough to well represent the spatial variability of your process, but not too fine in order to avoid computation burden\n-   the triangles should be regular, avoid long and thin triangles.\n-   The mesh should contain a buffer around your area of interest (this is what is defined in the `offset` option) in order to avoid boundary artefact in the estimated variance.\n:::\n\n### 2. Define the SPDE representation of the spatial GF {.unnumbered}\n\nTo define the SPDE representation of the spatial GF we use the function `inla.spde2.pcmatern`. This takes as input the mesh we have defined and the PC-priors definition for $\\rho$ and $\\sigma$ (the range and the marginal standard deviation of the field).\n\nPC priors Gaussian Random field are defined in (Fuglstad et al. 2018). From a practical perspective for the range $\\rho$ you need to define two paramters $\\rho_0$ and $p_{\\rho}$ such that you believe it is reasonable that\n\n$$\nP(\\rho<\\rho_0)=p_{\\rho}\n$$\n\nwhile for the marginal variance $\\sigma$ you need to define two parameters $\\sigma_0$ and $p_{\\sigma}$ such that you believe it is reasonable that\n\n$$\nP(\\sigma<\\sigma_0)=p_{\\sigma}\n$$\n\nHere are some alternatives for defining priors for our model\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nspde_model1 =  inla.spde2.pcmatern(mesh,\n                                  prior.sigma = c(.1, 0.5),\n                                  prior.range = c(30, 0.5))\nspde_model2 =  inla.spde2.pcmatern(mesh,\n                                  prior.sigma = c(10, 0.5),\n                                  prior.range = c(1000, 0.5))\nspde_model3 =  inla.spde2.pcmatern(mesh,\n                                  prior.sigma = c(1, 0.5),\n                                  prior.range = c(100, 0.5))\n```\n:::\n\n\n\n\n\n\n::: {.callout-tip icon=\"false\"}\n## {{< bi question-octagon color=#6dc83c >}} Question\n\nConsider the `pcod_sf`, the spatial extension and type of the data...is some of the previous choices more reasonable than other? \n\n* (A) spde_model1  \n* (B) spde_model2  \n* (C) spde_model3  \n\n\n\n**NOTE** Remember that a prior should be reasonable..but the model should not totally depend on it.\n:::\n\n### 3. Define the components of the linear predictor {.unnumbered}\n\nWe have now defined a mesh and a SPDE representation of the spatial GF. We now need to define the model components:\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncmp <- ~\n   Intercept_biomass(1) +\n    depth_biomass(depth_scaled, model = \"linear\") +\n    depth2_biomass(depth_scaled2, model = \"linear\") +\n    space_biomass(geometry, model = spde_model3) +\n    Intercept_caught(1) +\n    depth_caught(depth_scaled, model = \"linear\") +\n    depth2_caught(depth_scaled2, model = \"linear\") +\n    space_caught(geometry, model = spde_model3)\n```\n:::\n\n\n\n\n\n\n**NOTE** since the dataframe we use (`pcod_sf`) is an `sf` object the input in the `space()` component is the geometry of the dataset.\n\n### 4. Define the observation model {.unnumbered}\n\nSince we have two likelihoods we need to define two observational models\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfml_1 = density ~  Intercept_biomass + depth_biomass + depth2_biomass + space_biomass\nfml_2 = present ~ Intercept_caught + depth_caught + depth2_caught + space_caught\n  \nbiomass_obs <- bru_obs(formula = fml_1,\n      family = \"lognormal\",\n      data = pcod_sf  %>% filter(density>0)) # restrict to those locations where fish were caught\n\npresence_obs <- bru_obs(formula = fml_2 ,\n  family = \"binomial\",\n  data = pcod_sf,\n)\n```\n:::\n\n\n\n\n\n\n### 5. Run the model & Extract results {.unnumbered}\n\n<!-- Before we fit the model we can tell INLA that we want to compute different model comparison metrics: -->\n\n<!-- ```{r} -->\n\n<!-- bru_options_set(control.compute = list(dic = TRUE, -->\n\n<!--                                        waic = TRUE, -->\n\n<!--                                        mlik = TRUE, -->\n\n<!--                                        cpo = TRUE)) -->\n\n<!-- ``` -->\n\nNow we fit the model and produce some summaries:\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_hurdle <- bru(\n  cmp,\n  biomass_obs,\n  presence_obs\n)\n\nsummary(fit_hurdle)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ninlabru version: 2.13.0.9016 \nINLA version: 25.08.21-1 \nComponents: \nLatent components:\nIntercept_biomass: main = linear(1)\ndepth_biomass: main = linear(depth_scaled)\ndepth2_biomass: main = linear(depth_scaled2)\nspace_biomass: main = spde(geometry)\nIntercept_caught: main = linear(1)\ndepth_caught: main = linear(depth_scaled)\ndepth2_caught: main = linear(depth_scaled2)\nspace_caught: main = spde(geometry)\nObservation models: \n  Family: 'lognormal'\n    Tag: <No tag>\n    Data class: 'sf', 'tbl_df', 'tbl', 'data.frame'\n    Response class: 'numeric'\n    Predictor: \n        density ~ Intercept_biomass + depth_biomass + depth2_biomass + \n            space_biomass\n    Additive/Linear: TRUE/TRUE\n    Used components: effects[Intercept_biomass, depth_biomass, depth2_biomass, space_biomass], latent[] \n  Family: 'binomial'\n    Tag: <No tag>\n    Data class: 'sf', 'tbl_df', 'tbl', 'data.frame'\n    Response class: 'numeric'\n    Predictor: present ~ Intercept_caught + depth_caught + depth2_caught + space_caught\n    Additive/Linear: TRUE/TRUE\n    Used components: effects[Intercept_caught, depth_caught, depth2_caught, space_caught], latent[] \nTime used:\n    Pre = 1.36, Running = 9.21, Post = 0.833, Total = 11.4 \nFixed effects:\n                    mean    sd 0.025quant 0.5quant 0.975quant   mode   kld\nIntercept_biomass  3.499 0.332      2.885    3.482      4.227  3.435 0.000\ndepth_biomass     -0.509 0.294     -1.098   -0.506      0.058 -0.506 0.000\ndepth2_biomass    -0.340 0.228     -0.789   -0.339      0.106 -0.339 0.000\nIntercept_caught   1.735 2.020     -2.336    1.689      6.004  1.684 0.012\ndepth_caught      -2.612 0.552     -3.823   -2.555     -1.680 -2.379 0.000\ndepth2_caught     -1.520 0.340     -2.261   -1.489     -0.933 -1.434 0.000\n\nRandom effects:\n  Name\t  Model\n    space_biomass SPDE2 model\n   space_caught SPDE2 model\n\nModel hyperparameters:\n                                           mean     sd 0.025quant 0.5quant\nPrecision for the lognormal observations   1.15  0.548      0.465    1.027\nRange for space_biomass                   36.66 25.855      6.913   30.158\nStdev for space_biomass                    1.00  0.277      0.571    0.962\nRange for space_caught                   158.44 91.101     56.927  135.609\nStdev for space_caught                     2.20  0.662      1.192    2.105\n                                         0.975quant    mode\nPrecision for the lognormal observations       2.56   0.827\nRange for space_biomass                      103.59  18.563\nStdev for space_biomass                        1.66   0.888\nRange for space_caught                       399.11 101.377\nStdev for space_caught                         3.77   1.923\n\nMarginal log-Likelihood:  -657.88 \n is computed \nPosterior summaries for the linear predictor and the fitted values are computed\n(Posterior marginals needs also 'control.compute=list(return.marginals.predictor=TRUE)')\n```\n\n\n:::\n:::\n\n\n\n\n\n\n### Model Predictions:\n\nWe now want to extract the estimated posterior mean and sd of spatial GF. To do this we first need to define a grid of points where we want to predict:\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npxl1 = data.frame(crds(depth_r), \n                  as.data.frame(depth_r)) %>% \n       filter(!is.na(depth)) %>%\nst_as_sf(coords = c(\"x\",\"y\"),crs=st_crs(pcod_sf)) \n```\n:::\n\n\n\n\n\n\nthen compute the prediction for both the spatial GFs and both linear predictors (note that we use the `plogis` function to map $\\eta_i^{(1)}$ to $\\pi_i^{(i)}$). To compute the expected log-biomass density we need:\n\n-   $\\pi_i^{(i)}$ = Catching probability\n\n-   $\\mathbb{E}[Z(s)|Y(s)] = \\exp\\left(\\mu(s) + \\dfrac{1}{2\\tau_{e}}\\right)$ = conditional mean\n\n-   $\\mathbb{E}(Z(s)) =\\pi(s)\\times \\mathbb{E}[Z(s)|Y(s)]$ = unconditional mean\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npred <- predict( fit_hurdle , pxl1,\n  ~ {\n    pi <- plogis(Intercept_caught + depth_caught + depth2_caught + space_caught)  # catching probability\n    mu_log  <-  Intercept_biomass + depth_biomass + depth2_biomass + space_biomass  \n    sd <- sqrt(1/Precision_for_the_lognormal_observations)\n    conditional_mean <- exp(mu_log + 0.5 * sd^2)  \n    dens <- pi * conditional_mean # biomass density\n    list(\n      pi = pi,\n      conditional_mean = conditional_mean,\n      dens = dens)\n  },n.samples = 2500)\n```\n:::\n\n\n\n\n\n\nFinally, we can plot the maps\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot() + gg(pred$pi, geom = \"tile\",aes(fill = mean)) +  scale_fill_viridis() + ggtitle(\"Posterior mean for catch probability\")\n```\n\n::: {.cell-output-display}\n![](lab_3_files/figure-pdf/unnamed-chunk-37-1.pdf){fig-pos='H'}\n:::\n\n```{.r .cell-code}\nggplot() + gg(pred$dens, geom = \"tile\",aes(fill = mean))+  scale_fill_viridis(option = \"G\")+ ggtitle(\"Posterior mean of biomass density \")\n```\n\n::: {.cell-output-display}\n![](lab_3_files/figure-pdf/unnamed-chunk-37-2.pdf){fig-pos='H'}\n:::\n\n```{.r .cell-code}\nggplot() + gg(pred$dens, geom = \"tile\",aes(fill = sd)) + scale_fill_viridis(option = \"B\") + ggtitle(\"Posterior sd biomass density\")\n```\n\n::: {.cell-output-display}\n![](lab_3_files/figure-pdf/unnamed-chunk-37-3.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n\n\n**Note** The posterior sd is lowest at the observation points. Note how the posterior sd is inflated around the border, this is the \"border effect\" due to the SPDE representation.\n\n# Point Process Modelling {#sec-points}\n\n## Gorilla Nesting Sites\n\nIn this example we will fit a point process model to analyse the locations of nesting sites of gorillas, and associated covariates, in a National Park in Cameroon.\n\n![](images/clipboard-1355085094.png){fig-align=\"center\" width=\"444\"}\n\nThe data set is available from the `inlabru` R package. The data come from a study of gorillas in the Kagwene Gorilla Sanctuary, Cameroon, by the Wildlife Conservation Society Takamanda-Mone Landscape Project (WCS-TMLP). The dataset contains the spatial locations of 647 nesting sites of gorilla groups observed in the sanctuary over time. Locations are given as UTM (Zone 32N) coordinates in metres. The observation window is the boundary of the sanctuary, represented as a polygon.\n\nFirst lets load the data:\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(gorillas_sf, package = \"inlabru\")\n```\n:::\n\n\n\n\n\n\nHere we will extract the information about:\n\n-   The spatial location of the nests (`nests`)\n-   The mesh for building our spatial model (`mesh`)\n-   The observational window (`boundary`)\n-   The Digital elevation of terrain, in metres (`elev`).\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnests <- gorillas_sf$nests\nmesh <- gorillas_sf$mesh\nboundary <- gorillas_sf$boundary\ngcov <- gorillas_sf_gcov()\nelev <- gcov$elevation\nelev <- elev - mean(terra::values(elev), na.rm = TRUE) #scale the  covariate\n```\n:::\n\n\n\n\n\n\nWe can visualize the Data as follows:\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot() +\n  gg(elev) +\n  gg(boundary, alpha = 0.2) +\n  gg(nests, color = \"white\", cex = 0.5)+\n  scale_fill_viridis(option = \"F\")\n```\n\n::: {.cell-output-display}\n![](lab_3_files/figure-pdf/unnamed-chunk-40-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n\n\nWe can see that the elevation covariate has been extended beyond the region of interest. Recall that this is need for computational purposes as we need the covariate to be available at the quadrature locations for approximating our point process model.\n\n## The Model\n\nWe are now going to use the scaled-elevation as a covariate in addition to a GF to explain the variability of the intensity $\\lambda(s)$ over the domain of interest.\n\nOur model is $$\n\\log\\lambda(s) = \\beta_0 + \\beta_1x(s) + \\xi(s)\n$$ where $x(s)$ is the altitude at location $s$ and $\\xi(s)$ is a GF\n\nThe likelihood becomes:\n\n$$\n\\begin{aligned}\np(\\mathbf{y} | \\lambda)  & \\propto \\exp \\left( -\\int_\\Omega \\lambda(\\mathbf{s}) \\mathrm{d}\\mathbf{s} \\right) \\prod_{i=1}^n \\lambda(\\mathbf{s}_i) \\\\\n& = \\exp \\left( -\\int_\\Omega \\exp(\\beta_0 + \\beta_1x(s)) \\mathrm{d}\\mathbf{s} \\right) \\prod_{i=1}^n \\lambda(\\mathbf{s}_i) \\\\\n\\end{aligned}\n$$\n\nwhere $|\\Omega|$ is the area of the domain of interest.\n\nWe need to approximate the integral using a numerical integration scheme as:\n\n$$\n\\approx\\exp\\left(-\\sum_{k=1}^{N_k}w_k\\lambda(s_k)\\right)\\prod_{i=1}^n \\lambda(\\mathbf{s}_i)\n$$\n\nWhere $N_k$ is the number of integration points $s_1,\\dots,s_{N_k}$ and $w_1,\\dots,w_{N_k}$ are the integration weights. In addition we also need to approximate the GF - we can use the SPDE approach as before to achieve this.\n\nThe first step, as any time we use the SPDE approach is to define the mesh and the priors for the marginal variance and range:\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmesh <- gorillas_sf$mesh\n\nggplot() + gg(mesh) + geom_sf(data = nests)\n```\n\n::: {.cell-output-display}\n![](lab_3_files/figure-pdf/unnamed-chunk-41-1.pdf){fig-pos='H'}\n:::\n\n```{.r .cell-code}\nspde_model =  inla.spde2.pcmatern(mesh,\n  prior.sigma = c(0.1, 0.01),\n  prior.range = c(0.1, 0.01)\n)\n```\n:::\n\n\n\n\n\n\nWe can then define the integration weights. Here we use the same points to `define the SPDE approximation` and to `approximate the integral` in the likelihood (note that it does not have to be like this, BUT integration weight and SPDE weights have to be consistent with each other)\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nips = fm_int(mesh, samplers = boundary)\n\nggplot() + geom_sf(data = ips, aes(color = weight)) +\n  gg(mesh) +\n   scale_color_viridis()\n```\n\n::: {.cell-output-display}\n![](lab_3_files/figure-pdf/unnamed-chunk-42-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n\n\n## Fitting the Model\n\nThe model has three components: intercept, linear effect of altitude and the spatial GRF\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"show\"}\ncmp = ~ Intercept(1) + space(geometry, model = spde_model) + elev(elev, model = \"linear\")\n\nformula = geometry ~ Intercept + space + elev\n\n\nlik = bru_obs(\"cp\",\n              formula = formula,\n              data = nests,\n              ips = ips)\n\nfit_gorillas = bru(cmp, lik)\n\nsummary(fit_gorillas)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ninlabru version: 2.13.0.9016 \nINLA version: 25.08.21-1 \nComponents: \nLatent components:\nIntercept: main = linear(1)\nspace: main = spde(geometry)\nelev: main = linear(elev)\nObservation models: \n  Family: 'cp'\n    Tag: <No tag>\n    Data class: 'sf', 'tbl_df', 'tbl', 'data.frame'\n    Response class: 'numeric'\n    Predictor: geometry ~ Intercept + space + elev\n    Additive/Linear: TRUE/TRUE\n    Used components: effects[Intercept, space, elev], latent[] \nTime used:\n    Pre = 1.04, Running = 9.17, Post = 0.511, Total = 10.7 \nFixed effects:\n           mean    sd 0.025quant 0.5quant 0.975quant  mode kld\nIntercept 1.127 0.478      0.154    1.137      2.038 1.137   0\nelev      0.004 0.001      0.002    0.004      0.006 0.004   0\n\nRandom effects:\n  Name\t  Model\n    space SPDE2 model\n\nModel hyperparameters:\n                mean    sd 0.025quant 0.5quant 0.975quant  mode\nRange for space 1.76 0.217      1.376     1.75       2.23 1.713\nStdev for space 1.00 0.085      0.848     1.00       1.18 0.995\n\nMarginal log-Likelihood:  -1254.95 \n is computed \nPosterior summaries for the linear predictor and the fitted values are computed\n(Posterior marginals needs also 'control.compute=list(return.marginals.predictor=TRUE)')\n```\n\n\n:::\n:::\n\n\n\n\n\n\n## Model Predictions\n\nThe predict function of `inlabru` can take as argument an sf objects. So, we can use the `inlabru` function `fm_pixels` to generate an sf object with points only within the boundary, using its `mask` argument, as shown below:\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npred.df <- fm_pixels(mesh, mask = boundary)\n```\n:::\n\n\n\n\n\n\nThen, we simply supply the intensity and log intensity :\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ne.pred <- predict(\n  fit_gorillas,\n  pred.df,\n  ~ list(\n    int = exp(space + elev + Intercept),\n    int.log = space + elev + Intercept\n  )\n)\n\np1 <- ggplot() +\n  gg(e.pred$int, aes(fill = log(sd)), geom = \"tile\") +\n  gg(boundary, alpha = 0) +\n  gg(nests, shape = \"+\") \np2 <- ggplot() +\n  gg(e.pred$int.log, aes(fill = exp(mean + sd^2 / 2)), geom = \"tile\") +\n  gg(boundary, alpha = 0) +\n  gg(nests, shape = \"+\")\n(p1 | p2)\n```\n\n::: {.cell-output-display}\n![](lab_3_files/figure-pdf/unnamed-chunk-45-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n\n\n### Estimate abundance\n\nFinally, we want to use the fitted model to estimate the total number of nesting sites in the whole region. To do this, we first have to define the expected number of sites as:\n\n$$\nE(N_{\\Omega}) = \\int_{\\Omega}\\exp(\\lambda(s))\\ ds\n$$\n\nThen simulate a realization of $N_{\\Omega}$ to include also the likelihood variability in our estimate. To do so, we pass both the fitted model and the integration weights to the predict function. Then, we specify a reasonable range for where posterior poisson density is defined:\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nNest.e <- predict(\n  fit_gorillas,\n  fm_int(mesh, boundary),\n  ~ data.frame(\n    N = 400:900,\n    density = dpois(400:900,\n      lambda = sum(weight * exp(space + elev + Intercept))\n    )\n  ),\n  n.samples = 2000\n)\n```\n:::\n\n\n\n\n\n\nWe can plot the posterior density of the abundance as follows:\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(data = Nest.e) +\n  geom_line(aes(x = N, y = mean, colour = \"Posterior\")) +\n    geom_vline(xintercept = nrow(nests),\n             colour = \"red\") +\n  xlab(expression(Lambda))\n```\n\n::: {.cell-output-display}\n![](lab_3_files/figure-pdf/unnamed-chunk-47-1.pdf){fig-pos='H'}\n:::\n:::\n",
    "supporting": [
      "lab_3_files\\figure-pdf"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}