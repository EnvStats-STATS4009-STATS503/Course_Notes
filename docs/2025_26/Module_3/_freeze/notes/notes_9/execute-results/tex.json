{
  "hash": "69135a43f4b930a65ff216a1f56eb2d4",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Modelling Geostatistical Data\"\nformat:\n  html:    \n    fontsize: \"12\"\n    mainfont: serif\n    code-link: true\n    code-fold: show\n    code-tools:\n      source: false\n      toggle: true\n    toc: true\n    toc-location: left\n    toc-title: Contents\n    number-sections: true\n  PrettyPDF-pdf:\n    number-sections: true\nembed-resources: true\nexecute: \n  freeze: auto\neditor: visual\neditor_options: \n  chunk_output_type: console\nbibliography: references.bib\n---\n\n::: {.cell}\n\n:::\n\n\n\n\n\n\n# Point processes\n\nIn ecology many of the ecological processes of interest can be view as an aggregation of a spatial point process. For example, @fig-frompoints2occ shows how species occupancy (presence or absence of a species in a given area), and abundance (the number of individual of a species that occur in an area) are related quantities to a spatial point process.\n\n![Illustration of the unidirectional information exchange structure of different spatial processes in ecology.](images/frompointstoocc.png){#fig-frompoints2occ fig-align=\"center\"}\n\nMany contemporary data sources (e.g., Citizen science, GPS tracking, camera traps, bio-logging devices, etc) collect georeferenced information about the location where species occur. This point-based information provides valuable insights into species distributions and ecosystem dynamics.\n\nA key focus of spatial point pattern analysis is to quantify spatial arrangements, whether aggregated, uniform, or random. Recognizing these patterns is fundamental to interpreting ecological interactions, such as territoriality, competition, and social behavior. Moreover, spatial point patterns can reveal mechanisms of species coexistence and the factors influencing biodiversity.\n\nFrom a conservation perspective, the clustering of habitat patches---conceptualized as points across a landscape---can have profound effects on extinction risk and population persistence. Understanding the spatial scale of these patterns helps ecologists assess species invasion dynamics and long-term population viability.\n\nIn the following sections, we introduce spatial point pattern analysis and its application to ecological and conservation-related questions, emphasizing how species dispersion patterns shape and inform biodiversity management.\n\nA spatial point process is a set of locations...presumed to have been generated by some form of stochastic (random) mechanism\".\n\nIn other words, the point process is a random variable operating in continuous space, and we observe realisations of this variable as point patterns across space (and/or time). Spatial point pattern analysis focuses on examining patterns of points to establish whether there are regularities in the process they represent.\n\nConsider a fixed geographical region $A$*.* The set of locations at which events occur are denoted $\\mathbf{s} = s_1,\\ldots,s_n$. We let $N(A)$ be the random variable which represents the number of events in region $A$.\n\nOur primary interest is in measuring where events occur, so the **locations are our data**. We typically assume that a spatial point pattern is generated by an unique point process over the whole study area. This means that the delimitation of the study area will affect the observed point patters.\n\nThe observed distribution of points can be described based on the intensity of points within a delimited region. Thereof, we want to know whether there is any particular spatial pattern associated with the process.\n\nThere are three broad types of spatial structures which can be explored, each representing a different type of spatial dependence.\n\n-   **Complete spatial randomness** (CSR) - events occur at random, and independently of each other.\n\n-   **Clustered process** - events occur close to existing events.\n\n-   **Regular process** - events occur away from existing events.\n\n![Examples of spatial point patterns](images/SPP.png){fig-align=\"center\" width=\"530\"}\n\nUnfortunately, it still isn't straightforward to determine by eye which of these categories a set of points falls into. So we need a more systematic approach.\n\n## Summaries of Point processes\n\nWe can define the (first order) **intensity** of a point process as the **expected number of events per unit area.** This can also be thought of as a measure of the density of our points. In some cases, the intensity will be constant over space (homogeneous), while in other cases it can vary by location (inhomogeneous or heterogenous). If our intensity is homogeneous, we can define it as\n\n$$\n\\lambda(s) = \\dfrac{N(A)}{|A|} =  \\lambda\n$$ We can use the concept of intensity to help us define **complete spatial randomness** (CSR).\n\n::: {.callout-important appearance=\"simple\" icon=\"false\"}\n## Def.\n\nFor any spatial region A, CSR requires that:\n\n1.  *Uniformity and Independent scattering* : Given the number of events $N(A) = n$ in a region, the $n$ events are independently and uniformly distributed over space (i.e., each event has an equal probability of occurring anywhere in the study area).\n\n2.  *Poisson distribution of point counts*: The number of points in any set $A_i$ follows a Poisson distribution with mean $\\lambda|A_i|$, that is $$N(A_i) \\sim \\text{Poisson}(\\lambda|A_i|).$$\n\n3.  If these conditions are satisfied, we can describe our process as a **homogeneous Poisson process**.\n:::\n\nThe likelihood of a point pattern $\\mathbf{y} = \\left[ \\mathbf{s}_1, \\ldots, \\mathbf{s}_n \\right]^\\intercal$ distributed as a HPP with intensity $\\lambda$ and observation window $\\Omega$ is\n\n$$\n    p(\\mathbf{y} | \\lambda) \\propto \\lambda^n e^{ \\left( - |\\Omega| \\lambda \\right)} ,\n  $$\n\n-   $|\\Omega|$ is the size of the observation window.\n\n```         \n-   $\\lambda$ is the expected number of points per unit area.\n\n-   $|\\Omega|\\lambda$ the total expected number of points in the observation window.\n```\n\nA key property of a Poisson process is that the number of points within any subset $A_i$ of region $A$ is Poisson distributed with constant rate $|A_i|\\lambda$.\n\nWhile CSR rarely occurs in nature, it is the simplest null model that we use can to a determine whether an homogeneous Poisson process is appropriate for our data or not. We do so by contrasting the observed point pattern with a point pattern generated from the CRS model. One of the most common approaches for identifying point patterns is the **Ripley's *K*** function.\n\n## Ripley's K function\n\nRipley's $K$ calculates the degree of spatial aggregation of points within a circle (buffer) of radius $r$ and contrasts the observed pattern to that expected under CSR. Ripley's K is defined as:\n\n$$\nK(r) = \\dfrac{E[N(s_0,r)]}{\\lambda},\n$$\n\nHere, $N(s_0,r)$ denotes the number of events that occur within distance $r$ of an event $s_0$; clearly, as $r$ increases, so too will $K(r)$.\n\n![Buffer with radius $r$ centred at the event $s_0$.](images/ripleysK.png){fig-align=\"center\" width=\"297\"}\n\nRipley's $K$ can be estimated as:\n\n$$\n\\hat{K}(r) = \\frac{1}{n}\\sum_{i=1}^n\\sum_{i\\neq j}I(d_{ij}<r) \\times \\lambda^{-1}\n$$\n\nThis first part of the equation correspond to the expected number of events that occur within a buffer of radius $r$ , i.e. for each point $s_i \\in i,\\ldots,n$ we count the number of points other than $i$ that fall within the buffer of radius $r$ and then we sum up the number of neighbors for all points and weight it by the total number of points $n$ in the whole region $A$. The second part of the equation correspond to the density of events estimated as $\\lambda = n/|A|$.\n\nThe idea is to compare $\\hat{K}(r)$ against the expected $K(r)$ under CSR. If we assume an *homogeneous Poisson process* and given the area is that of a circle, we would expect that under CSR the expected $K(r)$ is:\n\n$$\nK_{CRS}(r) = \\dfrac{\\lambda \\pi r^2}{\\lambda}=\\pi\\times r^2\n$$\n\nThat is, under CSR we would expect that the $K$ function is equal to the area of the circle with radius $r$. Then we compare $\\hat{K}(r)$ and $K_{CSR}(r)$:\n\n-   If $\\hat{K}(r) > K_{CSR}(r)$ it means that more points are found within a radius $r$ than what would be expected under complete randomness, suggesting a clustering pattern. $\\hat{K}(r)$ will be relatively large for small values of $r$, since events are likely to be surrounded by further members of the same cluster. E.g., tree seedlings often cluster near parent trees due to seed dispersal limitations.\n\n-   If $\\hat{K}(r) < K_{CSR}(r)$, it indicates that the pattern is more regular since we observe fewer neighboring points within a distance $r$ than expected under CSR. For regular patterns, $K(r)$ will be relatively small for small values of $r$, since there is likely to be more empty space around events. E.g., territorial animals (e.g., nesting birds) often exhibit regular spacing due to competition for space.\n\nWhen working with real data, some natural variation is to be expected even when CSR holds. We therefore need an approach which accounts for this when assessing for CSR to determine whether or not the observed pattern is non-random.\n\nWe can estimate $\\hat{K}(r)$ across a set of distances $r$ for our set of observed events. Then, $\\hat{K}(r)$ can be compared to the theoretical function for CSR, $K(r) = \\pi \\times r^2$. If the two functions are similar, then CSR is reasonable. See the example below,\n\n![Simulated regular point process data and associate Ripley's K function. Red line indicates theoretical $K_{CSR}(r)$ and back line $\\hat{K}(r)$.](images/CellsK.png){fig-align=\"center\" width=\"465\"}\n\nIn this example we can notice that for most distances $\\hat{K}(r) < K_{CSR}(r)$ . However we cannot be completely sure if the observed point pattern is best described by a CSR process or if the pattern is more regular. Thus, we might wish to be more precise in our comparison of the two lines.\n\nWe can calculate what is known as a *simulation envelope* which is produced by simulating multiple sets of data under CSR. These are not theoretically the same as confidence intervals, but can be described as significance bands. If our observed line falls outside of the envelope, this implies that CSR is not reasonable. Constructing a simulation envelope in our example produces:\n\n![Simulated regular point process data and associate Ripley's $K$ function. Red line dotted indicates theoretical $K_{CSR}(r)$ with grey areas denoting the *simulation envelope* and black solid line $\\hat{K}(r)$.](images/CellsK_env.png){fig-align=\"center\" width=\"539\"}\n\nIn this example the black line lies outside the envelope between $r = 0$ and $r = 0.15$ therefore we still conclude that the data are regularly spaced. @fig-ripleys_ex shows examples of simulated cluster and regular patterns with their associated Ripley's $K$ function.\n\n![Simulated random (A) and clusterred (B) point processes data and associated Ripley's $K$ function. Red line dotted indicates theoretical $K_{CSR}(r)$ with grey areas denoting the *simulation envelope* and black solid line $\\hat{K}(r)$.](images/ripleysK_ex.png){#fig-ripleys_ex fig-align=\"center\" width=\"409\"}\n\nWhile Ripley's $K$ function is widely used in environmental and ecological studies it has some caveats. For example, $K(s)$ is a cumulative function, where all points less than $r$ are also used. So, if there is a strong clustering patter at 5m but no pattern at larger distances, then Ripley's $K$ could still indicate a strong clustering at larger scales due to the data $<$ 5m still being used.\n\nAnother issues with this method is its sensitivity to *edge effects.* This occurs because points near the boundaries of the study area have fewer neighboring points within distance $r$, leading to underestimation of \\$K(r)\\$ (when considering a radius $r$ from a point near a boundary, the number of observed points is likely lower than the true number of points if points could occur outside of study area).\n\nThe $K(r)$ function can be adjusted for edge effects by including some weights $w$. There are several weights to specify these weights. For example, if the $i$th buffer lies completely within the study area then it receives a weight of $w= 1$ but if it lies outside it receives a weight inverse of the fraction of circumference lying inside the area, (e.g., if half of a point's search area lies within the study area then the point will receive a weight $w=2$, meaning it contributes twice as much to compensate for the missing area).\n\nLastly, the above analyses assume that the point process is *stationary* (homogeneous over space) and isotropic (point process does not change with direction). These assumption rarely hold true in real-data, thus, *inhomogeneous point process* (IPP) *mode*l*s* are often used for inference prediction and mapping spatial patterns.\n\n# Inhomogeneous Point Process Models\n\nSo far we have assumed that the point process is stationary and isotropic. However, these assumption rarely hold true in real-data. Thus, *inhomogeneous Poisson process* (IPP) models are often used for inference prediction and mapping spatial patterns\n\nThe IPP has a spatially varying intensity $\\lambda(\\mathbf{s})$ defined in terms of spatially varying covariates that are available across the whole study area:\n\n$$\n\\lambda(s) = \\mathrm{exp}(\\alpha+\\beta x(s) +\\ldots )\n$$\n\nwhere $\\beta= \\{\\beta_1,\\ldots,\\beta_p\\}$ is a vector of parameters corresponding to the $p$ environmental covariates $\\mathbf{x}(s)$. Let $\\mathbf{y} = s_1,\\ldots,s_n$ the $n$ number of observed events/points in an observation window $\\Omega$\n\nFor an IPP with an intensity $\\lambda(s)$, the likelihood is given by:\n\n$$\np(\\mathbf{y} | \\lambda) \\propto \\exp \\left( -\\int_\\Omega \\lambda(\\mathbf{s}) \\mathrm{d}\\mathbf{s} \\right) \\prod_{i=1}^n \\lambda(\\mathbf{s}_i).\n$$\n\nNote that inf the case of an **HPP** the integral in the likelihood can easily be computed as $\\int_\\Omega \\lambda(\\mathbf{s}) \\mathrm{d}\\mathbf{s} =|\\Omega|\\lambda$. For an **HPP** with an intensity $\\lambda$, the log-likelihood is given by:\n\n$$\n    l(\\beta;\\mathbf{y}) = n\\log(\\lambda) -\\lambda|\\Omega|,\n$$\n\nThus, the maximum likelihood estimators is $\\hat{\\lambda} = n/|\\Omega|$. However, for an **IPP**, the integral in the likelihood has to be approximateda as a weighted sum.\n\n## Fitting an IPP\n\nThis integral is approximated as $\\int_\\Omega \\lambda(\\mathbf{s}) \\mathrm{d}\\mathbf{s} \\approx \\sum_{j=1}^J w_j \\lambda(\\mathbf{s}_j)$.\n\n-   $w_j$ are the integration weights\n\n-   $\\mathbf{s}_j$ are the quadrature locations.\n\nThis serves two purposes:\n\n1.  Approximating the integral\n\n2.  re-writing the inhomogeneous Poisson process likelihood as a regular Poisson likelihood.\n\nThe idea behind this trick is to rewrite the approximate likelihood by introducing a dummy vector $\\mathbf{z}$ and an integration weights vector $\\mathbf{w}$ of length $J + n$ such that\n\n-   $\\mathbf{z} = \\left[\\underbrace{0_1, \\ldots,0_J}_\\text{quadrature locations}, \\underbrace{1_1, \\ldots ,1_n}_{\\text{data points}} \\right]^\\intercal$\n\n-   $\\mathbf{w} = \\left[ \\underbrace{w_1, \\ldots, w_J}_\\text{quadrature locations}, \\underbrace{0_1, \\ldots, 0_n}_\\text{data points} \\right]^\\intercal$\n\nThen the approximate likelihood can be written as\n\n$$\n\\begin{aligned}\np(\\mathbf{z} | \\lambda) &\\propto \\prod_{i=1}^{J + n} \\eta_i^{z_i} \\exp\\left(-w_i \\eta_i \\right) \\\\\n\\eta_i &= \\log\\lambda(\\mathbf{s}_i) = \\mathbf{x}(s)'\\beta\n\\end{aligned}\n$$\n\nThis is similar to a product of Poisson distributions with means $\\eta_i$, exposures $w_i$ and observations $z_i$.\n\n# Log Gaussian Cox processes\n\nIPP models assume that data points are conditionally independent given the covariates, meaning that any spatial clustering is fully explained by environmental and sampling factors. However, this assumption often fails in practice due to biological processes like dispersal, social aggregation, or unmeasured environmental variables that create spatial dependence. Ignoring such dependencies can lead to misleading conclusions. A more flexible approach to account for spatial dependence is the **log-Gaussian Cox process (LGCP)**, a type of **Cox process** where the intensity function itself follows a Gaussian stochastic process $\\log(\\lambda(s) = Z(s)$ . This allows for a varying intensity across space, leading to an **inhomogeneous Poisson process** with spatially structured variability.\n\nAn LGCP extends the Poisson process by allowing the intensity function to vary spatially as:\n\n$$\n\\log(\\lambda(s))= \\mathbf{x}(s)'\\beta + Z(s)\n$$ {#eq-etaLGCP}\n\nWhere $Z(s)$ is a spatial Gaussian field with mean zero and a covariance function that depends on the distance between observations. The events are then assumed to be independent given the Gaussian field. Moreover, conditional on a realisation of the random field, a log-Gaussian Cox process is an inhomogeneous Poisson process.\n\nAn computationally efficient method to perform estimate LGCP is presented in Simpson et al. ([2016](https://www.semanticscholar.org/paper/Norges-Teknisk-naturvitenskapelige-Universitet-off-Simpson-Illian/4135d57fdb2ab1fa6f1edb388bdb6903b3768403)) This method represents the Gaussian random field using a finite-dimensional continuous formulation with basis functions:\n\n$$\nZ(s) = \\sum_{i=1}^n z_i\\phi_i(s),\n$$ {#eq-basisfx}\n\n$z =(z_1,\\ldots,z_n)'$ is a multivariate Gaussian random vector and $\\{\\phi_i(s)^n_{i=1}\\}$ is a set of linearly independent deterministic basis functions. Unlike lattice-based approaches that bin observations into cells, it models them at their exact locations and approximates the Gaussian random field using a triangulated mesh via the so-called SPDE approach.\n\nRecall that the stochastic partial differential equation (SPDE) approach proposed by Lindgren et al. (2011), represents the continuous spatial process (e.g., the $Z(\\cdot)$ GRF in @eq-etaLGCP ) as a discretely indexed spatial random process such as a Gaussian Markov Random Field (GMRF). The general idea is construct an appropriate lower-resolution approximation of the surface by sampling it in a set of well designed points (integration points) and constructing a piecewise linear interpolant (e.g., the piecewise linear basis functions defined in @eq-basisfx) .\n\n# Example: Forest fires in Castilla-La Mancha\n\nThis dataset is a record of forest fires in the Castilla-La Mancha region of Spain between 1998 and 2007. This region covers approximately 400 by 400 kilometres. The coordinates are recorded in kilometres.\n\n::: callout-note\nThe code and details of this section will be discussed in greater detail on the lecture and on the lab session\n:::\n\nFor more info about the data you can type:\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n?clmfires\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load some libraries\nlibrary(dplyr)\nlibrary(INLA)\nlibrary(ggplot2)\nlibrary(patchwork)\nlibrary(inlabru)     \nlibrary(spatstat)\nlibrary(sf)\nlibrary(scico)\nlibrary(spatstat)\nlibrary(lubridate)\nlibrary(terra)\nlibrary(tidyterra)\n```\n:::\n\n\n\n\n\n\nWe first read the data and transform them into an `sf` object. We also create a polygon that represents the border of the Castilla-La Mancha region. We select the data for year 2004 and only those fires caused by lightning.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(\"clmfires\")\npp = st_as_sf(as.data.frame(clmfires) %>%\n                mutate(x = x,\n                       y = y),\n              coords = c(\"x\",\"y\"),\n              crs = NA) %>%\n  filter(cause == \"lightning\",\n         year(date) == 2004)\n\npoly = as.data.frame(clmfires$window$bdry[[1]]) %>%\n  mutate(ID = 1)\n\nregion = poly %>%\n  st_as_sf(coords = c(\"x\", \"y\"), crs = NA) %>%\n  dplyr::group_by(ID) %>%\n  summarise(geometry = st_combine(geometry)) %>%\n  st_cast(\"POLYGON\")\n\nggplot() + geom_sf(data = region, alpha = 0) + geom_sf(data = pp)\n```\n\n::: {.cell-output-display}\n![Distribution of the observed forest fires caused by lightning in Castilla-La Mancha in 2004](notes_9_files/figure-pdf/fig-points-1.pdf){#fig-points fig-pos='H'}\n:::\n:::\n\n\n\n\n\n\n## Fit a homogeneous Poisson Process {#HPP}\n\nAs a first exercise we are going to fit a homogeneous Poisson process (HPP) to the data. This is a model that assume constant intensity over the whole space so our linear predictor is then:\n\n$$\n\\eta(s) = \\log\\lambda(s) = \\beta_0 , \\ \\mathbf{s}\\in\\Omega\n$$\n\nso the likelihood can be written as:\n\n$$\n\\begin{aligned}\np(\\mathbf{y} | \\lambda)  & \\propto \\exp \\left( -\\int_\\Omega \\lambda(\\mathbf{s}) \\mathrm{d}\\mathbf{s} \\right) \\prod_{i=1}^n \\lambda(\\mathbf{s}_i) \\\\\n& = \\exp\\left( -\\int_{\\Omega}\\exp(\\beta_0)ds\\right)\\prod_{i=1}^n \\lambda(\\mathbf{s}_i) \\\\\n\\end{aligned}\n$$\n\nwhere $|\\Omega|$ is the area of the domain of interest.\n\nWe need to approximate the integral using a numerical integration scheme as:\n\n$$\n\\approx\\exp\\left(-\\sum_{k=1}^{N_k}w_k\\lambda(s_k)\\right)\\prod_{i=1}^n \\lambda(\\mathbf{s}_i)\n$$\n\nWhere $N_k$ is the number of integration points $s_1,\\dots,s_{N_k}$ and $w_1,\\dots,w_{N_k}$ are the integration weights.\n\nIn this case, since the intensity is constant, the integration scheme is really simple: it is enough to consider one random point inside the domain with weight equal to the area of the domain.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# define integration scheme\n\nips <- fm_int_object(\n  st_sample(region, 1), # some random location inside the domain\n  weight = st_area(region), # integration weight is the area of the domain\n  name = \"geometry\"\n)\n\n\ncmp = ~ 0 + beta_0(1)\n\nformula = geometry ~ beta_0\n\nlik = bru_obs(data = pp,\n              family = \"cp\",\n              formula = formula,\n              ips = ips)\nfit1 = bru(cmp, lik)\n```\n:::\n\n\n\n\n\n\nWe can then:\n\n1.  Plot the estimated posterior distribution of the intensity\n2.  Compare the estimated expected number of fires on the whole domain with the observed ones.\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"show\"}\n# 1) The estimated posterior distribution of the  intensity is\n\npost_int = inla.tmarginal(function(x) exp(x), fit1$marginals.fixed$beta_0)\npost_int %>% ggplot() + geom_line(aes(x,y))\n```\n\n::: {.cell-output-display}\n![](notes_9_files/figure-pdf/unnamed-chunk-5-1.pdf){fig-align='center' fig-pos='H'}\n:::\n\n```{.r .cell-code  code-fold=\"show\"}\n# 2) To compute the expected number of points in the area we need to multiply the\n# estimated intensity by the area of the domain.\n# In the same plot we also show the number of observed fires as a vertical line.\n\npost_int = inla.tmarginal(function(x) st_area(region)* exp(x), fit1$marginals.fixed$beta_0)\npost_int %>% ggplot() + geom_line(aes(x,y)) +\n  geom_vline(xintercept = dim(pp)[1])\n```\n\n::: {.cell-output-display}\n![](notes_9_files/figure-pdf/unnamed-chunk-5-2.pdf){fig-align='center' fig-pos='H'}\n:::\n:::\n\n\n\n\n\n\n## Fit an Inhomogeneous Poisson Process {#NHPP}\n\nThe model above has the clear disadvantages that assumes a constant intensity and from @fig-points we clearly see that this is not the case.\n\nThe library `spatstat` contains also some covariates that can help explain the fires distribution. Figure @fit-altitude shows the location of fires together with the (scaled) altitude.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nelev_raster = rast(clmfires.extra[[2]]$elevation)\nelev_raster = scale(elev_raster)\nggplot() +\n  geom_spatraster(data = elev_raster) +\n  geom_sf(data = pp) +\n  scale_fill_scico()\n```\n\n::: {.cell-output-display}\n![Distribution of the observed forest fires and scaled altitude](notes_9_files/figure-pdf/fig-altitude-1.pdf){#fig-altitude fig-pos='H'}\n:::\n:::\n\n\n\n\n\n\nWe are now going to use the altitude as a covariate to explain the variability of the intensity $\\lambda(s)$ over the domain of interest.\n\nOur model is\n\n$$\n\\log\\lambda(s) = \\beta_0 + \\beta_1x(s)\n$$ where $x(s)$ is the altitude at location $s$.\n\nThe likelihood becomes:\n\n$$\n\\begin{aligned}\np(\\mathbf{y} | \\lambda)  & \\propto \\exp \\left( -\\int_\\Omega \\lambda(\\mathbf{s}) \\mathrm{d}\\mathbf{s} \\right) \\prod_{i=1}^n \\lambda(\\mathbf{s}_i) \\\\\n& = \\exp \\left( -\\int_\\Omega \\exp(\\beta_0 + \\beta_1x(s)) \\mathrm{d}\\mathbf{s} \\right) \\prod_{i=1}^n \\lambda(\\mathbf{s}_i) \\\\\n\\end{aligned}\n$$\n\nNow we need to choose an integration scheme to solve the integral.\n\nIn this case we will take a simple grid based approach where each quadrature location has an equal weight. Our grid consists of $N_k = 1000$ points and the weights are all equal to $|\\Omega|/N_k$.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nn.int = 1000\nips <- st_sample(region, size = n.int, type = \"regular\") # May not be exactly n.int points\nips <- fm_int_object(\n  ips,\n  weight = st_area(region) / length(ips),\n  name = \"geometry\"\n)\nggplot() + geom_sf(data = ips, aes(color = weight)) + geom_sf(data= region, alpha = 0)\n```\n\n::: {.cell-output-display}\n![Integration scheme.](notes_9_files/figure-pdf/fig-int2-1.pdf){#fig-int2 fig-pos='H'}\n:::\n:::\n\n\n\n\n\n\n**OBS**: The implicit assumption here is that the intensity is constant inside each grid box, *and so is the covariate*!!\n\nWe can now fit the model:\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncmp = ~ Intercept(1) + elev(elev_raster, model = \"linear\")\nformula = geometry ~ Intercept + elev\nlik = bru_obs(data = pp,\n              family = \"cp\",\n              formula = formula,\n              ips = ips)\nfit2 = bru(cmp, lik)\n```\n:::\n\n\n\n\n\n\n::: {.callout-warning icon=\"false\"}\n## {{< bi pencil-square color=#c8793c >}} Task\n\nWhat is the effect of the altitude on the (log) intensity of the process?\n\n\n<div class='webex-solution'><button>Take hint</button>\n\n\nYou can look at the summary for the fixed effects\n\n\n</div>\n\n\n\n\n\n\n\n::: {.cell layout-align=\"center\" webex.hide='Click here to see the solution'}\n\n<div class='webex-solution'><button>Click here to see the solution</button>\n\n```{.r .cell-code  code-fold=\"show\"}\nfit2$summary.fixed\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                mean         sd 0.025quant   0.5quant 0.975quant       mode kld\nIntercept -6.6426089 0.10465414 -6.8477273 -6.6426089 -6.4374906 -6.6426089   0\nelev       0.7383368 0.07450872  0.5923024  0.7383368  0.8843712  0.7383368   0\n```\n\n\n:::\n\n\n</div>\n:::\n\n\n\n\n\n:::\n\n::: callout-warning\n⚠️ **WARNING!!**⚠️ When fitting a Point process, the integration scheme has to be fine enough to capture the spatial variability of the covariate!!\n:::\n\nNow we want to predict the log-intensity over the whole domain. Use the grid from the elevation raster to predict the intensity over the domain.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nest_grid = st_as_sf(data.frame(crds(elev_raster)), coords = c(\"x\",\"y\"))\nest_grid  = est_grid[region,]\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"show\"}\npreds2 = predict(fit2, est_grid, ~ data.frame(log_scale = Intercept + elev,\n\n                                              lin_scale = exp(Intercept + elev)))\n# then visualize it like\npreds2$log_scale %>%\n  ggplot() +\n  geom_sf(aes(color = mean)) +\n  scale_color_scico()\n```\n\n::: {.cell-output-display}\n![](notes_9_files/figure-pdf/unnamed-chunk-9-1.pdf){fig-align='center' fig-pos='H'}\n:::\n:::\n\n\n\n\n\n\nFinally, we want to use the fitted model to estimate the total number of fires over the whole region. To do this we first have to fine the expected number of fires as:\n\n$$\nE(N_{\\Omega}) = \\int_{\\Omega}\\exp(\\lambda(s))\\ ds\n$$\n\nThen simulate possible realizations of $N_{\\Omega}$ to include also the likelihood variability in our estimate:\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nN_fires = generate(fit2, ips,\n                      formula = ~ {\n                        lambda = sum(weight * exp(elev + Intercept))\n                        rpois(1, lambda)},\n                    n.samples = 2000)\n\nggplot(data = data.frame(N = as.vector(N_fires))) +\n  geom_histogram(aes(x = N),\n                 colour = \"blue\",\n                 alpha = 0.5,\n                 bins = 20) +\n  geom_vline(xintercept = nrow(pp),\n             colour = \"red\") +\n  theme_minimal() +\n  xlab(expression(Lambda))\n```\n\n::: {.cell-output-display}\n![](notes_9_files/figure-pdf/unnamed-chunk-10-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n\n\n## Fit a Log-Gaussian Cox Process {#LGCP}\n\nFinally we want to fit a LGCP with log intensity:\n\n$$\n\\log(s) = \\beta_0 + \\beta_1x + u(s)\n$$\n\nwhere $\\beta_0$ is the intercept, $\\beta_1$ the effect of (standardized) altitude $x(s)$ as before and $u(s)$ is a Gaussian Random field defined through the SPDE approach.\n\n### Define the mesh\n\nThe first step, as any time we use the SPDE approach is to define the mesh and the priors for the marginal variance and range:\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmesh = fm_mesh_2d(boundary = region,\n                  max.edge = c(8, 20),\n                  cutoff = 4, crs = NA)\n\nggplot() + gg(mesh) + geom_sf(data = pp)\n```\n\n::: {.cell-output-display}\n![](notes_9_files/figure-pdf/unnamed-chunk-11-1.pdf){fig-pos='H'}\n:::\n\n```{.r .cell-code}\nspde_model =  inla.spde2.pcmatern(mesh,\n                                  prior.sigma = c(1, 0.5),\n                                  prior.range = c(100, 0.5))\n```\n:::\n\n\n\n\n\n\nWe can then define the integration weight. Here we use the same points to `define the SPDE approximation` and to `approximate the integral` in the likelihood. We will see later that this does not have to be like this, BUT integration weight and SPDE weights have to be consistent with each other!\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nips = fm_int(mesh, samplers = region)\n\nggplot() + geom_sf(data = ips, aes(color = weight)) +\n  gg(mesh) +\n   scale_color_scico()\n```\n\n::: {.cell-output-display}\n![](notes_9_files/figure-pdf/unnamed-chunk-12-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n\n\n### Run the model\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"show\"}\ncmp = ~ Intercept(1) + space(geometry, model = spde_model) + elev(elev_raster, model = \"linear\")\n\nformula = geometry ~ Intercept + space + elev\n\nlik = bru_obs(\"cp\",\n              formula = formula,\n              data = pp,\n              ips = ips)\n\nfit3 = bru(cmp, lik)\n```\n:::\n\n\n\n\n\n\n**Note** when running the model above you will get a warning:\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in bru_log_warn(msg): Model input 'elev_raster' for 'elev' returned some NA values.\nAttempting to fill in spatially by nearest available value.\nTo avoid this basic covariate imputation, supply complete data.\n```\n\n\n:::\n:::\n\n\n\n\n\n\nIt means that the `bru()` function cannot find the covariate values for some of the mesh nodes. This is a common situation. As the warning says, the `bru()` function automatically imputes the value of the covarite using the nearest nodes. This increases the running time of the`bru()` function, so one solution is to impute the values of the covariate over the whole mesh '*before*' running the `bru()` function.\n\nHere, we notice that there are points for which elevation values are missing (see @fig-points2 the red points that lies outside the raster extension ).\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Integration scheme for numerical approximation of the stochastic integral in La Mancha Region](notes_9_files/figure-pdf/fig-points2-1.pdf){#fig-points2 fig-align='center'}\n:::\n:::\n\n\n\n\n\n\nTo solve this, we can increase the raster extension so it covers all both data-points and quadrature locations as well. Then, we can use the `bru_fill_missing()` function to input the missing values with the nearest-available-value. We can achieve this using the following code:\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Extend raster ext by 30 % of the original raster so it covers the whole mesh\nre <- extend(elev_raster, ext(elev_raster)*1.3)\n# Convert to an sf spatial object\nre_df <- re %>% stars::st_as_stars() %>%  st_as_sf(na.rm=F)\n# fill in missing values using the original raster\nre_df$lyr.1 <- bru_fill_missing(elev_raster,re_df,re_df$lyr.1)\n# rasterize\nelev_rast_p <- stars::st_rasterize(re_df) %>% rast()\nggplot() + geom_spatraster(data = elev_rast_p)+\n  geom_sf(data=ips,alpha=0.25,col=1)\n```\n\n::: {.cell-output-display}\n![](notes_9_files/figure-pdf/unnamed-chunk-15-1.pdf){fig-align='center' fig-pos='H'}\n:::\n:::\n\n\n\n\n\n\n::: callout-note\nThe `bru_fill_missing()` function was added mainly to handle very local infilling on domain boundaries. For properly missing data, one should consider doing a proper model of the spatial field instead.\n:::\n\n### Results\n\nWe can plot the estimated mean and standard deviation of the spatial GF and the log-intensity over the domain of interest\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"show\"}\npxl = fm_pixels(mesh, mask= region, dims = c(200,200))\npreds = predict(fit3, pxl, ~data.frame(spde = space,\n                                       log_int = Intercept + space + elev))\n\n#and plot as\nlibrary(scico)\nlibrary(patchwork)\n\nggplot(data= preds$spde) +\n  geom_sf(aes(color = mean)) +\n  scale_color_scico() +\n ggtitle(\"spde mean\") +\nggplot(data=preds$spde ) +\n  geom_sf(aes(color = sd)) +\n  scale_color_scico() +\n ggtitle(\"spde sd\") +\n\nggplot(data=preds$log_int) +\n  geom_sf(aes(color = mean)) +\n  scale_color_scico() +\n ggtitle(\"log-int mean\")\n```\n\n::: {.cell-output-display}\n![](notes_9_files/figure-pdf/unnamed-chunk-16-1.pdf){fig-align='center' fig-pos='H'}\n:::\n\n```{.r .cell-code  code-fold=\"show\"}\nggplot(data=preds$log_int) +\n  geom_sf(aes(color = sd)) +\n  scale_color_scico() +\n ggtitle(\"log-int sd\") +\n  plot_layout(ncol=2)\n```\n\n::: {.cell-output-display}\n![](notes_9_files/figure-pdf/unnamed-chunk-16-2.pdf){fig-align='center' fig-pos='H'}\n:::\n:::\n\n\n\n\n\n\n# Distance Sampling\n\nDistance sampling encompasses a family of related methods for estimating the abundance and spatial distribution of wild populations. The fundamental insight underlying these methods is that animals located further from observers are harder to detect than those that are nearer. This intuitive concept is formalized through a detection function that models the probability of detecting an animal as a function of its distance from the observer. As distance increases, the detection probability correspondingly declines, allowing us to account for animals that are present in the area but remain undetected during surveys.\n\n\n\n\n\n\n::: {.cell}\n\n:::\n\n\n\n\n\n\n\n::: {.content-visible when-format=\"html\"}\n![Animation illustrating the distance sampling concept](images/distance-animation.gif){width=\"80%\"}\n:::\n\n::: {.content-visible when-format=\"PrettyPDF-pdf\"}\n![Animation illustrating the distance sampling concept](images/distance-animation.png){width=\"80%\"}\n:::\n\n\n\n## Density Surface Models\n\nCoupling distance sampling data with spatial modelling enables the production of maps showing spatially varying population density. This represents a significant advance over global abundance estimates, as it reveals how animals distribute themselves across the landscape in relation to environmental features. Historically, density surface modelling has been implemented through a two-stage process. The first stage involves estimating detectability to create an offset vector that accounts for imperfect detection. This offset is then incorporated into a second-stage model—typically a Generalized Linear Model (GLM) or Generalized Additive Model (GAM)—that analyzes count response data. Implementing this approach requires discretizing the continuous survey area into segments and aggregating the detected animals into counts within each spatial unit.\n\n![Schematic of the two-stage density surface modelling approach](images/dsm_approach.png){fig-align=\"center\" width=\"57%\"}\n\nThe discretization process necessary for this approach is illustrated below, showing how continuous survey transects are divided into segments for analysis:\n\n![Discretization of survey data for two-stage modelling](images/discrete_DS.png){fig-align=\"center\" width=\"27%\"}\n\nDespite its widespread use, the two-stage approach suffers from a critical limitation: it fails to properly propagate uncertainty from the first-stage detection model to the second-stage spatial model. The detection function parameters are estimated with associated uncertainty, but this uncertainty is effectively fixed when creating the offset, leading to potentially overconfident predictions and underestimated standard errors in the final spatial maps.\n\n**The solution**\n\nThe goal of contemporary methodological development has been to create a one-stage distance sampling model that simultaneously estimates detectability and the spatial distribution of animals. This unified approach is achieved through a point process framework, which naturally accommodates both the ecological process generating animal locations and the observation process determining which animals are detected.\n\n![The one-stage approach using a point process framework](images/inlabru_DS.png){fig-align=\"center\" width=\"56%\"}\n\nThis integrated framework ensures that all sources of uncertainty are accounted for in a single, coherent analysis, producing more reliable estimates and predictions.\n\n## Thinned Point Processes\n\nThe Log-Gaussian Cox Process (LGCP) provides a flexible and powerful framework for modelling spatial point patterns. It models the intensity of points—representing animal locations—by incorporating the effects of spatial covariates and including a mean-zero spatially structured random effect. This random effect accounts for unexplained heterogeneity in the distribution that is not captured by the measured covariates, making the LGCP particularly suitable for ecological data where many influential factors may be unobserved.\n\nEcological survey data never represents a complete census of all animals present. To account for the imperfect detection inherent in distance sampling, we specify a thinning probability function:\n\n$$\ng(s) = \\mathbb{P}(\\text{a point at s is detected}|\\text{a point is at s})\n$$\n\nThis function represents the probability that an animal at location $s$ is detected by the observer, given that an animal is actually present at that location.\n\nA key and remarkably useful property of the Log-Gaussian Cox Process is that a realization of a point process with intensity $\\lambda(s)$, when thinned by the probability function $g(s)$, itself follows a LGCP. The intensity of this observed, thinned process is simply the product of the true intensity and the thinning probability:\n\n$$\n\\underbrace{\\tilde{\\lambda}(s)}_{\\text{observed process}} = \\underbrace{\\lambda(s)}_{\\text{true process}} \\times \\underbrace{g(s)}_{\\text{thinning probability}}\n$$\n\nThis mathematical formulation allow us to build unified models that simultaneously estimate both the true density of animals and the detection process. The thinning framework then separates the ecological process of interest from the observation process, while maintaining them within a single, coherent statistical model.\n\nStandard distance sampling approaches specify $g(s)$ as a function that declines with increasing distance: - horizontal distance to the observer for *point transects* - perpendicular distance to the transect line for *line transects*\n\n![](images/whale_watch.png){fig-align=\"center\" width=\"398\"}\n\nIn this course we will focus on perpendicular distances only. The thinning probability function is specified as a parametric family of functions:\n\n-   **Half-normal**: $g(\\mathbf{s}|\\sigma) = \\exp(-0.5 (d(\\mathbf{s})/\\sigma)^2)$\n\n-   **Hazard-rate** :$g(\\mathbf{s}|\\sigma) = 1 - \\exp(-(d(\\mathbf{s})/\\sigma)^{-1})$\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](notes_9_files/figure-pdf/unnamed-chunk-18-1.pdf){fig-align='center'}\n:::\n:::\n\n\n\n\n\n\nTo make $g(s)$ and $\\lambda(s)$ identifiable, we assume intensity is constant with respect to distance from the observer. In practice this means we assume animals are uniformly distributed with respect to distance from the line\n\n## Summary of Distance sampling PP approach\n\n1.  The true point pattern $Y = \\mathbf{s}_1,\\ldots,\\mathbf{s}_n$ are a realization of a Point process with intensity $\\lambda(s)$\n\n![](images/thinweeds0-1.png){fig-align=\"center\" width=\"500\"}\n\n2.  We design a sampling survey to collect the data along transects\n\n![](images/thinweeds1-1.png){fig-align=\"center\" width=\"500\"}\n\n3\\. [detected points]{style=\"color:grey;\"} are generated from the thinned PP with intensity $\\color{red}{\\tilde{\\lambda}(s)}= \\lambda(s)\\color{red}{g(d(s))}$ $$ \\therefore \\log \\color{red}{\\tilde{\\lambda}(s)} = \\overbrace{\\log \\lambda (s)}^{\\mathbf{x}'\\beta + \\xi(s)} + \\overbrace{\\log \\color{red}{g(d(s))}}^{-0.5~d(\\mathbf{s})^2\\sigma^{-2}}$$ ![](images/thinweeds2-1.png){fig-align=\"center\" width=\"500\"}\n\n4.  The **encounter rate**, i.e. the number of observed animals within a distance $W$ follows $m \\sim \\text{Poisson} \\left(\\int_0^W \\tilde{\\lambda}(d)\\text{d}d\\right)$\n\n5.  The pdf of detected *distances* is $\\pi(d_1,\\ldots,d_m|m) \\propto \\prod_{i=1}^m\\dfrac{\\tilde{\\lambda}(d_i)}{\\int_0^W \\tilde{\\lambda}(d)\\text{d}d}$ where $\\color{red}{\\tilde{\\lambda}(d_i)} = \\lambda \\color{red}{g(d_i)}$ if we assume constant intensity with respect the distance to the observer.\n\nIf the strips width ( $2W$ ) is narrow compared to study region ($\\Omega$) we can treat them as lines. This mean we need to define the Poisson process likelihood along the kronecker spaces (line $\\times$ distance). Accounting for imperfect detection the thinned Poisson process model on (space, distance) along the transects becomes:\n\n$$\n\\begin{aligned}\n\\log \\tilde{\\lambda}(s,\\text{distance}) &= \\overbrace{\\mathbf{x}'\\beta + \\xi(s)}^{\\log \\lambda(s)} + \\log \\mathbb{P}(\\text{detection at }s|\\text{distance},\\sigma) + \\log(2)\\\\\n\\mathbb{P}(\\text{detection}) &=1-\\exp\\left(-\\frac{\\sigma}{\\text{distance}}\\right)\n\\end{aligned}\n$$\n\n-   Here $\\log 2$ accounts for the two-sided detection.\n\n-   Typically $\\mathbb{P}(distance)$ is a non-linear function, that is where `inlabru` can help via a **Fixed point iteration scheme** (further details available in this [vignette](https://cran.r-project.org/web/packages/inlabru/vignettes/method.html))\n\n-   we define $\\log (\\sigma)$ as a latent Gaussian variable and iteratively linearise it.\n\n# Example: Dolphins in the Gulf of Mexico\n\nIn the next example, we will explore data from a combination of several NOAA shipboard surveys conducted on pan-tropical spotted dolphins in the Gulf of Mexico.\n\n-   A total of 47 observations of groups of dolphins were detected. The group size was recorded, as well as the Beaufort sea state at the time of the observation.\n\n-   Transect width is 16 km, i.e. maximal detection distance 8 km (transect half-width 8 km).\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](notes_9_files/figure-pdf/unnamed-chunk-19-1.pdf)\n:::\n:::\n\n\n\n\n\n\n::: callout-note\nThe code and details of the following section will be discussed in greater detail in the lecture and in the lab session\n:::\n\nFirst, we need to create the mesh used to approximate the random field. We can use a pre-define `sf` boundary and specify this directly into the mesh construction via the `fm_mesh_2d` function.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(fmesher)\n\nmesh_1 = fm_mesh_2d(boundary = mexdolphin$ppoly,\n                    max.edge = c(30, 150),\n                    cutoff = 15,\n                    crs = fm_crs(mexdolphin$points))\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](notes_9_files/figure-pdf/unnamed-chunk-21-1.pdf)\n:::\n:::\n\n\n\n\n\n\nWe use the `inla.spde2.pcmatern` to define the SPDE model using PC priors through the following probability statements\n\n-   $P(\\rho < 50) = 0.1$\n\n-   $P(\\sigma > 2) = 0.1$\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nspde_model =  inla.spde2.pcmatern(\n  mexdolphin$mesh,\n  prior.sigma = c(2, 0.1),\n  prior.range = c(50, 0.1)\n)\n```\n:::\n\n\n\n\n\n\nNowe we need to define the Detection function. We start by plotting the distances and histogram of frequencies in distance intervals.\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](notes_9_files/figure-pdf/unnamed-chunk-23-1.pdf){fig-align='center'}\n:::\n:::\n\n\n\n\n\n\nThen, we need to define a half-normal detection probability function. This must take distance as its first argument and the linear predictor of the sigma parameter as its second:\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# define detection function\nhn <- function(distance, sigma) {\n  exp(-0.5 * (distance / sigma)^2)\n}\n```\n:::\n\n\n\n\n\n\n::: panel-tabset\n## The LGCP Model\n\n$$\n\\begin{aligned}\np(\\mathbf{y} | \\lambda)  & \\propto \\exp \\left( -\\int_\\Omega \\lambda(\\mathbf{s}) p(\\mathbf{s}) \\mathrm{d}\\mathbf{s} \\right) \\prod_{i=1}^n \\lambda(\\mathbf{s}_i) p(\\mathbf{s}_i)) \\\\\n\\eta(s) &  = \\beta_0+  \\omega(s) +  \\log p(s) \\\\\n\\end{aligned}\n$$\n\n## The code\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# build integration scheme\ndistance_domain <-  fm_mesh_1d(seq(0, 8,\n                              length.out = 30))\nips = fm_int(list(geometry = mesh,\n                  distance = distance_domain),\n             samplers = mexdolphin$samplers)\n\n# define model component\ncmp = ~ Intercept(1) +\n  space(main = geometry, model = spde_model) +\n  sigma(1,\n    prec.linear = 1,\n    marginal = bru_mapper_marginal(qexp, pexp, dexp, rate = 1 / 8)\n  )\n\n# define model predictor\neta  = geometry + distance ~ space +\n  log(hn(distance, sigma)) +\n  Intercept + log(2)\n\n# build the observation model\nlik = bru_obs(\"cp\",\n              formula = eta,\n              data = mexdolphin$points,\n              ips = ips)\n# fit the model\nfit = bru(cmp, lik)\n```\n:::\n\n\n\n\n\n:::\n\n## Results\n\n**posterior summaries**\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\\begin{table}\n\\fontsize{12.0pt}{14.0pt}\\selectfont\n\\begin{tabular*}{\\linewidth}{@{\\extracolsep{\\fill}}l|rrr}\n\\toprule\n & mean & 0.025quant & 0.975quant \\\\ \n\\midrule\\addlinespace[2.5pt]\nIntercept & -8.41 & -9.47 & -7.62 \\\\ \nsigma & -0.05 & -0.46 & 0.36 \\\\ \nRange for space & 131.74 & 41.79 & 320.28 \\\\ \nStdev for space & 1.17 & 0.72 & 1.78 \\\\ \n\\bottomrule\n\\end{tabular*}\n\\end{table}\n\n:::\n:::\n\n\n\n\n\n\nWe can also to plot the posterior density of the Matérn field parameters\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nspde.posterior(fit, \"space\", what = \"range\") %>% plot()\n```\n\n::: {.cell-output-display}\n![](notes_9_files/figure-pdf/unnamed-chunk-27-1.pdf){fig-pos='H'}\n:::\n\n```{.r .cell-code}\nspde.posterior(fit, \"space\", what = \"log.variance\") %>% plot()\n```\n\n::: {.cell-output-display}\n![](notes_9_files/figure-pdf/unnamed-chunk-27-2.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n\n\nTo map the spatial intensity we first need to define a grid of points where we want to predict.\n\n-   We do this using the function `fm_pixel()` which creates a regular grid of points covering the mesh\n-   Then, we use the `predict` function which takes as input\n    -   the fitted model (`fit`)\n    -   the prediction points (`pxl`)\n    -   the model components we want to predict (e.g., $e^{\\beta_0 + \\xi(s)}$)\n-   To plot this you can use `ggplot` and add a `gg()` layer with your output of interest (E.g., `pr.int$spatial`)\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(patchwork)\npxl <- fm_pixels(mesh, dims = c(200, 100), mask = mexdolphin$ppoly)\npr.int <- predict(fit, pxl, ~ data.frame(spatial = space,\n                                      loglambda = Intercept + space,\n                                      lambda = exp(Intercept + space)))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot() +\n  gg(pr.int$spatial, geom = \"tile\")\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](notes_9_files/figure-pdf/unnamed-chunk-30-1.pdf)\n:::\n:::\n\n\n\n\n\n\nWe can also use the `predict` function to predict the detection probabilities:\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndistdf <- data.frame(distance = seq(0, 8, length.out = 100))\ndfun <- predict(fit, distdf, ~ hn(distance, sigma))\nplot(dfun)\n```\n\n::: {.cell-output-display}\n![](notes_9_files/figure-pdf/unnamed-chunk-31-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n\n\n**Data level prediction\\* {.smaller}**\n\n47 groups were seen. How many would be seen along the transects under perfect detection?\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredpts_transect <- fm_int(mexdolphin$mesh, mexdolphin$samplers)\nLambda_transect <- predict(fit,\n                           predpts_transect,~ 16 * sum(weight * exp(space + Intercept)))\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n\\begin{table}\n\\fontsize{12.0pt}{14.0pt}\\selectfont\n\\begin{tabular*}{\\linewidth}{@{\\extracolsep{\\fill}}rrrrrrrr}\n\\toprule\nmean & sd & q0.025 & q0.5 & q0.975 & median & sd.mc\\_std\\_err & mean.mc\\_std\\_err \\\\ \n\\midrule\\addlinespace[2.5pt]\n93.34 & 27.23 & 51.08 & 87.78 & 153.90 & 87.78 & 3.69 & 3.46 \\\\ \n\\bottomrule\n\\end{tabular*}\n\\end{table}\n\n:::\n:::\n\n\n\n\n\n\nHow many would be seen under perfect detection across the whole study area (i.e., the **mean** expected number of dolphins)?\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredpts <- fm_int(mexdolphin$mesh, mexdolphin$ppoly)\nLambda <- predict(fit, predpts, ~ sum(weight * exp(space + Intercept)))\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n\\begin{table}\n\\fontsize{12.0pt}{14.0pt}\\selectfont\n\\begin{tabular*}{\\linewidth}{@{\\extracolsep{\\fill}}rrrrrrrr}\n\\toprule\nmean & sd & q0.025 & q0.5 & q0.975 & median & sd.mc\\_std\\_err & mean.mc\\_std\\_err \\\\ \n\\midrule\\addlinespace[2.5pt]\n330.57 & 106.82 & 187.42 & 312.41 & 583.07 & 312.41 & 8.57 & 12.40 \\\\ \n\\bottomrule\n\\end{tabular*}\n\\end{table}\n\n:::\n:::\n\n\n\n\n\n\n**expected counts**\n\nWhat’s the predictive distribution of group counts?\n\nWe can get Monte Carlo samples for the expected number of dolphins as follows:\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nNs <- seq(50, 450, by = 1)\n\nNest <- predict(fit, predpts,\n  ~ data.frame(\n    N = Ns,\n    density = dpois(\n      Ns,\n      lambda = sum(weight * exp(space + Intercept))\n    )\n  ),\n  n.samples = 2000\n)\n\nNest <- dplyr::bind_rows(\n  cbind(Nest, Method = \"Posterior\"),\n  data.frame(\n    N = Nest$N,\n    mean = dpois(Nest$N, lambda = Lambda$mean),\n    mean.mc_std_err = 0,\n    Method = \"Plugin\"\n  )\n)\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](notes_9_files/figure-pdf/unnamed-chunk-37-1.pdf)\n:::\n:::\n\n\n\n\n\n\n# Summary of points\n\n-   Point process are a stochastic processes that describe the locations where events occur\n\n-   Unlike geostatistical data where the locations are fixed, here the locations have a stochastic nature *the locations are our data*!\n\n-   CSR as a realisation of an HPP that describe events that occur independently and uniformly at random across space, such that the number of events in any region follows a Poisson distribution with mean $\\lambda \\times \\text{area}$.\n\n-   K functions can be used to distinguish between CSR, spatial clustering or regular point patterns.\n\n-   IPP allows the intensity of the point process to vary across space through spatially varying covariates.\n\n-   Numerical integration schemes are required to estimate the parameters of an IPP\n\n-   LGCP are a double stochastic process that extend IPP models by allowing the intensity function to vary spatially according to a structured spatial random effect\n\n-   Thinned Point Processes offer improved accuracy by accounting the observational process of how individuals are detected\n",
    "supporting": [
      "notes_9_files\\figure-pdf"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "\\usepackage{booktabs}\n\\usepackage{caption}\n\\usepackage{longtable}\n\\usepackage{colortbl}\n\\usepackage{array}\n\\usepackage{anyfontsize}\n\\usepackage{multirow}\n"
      ]
    },
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}