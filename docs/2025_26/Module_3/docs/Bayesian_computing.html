<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.42">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Introduction to the Integrated Laplace Approximation – Module 3</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-2f5df379a58b258e96c21c0638c20c03.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-dark-b53751a350365c71b6c909e95f209ed1.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-d976503a77e3cc1e05540cf55dea42ea.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="site_libs/bootstrap/bootstrap-dark-4127622b2b65deacbfc6fc4bd5f4cca7.min.css" rel="prefetch" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="styles.css">
<link rel="stylesheet" href="include/webex.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="./index.html" class="navbar-brand navbar-brand-logo">
    <img src="./UofG.png" alt="" class="navbar-logo">
    </a>
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">Module 3</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-notes" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Notes</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-notes">    
        <li>
    <a class="dropdown-item" href="./notes/notes_7.html">
 <span class="dropdown-text">Week 7 - Areal Modelling</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./notes/notes_8.html">
 <span class="dropdown-text">Week 8 - Geostatistical Modelling</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./notes/notes_9.html">
 <span class="dropdown-text">Week 9 - Methods for Point referrenced Data</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-slides" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Slides</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-slides">    
        <li>
    <a class="dropdown-item" href="./slides/slides_7.html">
 <span class="dropdown-text">Week 7 - Areal Modelling</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./slides/slides_8.html">
 <span class="dropdown-text">Week 8 - Geostatistical Modelling</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./slides/slides_9.html">
 <span class="dropdown-text">Week 9 - Methods for Point referrenced Data</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-tutorials" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Tutorials</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-tutorials">    
        <li>
    <a class="dropdown-item" href="./notes/tutorial_sheet_4.html">
 <span class="dropdown-text">Tutorial sheet 4</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-labs" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Labs</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-labs">    
        <li>
    <a class="dropdown-item" href="./lab_3.html">
 <span class="dropdown-text">Lab 3</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-supplementary-material" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Supplementary material</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-supplementary-material">    
        <li>
    <a class="dropdown-item" href="./Bayesian_computing.html">
 <span class="dropdown-text">INLA</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item">
    <a class="nav-link" href="./reading_list.html"> 
<span class="menu-text">Reading List</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="./index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#inla" id="toc-inla" class="nav-link active" data-scroll-target="#inla">INLA</a></li>
  <li><a href="#inla-and-latent-gaussian-models" id="toc-inla-and-latent-gaussian-models" class="nav-link" data-scroll-target="#inla-and-latent-gaussian-models">INLA and latent Gaussian models</a>
  <ul class="collapse">
  <li><a href="#definition-gaussian-markov-random-field-gmrf" id="toc-definition-gaussian-markov-random-field-gmrf" class="nav-link" data-scroll-target="#definition-gaussian-markov-random-field-gmrf">Definition: Gaussian Markov Random Field (GMRF)</a></li>
  </ul></li>
  <li><a href="#inla-in-a-nutshell" id="toc-inla-in-a-nutshell" class="nav-link" data-scroll-target="#inla-in-a-nutshell">INLA in a nutshell</a>
  <ul class="collapse">
  <li><a href="#approximating-the-posterior-of-hyperparameters" id="toc-approximating-the-posterior-of-hyperparameters" class="nav-link" data-scroll-target="#approximating-the-posterior-of-hyperparameters"><strong>Approximating the posterior of hyperparameters</strong></a></li>
  <li><a href="#approximating-the-posterior-for-the-latent-field" id="toc-approximating-the-posterior-for-the-latent-field" class="nav-link" data-scroll-target="#approximating-the-posterior-for-the-latent-field"><strong>Approximating the posterior for the latent field</strong></a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Introduction to the Integrated Laplace Approximation</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<!-- # Monte Carlo Integration -->
<!-- We want to approximate an integral of the form -->
<!-- $$ -->
<!-- H = \int h(x)\, f(x)\, dx, -->
<!-- $$ -->
<!-- where $(f(x))$ is a probability density function.\ -->
<!-- This can be written as an expectation: -->
<!-- $$ -->
<!-- H = \mathbb{E}_f[h(X)], \quad X \sim f. -->
<!-- $$ -->
<!-- ------------------------------------------------------------------------ -->
<!-- ## Monte Carlo Idea -->
<!-- If $(X_1, \dots, X_N \sim f(x))$ are independent samples, then by the Law of Large Numbers, -->
<!-- $$ -->
<!-- \hat H_N = \frac{1}{N} \sum_{i=1}^N h(X_i) -->
<!-- \;\xrightarrow{N \to \infty}\; -->
<!-- H. -->
<!-- $$ -->
<!-- This suggests a simple algorithm: -->
<!-- 1.  Sample $(X_1, \dots, X_N \sim f(x))$ -->
<!-- 2.  Compute $(h(X_i))$ -->
<!-- 3.  Average the results -->
<!-- ------------------------------------------------------------------------ -->
<!-- ## Toy Example -->
<!-- Let -->
<!-- -   $(f(x))$ be the standard normal density $(N(0,1))$ -->
<!-- -   $(h(x) = x^2)$ -->
<!-- The true value is known analytically: -->
<!-- $$ -->
<!-- H = \mathbb{E}[X^2] = 1. -->
<!-- $$ -->
<!-- ::: callout-note -->
<!-- ## proof -->
<!-- $$ -->
<!-- \mathbb{E}[X^2] -->
<!-- = \operatorname{Var}(X) + (\mathbb{E}[X])^2. -->
<!-- $$ -->
<!-- If $X \sim \mathcal{N}(0,1)$, then $\mathbb{E}[X] = 0$ and $\operatorname{Var}(X) = 1$. Hence, -->
<!-- $$ -->
<!-- \mathbb{E}[X^2] = 1. -->
<!-- $$ -->
<!-- ::: -->
<!-- ------------------------------------------------------------------------ -->
<!-- ## Monte Carlo Approximation in R -->
<!-- ```{r} -->
<!-- set.seed(123) -->
<!-- N <- 10000 -->
<!-- # Step 1: sample from f(x) -->
<!-- X <- rnorm(N) -->
<!-- # Step 2: evaluate h(x) -->
<!-- hX <- X^2 -->
<!-- # Step 3: Monte Carlo estimator -->
<!-- H_hat <- mean(hX) -->
<!-- H_hat -->
<!-- ``` -->
<!-- # Monte Carlo Methods for Bayesian Inference -->
<!-- Bayesian inference is centered on the posterior distribution of a (univariate) parameter $\theta$, -->
<!-- $$ -->
<!-- p(\theta \mid y) \propto p(y \mid \theta)\, p(\theta), -->
<!-- $$ -->
<!-- where $y$ denotes the observed data. -->
<!-- ------------------------------------------------------------------------ -->
<!-- ## Posterior Expectations -->
<!-- For a given function $h(\cdot)$, the posterior mean is defined as -->
<!-- $$ -->
<!-- \mathbb{E}[h(\theta) \mid y] -->
<!-- = \int_{\Theta} h(\theta)\, p(\theta \mid y)\, d\theta. -->
<!-- $$ -->
<!-- When this integral cannot be evaluated analytically, Monte Carlo (MC) methods can be used for approximation. -->
<!-- ------------------------------------------------------------------------ -->
<!-- ## Monte Carlo Approximation -->
<!-- Assume that we can simulate independent draws $\{\theta^{(1)}, \dots, \theta^{(m)}\}$ from the posterior distribution $p(\theta \mid y)$. Then, -->
<!-- $$ -->
<!-- \widehat{\mathbb{E}}[h(\theta) \mid y] -->
<!-- = \frac{1}{m} \sum_{i=1}^m h\bigl(\theta^{(i)}\bigr) -->
<!-- \;\xrightarrow{m \to \infty}\; -->
<!-- \mathbb{E}[h(\theta) \mid y). -->
<!-- $$ -->
<!-- The function $h(\theta)$ represents the posterior quantity of interest. Different choices of $h$ correspond to different posterior summaries. For example: -->
<!-- -   $h(\theta) = \theta$ gives the posterior mean $\mathbb{E}[\theta \mid y]$ -->
<!-- -   $h(\theta) = \theta^2$ gives the second posterior moment -->
<!-- -   $h(\theta) = \mathbf{1}\{\theta \in \mathcal{A}\}$ gives the posterior probability $\mathbb{P}(\theta \in \mathcal{A} \mid y)$ -->
<!-- Monte Carlo methods approximate these quantities by replacing expectations with sample averages. -->
<!-- For example, if $h(\theta) = \theta^2$, the posterior expectation becomes -->
<!-- $$ -->
<!-- \mathbb{E}[\theta^2 \mid y] -->
<!-- = \int \theta^2\, p(\theta \mid y)\, d\theta, -->
<!-- $$ -->
<!-- which can be approximated using Monte Carlo samples $\{\theta^{(1)}, \dots, \theta^{(m)}\}$ as -->
<!-- $$ -->
<!-- \frac{1}{m} \sum_{i=1}^m (\theta^{(i)})^2. -->
<!-- $$ -->
<!-- More generally, since the empirical distribution of the MC sample approximates $p(\theta \mid y)$, posterior probabilities can be estimated. -->
<!-- For a set $\mathcal{A} \subset \Theta$, -->
<!-- $$ -->
<!-- \mathbb{P}(\theta \in \mathcal{A} \mid y) -->
<!-- = \int_{\mathcal{A}} p(\theta \mid y)\, d\theta -->
<!-- \approx -->
<!-- \frac{1}{m} -->
<!-- \sum_{i=1}^m \mathbf{1}\{\theta^{(i)} \in \mathcal{A}\}, -->
<!-- $$ -->
<!-- where $\mathbf{1}\{\cdot\}$ is the indicator function. -->
<!-- ------------------------------------------------------------------------ -->
<!-- ## Practical Remark -->
<!-- Monte Carlo methods require the ability to simulate independent samples from the posterior distribution. This is straightforward when $p(\theta \mid y)$ has a known form; otherwise, more advanced methods (e.g., MCMC) are required. -->
<!-- ## A Toy Bayesian Monte Carlo Example -->
<!-- We illustrate Monte Carlo methods for Bayesian inference using a simple conjugate model (se we know the exact form of the posterior), and we make explicit how different choices of the function $h(\theta)$ correspond to different posterior quantities of interest. -->
<!-- Consider the following simple Bayesian model: -->
<!-- $$ -->
<!-- y_1, \dots, y_n \mid \theta \;\sim\; \mathcal{N}(\theta, \sigma^2), -->
<!-- \qquad \sigma^2 \text{ known}, -->
<!-- $$ -->
<!-- with prior -->
<!-- $$ -->
<!-- \theta \sim \mathcal{N}(\mu_0, \tau_0^2). -->
<!-- $$ -->
<!-- By conjugacy, the posterior distribution of $\theta$ is also normal, -->
<!-- $$ -->
<!-- \theta \mid y \sim \mathcal{N}(\mu_n, \tau_n^2), -->
<!-- $$ -->
<!-- where -->
<!-- $$ -->
<!-- \tau_n^2 -->
<!-- = \left( \frac{n}{\sigma^2} + \frac{1}{\tau_0^2} \right)^{-1}, -->
<!-- \qquad -->
<!-- \mu_n -->
<!-- = \tau_n^2 \left( \frac{n \bar y}{\sigma^2} -->
<!-- + \frac{\mu_0}{\tau_0^2} \right). -->
<!-- $$ -->
<!-- For any function $h(\theta)$, the posterior mean is -->
<!-- $$ -->
<!-- \mathbb{E}[h(\theta) \mid y] -->
<!-- = \int h(\theta)\, p(\theta \mid y)\, d\theta. -->
<!-- $$ -->
<!-- Even though this expectation is available in closed form for many choices of $h$, we use Monte Carlo methods for illustration. -->
<!-- **Monte Carlo Approximation** -->
<!-- Draw independent samples from the posterior distribution: -->
<!-- ```{r} -->
<!-- m <- 10000 -->
<!-- # observed data (toy values) -->
<!-- y <- c(1.2, 0.9, 1.4, 1.1, 1.0) -->
<!-- n <- length(y) -->
<!-- # known variance and prior parameters -->
<!-- sigma2 <- 1 -->
<!-- mu0 <- 0 -->
<!-- tau02 <- 1 -->
<!-- # posterior parameters -->
<!-- tau_n2 <- 1 / (n / sigma2 + 1 / tau02) -->
<!-- mu_n <- tau_n2 * (n * mean(y) / sigma2 + mu0 / tau02) -->
<!-- # Monte Carlo sample from the posterior -->
<!-- theta_mc <- rnorm(m, mean = mu_n, sd = sqrt(tau_n2)) -->
<!-- ``` -->
<!-- Posterior mean of $\theta$ -->
<!-- ```{r} -->
<!-- mean(theta_mc) -->
<!-- ``` -->
<!-- Posterior probability $\mathbb{P}(\theta > 0 \mid y)$ -->
<!-- ```{r} -->
<!-- mean(theta_mc > 0) -->
<!-- ``` -->
<!-- We can compare the distribution of our MC samples, the posterior mean and th Exact posterior density. -->
<!-- ```{r} -->
<!-- # Histogram of Monte Carlo samples -->
<!-- hist(theta_mc, breaks = 50, probability = TRUE, -->
<!--      col = "lightblue", border = "black", -->
<!--      main = "Monte Carlo Posterior Samples vs Exact Posterior", -->
<!--      xlab = expression(theta), -->
<!--      ylab = "Density") -->
<!-- # Overlay exact posterior using curve() -->
<!-- curve(dnorm(x, mean = mu_n, sd = sqrt(tau_n2)), -->
<!--       from = min(theta_mc) - 0.5, to = max(theta_mc) + 0.5, -->
<!--       col = "red", lwd = 2, add = TRUE) -->
<!-- # Add posterior mean as a vertical dashed line -->
<!-- abline(v = mu_n, col = "darkgreen", lwd = 2, lty = 2) -->
<!-- # Add legend -->
<!-- legend("topright", legend = c("MC Samples", "Exact Posterior", "Posterior Mean"), -->
<!--        fill = c("lightblue", NA, NA), -->
<!--        border = c("black", NA, NA), -->
<!--        lty = c(NA, 1, 2), col = c(NA, "red", "darkgreen"), lwd = c(NA, 2, 2)) -->
<!-- ``` -->
<!-- ## **A slightly more complex example** -->
<!-- So far we have only considered the case when the likelihood depends only on a single generic parameter (in the last example $\sigma^2$ was assumed known). What happens if we are interested in the joint distribution of two or more parameters? Lets take as an example where observations arise from a Normal($\mu,\sigma^2$) density with unknown mean and variance. In this case $\theta =\{\mu,\sigma^2\}$, lets use MC methods to simulate from the joint posterior distribution expressed as the product of a conditional and a marginal distribution: -->
<!-- $$ -->
<!-- p(\mu,\sigma^2\mid \mathbf{y}) = p(\mu\mid\sigma^2,\mathbf{y})p(\sigma^2\mid \mathbf{y}) -->
<!-- $$ -->
<!-- A common choice is a **noninformative prior**: -->
<!-- $$ -->
<!-- p(\mu, \sigma^2) \propto \frac{1}{\sigma^2} -->
<!-- $$ -->
<!-- Under this prior, the **posterior distributions** are known: -->
<!-- -   The conditional probability of $\mu\mid \sigma^2,\mathbf{y} \sim N(\bar{y},\sigma^2/n)$ -->
<!-- -   The marginal of $\sigma^2$ is computed by integrating out $\mu$ from the joint density, i.e., $p(\sigma^2\mid\mathbf{y})=\int_{-\infty}^\infty p(\mu,\sigma^2\mid\mathbf{y})\mathrm{d}\mu \propto \mathrm{invChi}(n-1,s^2)$ -->
<!--     -   This is a scaled inverse Chi-square distribution with $(n − 1)$ degrees of freedom and scale parameter given by the sample variance. -->
<!-- Lets simulate some Gaussian data with some (unknown) mean and variance that we will try to estimate using MC samples: -->
<!-- ```{r} -->
<!-- set.seed(44566) -->
<!-- mu <- 10 -->
<!-- sigma2 <- 1.5 -->
<!-- n <- 100 -->
<!-- y <- rnorm(n=n, mean=mu, sd=sqrt(sigma2)) -->
<!-- ``` -->
<!-- The MC algorithm goes as follows: -->
<!-- 1.  Select the number of MC samples we will draw -->
<!--     ```{r} -->
<!--      m <- 1000 -->
<!--     ``` -->
<!-- <!-- -->
<p>–&gt;</p>
<!-- 2.  sample the value $\sigma^2$ from the marginal posterior distribution $\sigma^2\mid\mathbf{y}$: -->
<!--     ```{r} -->
<!--     sigma2.sim <- (n-1)*var(y)/rchisq(n=m,df=n-1) -->
<!--     ``` -->
<!-- 3.  sample $\mu$ from the conditional posterior density $\mu\mid \sigma^2,\mathbf{y}$: -->
<!--     ```{r} -->
<!--     mu.sim <-rnorm(n=m, mean=mean(y), sd=sqrt(sigma2.sim/n)) -->
<!--     ``` -->
<!-- 4.  Compute the posterior mean for $\mu$ and $\sigma^2$ samples which should be close to the true values used in the simulation: -->
<!--     ```{r} -->
<!--     mean(mu.sim) -->
<!--     mean(sigma2.sim) -->
<!--     ``` -->
<!-- # Markov chain Monte Carlo methods -->
<!-- In practice, drawing Monte Carlo samples from the posterior distribution can be difficult for several reasons. For example, the dimension of the parameter vector $\theta$ may be very high, or the posterior distribution may be nonstandard or have no closed-form expression, which makes direct sampling challenging. -->
<!-- Instead, we can generate a sample by running a **Markov chain**, which is a sequence of dependent random variables designed so that its **long-run behavior matches the posterior distribution**. After running the chain for enough steps, the resulting sample can be used just like independent Monte Carlo samples to estimate posterior quantities such as means, quantiles, or probabilities. -->
<!-- A **Markov chain** is a sequence of random variables $\{X^{(0)}, X^{(1)}, \dots, X^{(t)}, \dots\}$, each taking values in some set called the **state space** $\chi$. What makes it special is the **Markov property**: the conditional distribution of the next value $X^{(t)}$ depends only on the current value $X^{(t-1)}$ and not on all the previous values $(X^{(0)}, \dots, X^{(t-2)})$. Mathematically, we write this as -->
<!-- $$ -->
<!-- p(X^{(t)} \mid X^{(0)}, X^{(1)}, \dots, X^{(t-1)}) = p(X^{(t)} \mid X^{(t-1)}), -->
<!-- $$ -->
<!-- where $p(X^{(t)} \mid X^{(t-1)})$ is called the **transition probability**, describing how the chain moves from one state to the next. -->
<!-- In **Markov Chain Monte Carlo (MCMC)**, the idea is to construct a Markov chain whose **stationary distribution** is exactly the distribution we want to sample from. -->
<!-- Stationarity means that once the chain has reached this distribution, the random variables $X^{(t)}$ produced by the chain will have the target distribution regardless of $t$: if $X^{(t)} \sim \pi$, then $X^{(t+1)} \sim \pi$ as well. In other words, the distribution "*stabilizes*" and does not change over time. -->
<!-- For a unique stationary distribution to exist, the chain must satisfy some basic properties: -->
<!-- 1.  **Irreducibility**: starting from any value $X^{(0)}$, the chain has a positive probability of eventually reaching any region of the state space $\chi$.\ -->
<!-- 2.  **Recurrence**: the chain is guaranteed to return to regions of interest infinitely often (formally, the expected number of returns to a set $\mathcal{A} \subset \chi$ is infinite).\ -->
<!-- 3.  **Aperiodicity**: the chain does not get stuck in cycles; it can move through the state space without being forced into a repeating pattern. -->
<!-- When these conditions hold, the stationary distribution $\pi$ is also the **limiting distribution**, meaning that as $t \to \infty$, the distribution of $X^{(t)}$ converges to $\pi$, regardless of where the chain started. -->
<!-- This is the fundamental principle behind MCMC: even though each individual sample $X^{(t)}$ depends on the previous one, after running the chain long enough we can treat the collected values as if they were drawn from the target distribution. -->
<!-- In Bayesian inference, the **target distribution** $\pi$ is usually the posterior distribution $p(\theta \mid y)$. Once a Markov chain has run long enough and reached its **stationary (or invariant) distribution**, the sequence of values ${\theta^{(1)}, \theta^{(2)}, \dots}$ can be treated as an approximate sample from the posterior. These samples can then be used in the same way as standard Monte Carlo draws to compute posterior summaries, such as means, quantiles, variances, or probabilities of events. -->
<!-- ## Gibbs sampler -->
<!-- The **Gibbs sampler** is a special type of MCMC algorithm that is very useful when the posterior distribution is **multivariate** (i.e., when $\theta$ is a vector) and difficult to sample from directly. -->
<!-- The key idea is simple: instead of trying to sample from the full joint posterior $p(\theta_1, \theta_2, \dots, \theta_d \mid y)$ at once, we **sample each component** $\theta_j$ conditionally on the current values of all the other components. -->
<!-- Suppose $\theta = (\theta_1, \theta_2, \dots, \theta_d)$. Starting from an initial value $(\theta_1^{(0)}, \dots, \theta_d^{(0)})$, the Gibbs sampler generates a sequence of samples as follows: -->
<!-- 1.  Sample $\theta_1^{(t+1)} \sim p(\theta_1 \mid \theta_2^{(t)}, \dots, \theta_d^{(t)}, y)$\ -->
<!-- 2.  Sample $\theta_2^{(t+1)} \sim p(\theta_2 \mid \theta_1^{(t+1)}, \theta_3^{(t)}, \dots, \theta_d^{(t)}, y)$\ -->
<!-- 3.  $\dots$\ -->
<!-- 4.  Sample $\theta_d^{(t+1)} \sim p(\theta_d \mid \theta_1^{(t+1)}, \dots, \theta_{d-1}^{(t+1)}, y)$ -->
<!-- After completing all $d$ steps, we have one new vector $(\theta_1^{(t+1)}, \dots, \theta_d^{(t+1)})$ of the chain. Repeating this process for many iterations generates a sample from the joint posterior. -->
<!-- A key requirement of the Gibbs sampler is that we must **know how to sample from the full conditional distributions** of each parameter. In some models, these conditionals are easy to identify (as in the Normal $(\mu, \sigma^2)$ example), but in more complex models, the full conditionals may be **difficult or impossible to sample from directly**. -->
<!-- This limitation motivates the use of more general MCMC algorithms, such as **Metropolis-Hastings (MH)**. The MH algorithm does not require that we can sample directly from the full conditionals. Instead, it allows us to propose candidate values for parameters and accept or reject them in a way that ensures the Markov chain converges to the target posterior distribution. -->
<!-- In other words, MH **extends the Gibbs sampler** to situations where full conditionals are not available in closed form, making MCMC applicable to a much wider class of Bayesian models. -->
<!-- ## Metropolis algorithm -->
<!-- The Metropolis algorithm was first proposed by Metropolis *et al*. (1953) and then generalized by Hastings (1970). Consider the case of a single generic parameter $\theta$. Starting from an initial value $\theta^{(0)}$, the $t$-th iteration of the MH algorithm proceeds as follows: -->
<!-- 1.  **Propose a candidate value** $\theta^*$ from a proposal distribution $q(\theta^* \mid \theta^{(t)})$. -->
<!--     -   This distribution can be anything convenient (e.g., Normal centered at the current value $\theta^{(t)}$). -->
<!--     -   The choice of $q$ affects the efficiency of the algorithm but not the correctness of the stationary distribution. -->
<!-- 2.  Compute the **probability** **ratio**: -->
<!--     $$ -->
<!--     r= \dfrac{p(\theta^*\mid y)}{p(\theta^{t-1}\mid y)}=\dfrac{p(y\mid \theta^*)p(\theta^*)}{p(y\mid \theta^{t-1})p(\theta^{t-1})}, -->
<!--     $$ -->
<!-- 3.  **Accept or reject the candidate** with probability $r$ -->
<!--     $$ -->
<!--     \theta^{(t)} =\begin{cases}\theta^* & \text{with probability } \min(r,1), \\\theta^{(t-1)} & \text{with probability } 1 - \min(r,1).\end{cases} -->
<!--     $$ -->
<!--     In practice: -->
<!--     -   Generate a uniform random number $u \sim \text{Uniform}(0,1)$ and set $\theta^{(t)} = \theta^*$. -->
<!--     -   If $u < r$, accept the candidate $\theta^*$ or reject it if $u \geq r$ and keep the previous value $\theta^{(t-1)}$. -->
<!--     -   **Repeat** steps 1–3 for many iterations to generate a Markov chain $\{\theta^{(0)}, \theta^{(1)}, \theta^{(2)}, \dots\}$. -->
<!-- After sufficient iterations, the distribution of $\theta^{(t)}$ converges to the **target distribution** $\pi(\theta)$. Once the chain has converged, the samples can be used to compute posterior summaries such as means, variances, quantiles, or probabilities of events, just like in standard Monte Carlo. -->
<!-- Usually, a symmetric density is preferred, such that $q(\theta^* \mid \theta^{(t)}) = q(\theta^{(t)} \mid \theta^*)$. In this case, a possible choice is the Uniform or the Gaussian distribution centered around the previous value (e.g, $q(\theta^* \mid \theta^{(t-1)})\sim N(\theta^{(t-1)},\gamma^2)$, where $\gamma$ is a tuning parameter chosen appropriately with respect to the algorithm efficiency). -->
<!-- ### Example: -->
<!-- Suppose we have data $y_1, \dots, y_n$ from the following simple linear regression model -->
<!-- $$ -->
<!-- \begin{aligned} -->
<!-- y_i &\sim \text{Normal}(\mu_i, \sigma^2), \quad i = 1, \dots, n\\ -->
<!-- \mu_i &= \beta_0 + \beta_1 x_i -->
<!-- \end{aligned} -->
<!-- $$ -->
<!-- ```{r} -->
<!-- #| fig-width: 4.5 -->
<!-- #| fig-height: 4.5 -->
<!-- #| fig-align: center -->
<!-- set.seed(123) -->
<!-- # Simulated data -->
<!-- n <- 50 -->
<!-- # intercept -->
<!-- beta0_true <- 2 -->
<!-- # slope -->
<!-- beta1_true <- 4 -->
<!-- # Simulated covariate -->
<!-- x <- rnorm(n, 10, sqrt(5)) -->
<!-- # Liner predictor -->
<!-- mu_true <- beta0_true + beta1_true*x -->
<!-- # random error -->
<!-- sigma_true <- 2 -->
<!-- y <- rnorm(n, mean = mu_true, sd = sigma_true) -->
<!-- plot(x,y) -->
<!-- ``` -->
<!-- Lets explore how to implement **Metropolis algorithm** to sample from $\theta= (\beta_0,\beta_1, \sigma^2)$. -->
<!-- 1.  Algorithm settings: -->
<!--     ```{r} -->
<!--     # number of iterations -->
<!--     n_iter <- 50000 -->
<!--     # empty vector to store samples for the parameters -->
<!--     beta0 <- rep(NA, n_iter) -->
<!--     beta1 <- rep(NA, n_iter) -->
<!--     sigma <- rep(NA, n_iter) -->
<!--     # Set starting values -->
<!--     beta0[1] <- 1 -->
<!--     beta1[1] <- 2 -->
<!--     sigma[1] <- 1 -->
<!--     # Burnin period  (disregard first samples) -->
<!--     burn <- 5000 -->
<!--     ``` -->
<!-- 2.  Define the log-likelihood function $\sum_i^n \log p(y_i\mid \mu_i,\sigma^2)$: -->
<!--     ```{r} -->
<!--     log_likelihood = function(theta){ -->
<!--         beta_0 = theta[1] # intercept -->
<!--         beta_1 = theta[2] # slope -->
<!--         sd = theta[3]     # observaitonal error -->
<!--         # linear predictor -->
<!--         pred = beta_0 + beta_1*x -->
<!--         # log-Gaussian density -->
<!--         loglik = dnorm(y, mean = pred, sd = sd, log = T) -->
<!--         sumll = sum(loglik) -->
<!--         return(sumll) -->
<!--     } -->
<!--     ``` -->
<!-- 3.  Define the priors for $\theta = \{\beta_0,\beta_1,\sigma\}$. The priors we will use are: -->
<!--     1.  ${\beta_0,\beta_1,\sigma}\sim N(0,25)$ -->
<!--     ```{r} -->
<!--     # Prior distribution -->
<!--     prior = function(theta){ -->
<!--         beta_0 = theta[1] -->
<!--         beta_1 = theta[2] -->
<!--         sd = theta[3] -->
<!--         b0_prior <- dnorm(beta_0, sd=5, log=TRUE) -->
<!--         b1_prior <- dnorm(beta_1, sd=5, log=TRUE) -->
<!--         sd_prior <- dnorm(sd, sd=5, log=TRUE) -->
<!--         return(b0_prior+b1_prior+sd_prior) -->
<!--     } -->
<!--     ``` -->
<!-- 4.  Define the posterior as the product between the likelihood and the prior (in this case the sum because we work with logarithms). -->
<!--     ```{r} -->
<!--     posterior = function(theta){ -->
<!--        return (log_likelihood(theta) + prior(theta)) -->
<!--     } -->
<!--     ``` -->
<!-- 5.  Define our Gaussian proposal function $q(\theta^* \mid \theta^{(t-1)})\sim N(\theta^{(t-1)},\gamma^2)$, here we will use different tuning parameters $\gamma = (0.5,0.1,0.25)$ for each element of $\theta$ -->
<!--     ```{r} -->
<!--     proposalfunction = function(theta){ -->
<!--       # Draw a candidate value from a Gaussian density centred at the previous value of theta^(k) -->
<!--         return(rnorm(3,mean = theta, sd=c(0.5, 0.1, 0.25))) -->
<!--     } -->
<!--     ``` -->
<!-- 6.  We run the MCMC algorithm as follows: -->
<!--     ```{r} -->
<!--     set.seed(123) -->
<!--     for(t in 2:n_iter){ -->
<!--       #  Propose a new candidate for each component -->
<!--       theta_star <- proposalfunction(c(beta0[t-1], beta1[t-1], sigma[t-1])) -->
<!--       # Compute acceptance ratio (use exp because we were working on log scale) -->
<!--       r <- exp(posterior(theta_star) - posterior( -->
<!--         c(beta0[t-1],beta1[t-1], sigma[t-1]))) -->
<!--       # Accept or reject -->
<!--       if(runif(1) < min(1, r)){ -->
<!--         beta0[t] <- theta_star[1] -->
<!--         beta1[t] <- theta_star[2] -->
<!--         sigma[t] <- theta_star[3] -->
<!--       } else { -->
<!--         beta0[t] <- beta0[t-1] -->
<!--         beta1[t] <- beta1[t-1] -->
<!--         sigma[t] <- sigma[t-1] -->
<!--       } -->
<!--     } -->
<!--     ``` -->
<!-- **Visualize Trace Plots** -->
<!-- ```{r} -->
<!-- par(mfrow=c(1,3)) -->
<!-- plot(beta0[burn:n_iter], type='l', main=expression("Trace plot: " * beta[0]), ylab='value', xlab='Iteration') -->
<!-- abline(h=beta0_true, col='red', lty=2) # true value -->
<!-- plot(beta1[burn:n_iter], type='l', main=expression("Trace plot: " * beta[1]), ylab='value', xlab='Iteration') -->
<!-- abline(h=beta1_true, col='red', lty=2) # true value -->
<!-- plot(sigma[burn:n_iter], type='l',main=expression("Trace plot: " * sigma), ylab='value', xlab='Iteration') -->
<!-- abline(h=sigma_true, col='red', lty=2) # true value -->
<!-- ``` -->
<!-- **Visualize Density Plots** -->
<!-- ```{r} -->
<!-- par(mfrow=c(1,3)) -->
<!-- hist(beta0[burn:n_iter], breaks=50, probability=TRUE, col='lightblue', main=expression(beta[0])) -->
<!-- hist(beta1[burn:n_iter], breaks=50, probability=TRUE, col='lightgreen', main=expression(beta[1])) -->
<!-- hist(sigma[burn:n_iter], breaks=50, probability=TRUE, col='lightpink', main=expression(sigma)) -->
<!-- ``` -->
<!-- ## Metropolis-Hastings Algorithm -->
<!-- A central component of the Metropolis algorithm is the **proposal distribution**, which generates candidate parameter values. For each proposed value, we compute the acceptance ratio $r$ and then randomly decide whether to accept or reject it. In this example, we use a **normal proposal distribution** with standard deviations $\gamma =(0.5, 0.1, 0.25)$ for the three parameters. The **Metropolis-Hastings algorithm** generalizes this approach by allowing the proposal distribution to be **asymmetric**. The acceptance ratio compares the posterior densities at the candidate and current parameter values: -->
<!-- $$ -->
<!-- r = \dfrac{p(\theta^*\mid y)}{p(\theta^{t-1}\mid y)} = \dfrac{p(y\mid \theta^*)p(\theta^*)}{p(y\mid \theta^{t-1})p(\theta^{t-1})} -->
<!-- $$ -->
<!-- For symmetric proposals, we accept the candidate $\theta^*$ with probability $\min(1, r)$. However, for asymmetric proposals $q(\theta^* \mid \theta^{t-1})$, we must adjust this ratio to: -->
<!-- $$ -->
<!-- r = \dfrac{p(y\mid \theta^*)p(\theta^*)}{p(y\mid \theta^{t-1})p(\theta^{t-1})} \cdot \dfrac{q(\theta^{t-1} \mid \theta^*)}{q(\theta^* \mid \theta^{t-1})} -->
<!-- $$ -->
<!-- To illustrate the generalization to **Metropolis-Hastings**, we can make the proposal **asymmetric** for some parameters. For example, we can use an **exponential** proposal for $\sigma$ to ensure it remains positive. -->
<!-- We propose $\sigma^* \sim \text{Exp}(1/\sigma^{(t-1)})$, which centers the proposal around the current value but with positive-only support: -->
<!-- ```{r} -->
<!--  proposalfunction_2 <- function(theta){ -->
<!--   beta0_star <- rnorm(1, mean = theta[1], sd = 0.5) -->
<!--   beta1_star <- rnorm(1, mean = theta[2], sd = 0.1) -->
<!--   sigma_star <- rexp(1, rate = 1 / theta[3]) -->
<!--   c(beta0_star, beta1_star, sigma_star) -->
<!-- } -->
<!-- ``` -->
<!-- This asymmetric proposal requires modifying the acceptance ratio to account for the differing proposal densities in forward and reverse directions: -->
<!-- $$ -->
<!-- r = \dfrac{p(y\mid \beta_0^*, \beta_1^*, \sigma^*)p(\beta_0^*, \beta_1^*, \sigma^*)} -->
<!--           {p(y\mid \beta_0^{t-1}, \beta_1^{t-1}, \sigma^{t-1})p(\beta_0^{t-1}, \beta_1^{t-1}, \sigma^{t-1})} -->
<!--      \times -->
<!--      \dfrac{q(\beta_0^{t-1}, \beta_1^{t-1}, \sigma^{t-1} \mid \beta_0^*, \beta_1^*, \sigma^*)} -->
<!--           {q(\beta_0^*, \beta_1^*, \sigma^* \mid \beta_0^{t-1}, \beta_1^{t-1}, \sigma^{t-1})} -->
<!-- $$ -->
<!-- where the proposal density ratio factorizes into its components: -->
<!-- $$ -->
<!-- \dfrac{q(\cdot\mid\cdot)}{q(\cdot\mid\cdot)} =  -->
<!-- \dfrac{\phi(\beta_0^{t-1} \mid \beta_0^*, 0.5) \cdot \phi(\beta_1^{t-1} \mid \beta_1^*, 0.1) \cdot \text{Exp}(\sigma^{t-1} \mid 1/\sigma^*)} -->
<!--      {\phi(\beta_0^* \mid \beta_0^{t-1}, 0.5) \cdot \phi(\beta_1^* \mid \beta_1^{t-1}, 0.1) \cdot \text{Exp}(\sigma^* \mid 1/\sigma^{t-1})} -->
<!-- $$ -->
<!-- Here $\phi(\cdot)$ denotes the Gaussian density. The Gaussian proposals for $\beta_0$ and $\beta_1$ remain symmetric and cancel in the ratio, while the exponential proposal for $\sigma$ requires explicit evaluation of both forward and reverse proposal densities: -->
<!-- ```{r} -->
<!-- proposal_density <- function(theta_proposed, theta_current) { -->
<!--   # For beta0 and beta1: Gaussian proposals -->
<!--   dens_beta0 <- dnorm(theta_proposed[1], mean = theta_current[1], sd = 0.5, log = TRUE) -->
<!--   dens_beta1 <- dnorm(theta_proposed[2], mean = theta_current[2], sd = 0.1, log = TRUE) -->
<!--   # For sigma: Exponential proposal (asymmetric) -->
<!--   dens_sigma <- dexp(theta_proposed[3], rate = 1/theta_current[3], log = TRUE) -->
<!--   return(dens_beta0 + dens_beta1 + dens_sigma) -->
<!-- } -->
<!-- ``` -->
<!-- ```{r} -->
<!-- set.seed(123) -->
<!-- # Set empty vector to store samples for the parameters -->
<!-- beta0 <- rep(NA, n_iter) -->
<!-- beta1 <- rep(NA, n_iter) -->
<!-- sigma <- rep(NA, n_iter) -->
<!-- # Set starting values -->
<!-- beta0[1] <- 1 -->
<!-- beta1[1] <- 2 -->
<!-- sigma[1] <- 1 -->
<!-- for(t in 2:n_iter){ -->
<!--   # Store current theta -->
<!--   theta_current <- c(beta0[t-1], beta1[t-1], sigma[t-1]) -->
<!--   # Propose a new candidate -->
<!--   theta_star <- proposalfunction_2(theta_current) -->
<!--   # Compute acceptance ratio with proposal density correction -->
<!--   log_r <- (posterior(theta_star) - posterior(theta_current)) + -->
<!--            (proposal_density(theta_current, theta_star) -  -->
<!--             proposal_density(theta_star, theta_current)) -->
<!--   # Accept or reject -->
<!--   if(log(runif(1)) < min(0, log_r)){   -->
<!--     beta0[t] <- theta_star[1] -->
<!--     beta1[t] <- theta_star[2] -->
<!--     sigma[t] <- theta_star[3] -->
<!--   } else { -->
<!--     beta0[t] <- beta0[t-1] -->
<!--     beta1[t] <- beta1[t-1] -->
<!--     sigma[t] <- sigma[t-1] -->
<!--   } -->
<!-- } -->
<!-- ``` -->
<!-- **Visualize Trace Plots** -->
<!-- ```{r} -->
<!-- par(mfrow=c(1,3)) -->
<!-- plot(beta0[burn:n_iter], type='l', main=expression("Trace plot: " * beta[0]), ylab='value', xlab='Iteration') -->
<!-- abline(h=beta0_true, col='red', lty=2) # true value -->
<!-- plot(beta1[burn:n_iter], type='l', main=expression("Trace plot: " * beta[1]), ylab='value', xlab='Iteration') -->
<!-- abline(h=beta1_true, col='red', lty=2) # true value -->
<!-- plot(sigma[burn:n_iter], type='l',main=expression("Trace plot: " * sigma), ylab='value', xlab='Iteration') -->
<!-- abline(h=sigma_true, col='red', lty=2) # true value -->
<!-- ``` -->
<!-- **Visualize Density Plots** -->
<!-- ```{r} -->
<!-- par(mfrow=c(1,3)) -->
<!-- hist(beta0[burn:n_iter], breaks=50, probability=TRUE, col='lightblue', main=expression(beta[0])) -->
<!-- hist(beta1[burn:n_iter], breaks=50, probability=TRUE, col='lightgreen', main=expression(beta[1])) -->
<!-- hist(sigma[burn:n_iter], breaks=50, probability=TRUE, col='lightpink', main=expression(sigma)) -->
<!-- ``` -->
<section id="inla" class="level1">
<h1>INLA</h1>
</section>
<section id="inla-and-latent-gaussian-models" class="level1">
<h1>INLA and latent Gaussian models</h1>
<p>The Integrated Nested Laplace Approximation (INLA) is a fast and accurate Bayesian approximation method for the albeit wide class of Latent Gaussian Models (LGMs). This includes GLMs, GLMMs,GAM-like models and a wide range of spatial and spatiotemporal models.</p>
<p>Models of this type can be written a</p>
<p><span class="math display">\[
\begin{aligned}
\mathbf{y} \mid \mathbf{u}, \theta
&amp;\sim \prod_i \pi(y_i \mid \eta_i, \theta), \\[6pt]
\boldsymbol{\eta}
&amp;= A_1 \mathbf{u}_1 + A_2 \mathbf{u}_2 + \dots + A_k \mathbf{u}_k, \\[6pt]
\mathbf{u} \mid \theta
&amp;\sim \mathcal{N}\big(\mathbf{0}, \mathbf{Q}^{-1}(\theta)\big), \\[6pt]
\theta
&amp;\sim \pi(\theta).
\end{aligned}
\]</span></p>
<p>The first step in defining a LGM within the Bayesian framework is to identify a distribution for the observed data <span class="math inline">\(\mathbf{y} = (y_1, \dots, y_n)\)</span> where typically the mean <span class="math inline">\(\mathbb{E}(y_i) = \mu_i\)</span> of each observation <span class="math inline">\(y_i\)</span> is linked to a linear predictor <span class="math inline">\(\eta_i\)</span> through an appropriate link function <span class="math inline">\(g(\cdot)\)</span>. The additive linear predictor <span class="math inline">\(\eta_i\)</span> can be defined by covariates (i.e., fixed effects) and different types of random effects:</p>
<p><span class="math display">\[
\eta_i = \alpha + \sum_{j=1}^{P} \beta_j x_{ij} + \sum_{k=1}^{L} f_{k}(z_{ik})  \qquad i = 1, \dots, n.
\]</span></p>
<p>Here, <span class="math inline">\(\alpha\)</span> is the intercept, <span class="math inline">\(\beta_j\)</span>, <span class="math inline">\(j = 1, \dots, P\)</span>, are coefficients of <span class="math inline">\(P\)</span> covariates, and the functions <span class="math inline">\(f\_{k}(\cot)\)</span> can take different forms such as smooth and nonlinear effects of covariates, time trends and seasonal effects, random intercept and slopes as well as temporal or spatial random effects. We denote the vector of all latent effects as <span class="math inline">\(\mathbf{u}\)</span> and its is assumed to be Gaussian Markov random field (GMRF). This GMRF will have a zero mean and precision matrix <span class="math inline">\(\mathbf{Q}(\theta)\)</span> which consists of sums of the precision matrices of the fixed effects and the other model components. Furthermore, <span class="math inline">\(\theta\)</span> represents a vector of hypeparameters that control the behavior of the latent components and the likelihood of observations (<span class="math inline">\(p~(\mathbf{y} \mid \mathbf{u},~ \theta)\)</span>), which we assume are conditionally independent given the latent field <span class="math inline">\(\mathbf{u}\)</span>.</p>
<p>The core idea behind the INLA approach is that, rather than estimating the joint posterior distribution of the model parameters <span class="math inline">\(p(\mathbf{u},\theta|\mathbf{y})\)</span>, we focus on individual posterior marginals of the model parameters (<span class="math inline">\(p(u_i|\mathbf{y}\)</span> and <span class="math inline">\(p(\theta_j \mid \mathbf{y}\)</span>)). This is achieved thanks to the computational properties of GMRF and the Laplace approximation for multidimensional integration.</p>
<section id="definition-gaussian-markov-random-field-gmrf" class="level2 callout-definition">
<h2 class="anchored" data-anchor-id="definition-gaussian-markov-random-field-gmrf">Definition: Gaussian Markov Random Field (GMRF)</h2>
<p>A random variable <span class="math inline">\(\mathbf{u}\)</span> is said to be a <strong>Gaussian Markov random field (GMRF)</strong> with respect to a graph <span class="math inline">\(G\)</span>, with vertices <span class="math inline">\(\{1,2,\dots,n\}\)</span> and edges <span class="math inline">\(E\)</span>, mean vector <span class="math inline">\(\boldsymbol{\mu}\)</span>, and precision matrix <span class="math inline">\(\mathbf{Q}\)</span>, if its probability distribution is</p>
<p><span class="math display">\[
\pi(\mathbf{u})
=
|\mathbf{Q}|^{1/2}(2\pi)^{-n/2}
\exp\left\{
-\frac{1}{2}
(\mathbf{u}-\boldsymbol{\mu})^{\top}
\mathbf{Q}
(\mathbf{u}-\boldsymbol{\mu})
\right\},
\]</span></p>
<p>and the precision matrix satisfies the Markov property</p>
<p><span class="math display">\[
Q_{ij} \neq 0
\;\Longleftrightarrow\;
\{i,j\} \in E.
\]</span></p>
<p>That is, conditional independence between <span class="math inline">\(u_i\)</span> and <span class="math inline">\(u_j\)</span> corresponds to zeros in the precision matrix.</p>
</section>
</section>
<section id="inla-in-a-nutshell" class="level1">
<h1>INLA in a nutshell</h1>
<p>We are interested in estimating the posterior marginals <span class="math inline">\(p(u_i|\mathbf{y})\)</span>. We could compute this by integrating out all the other components in the model i.e., <span class="math inline">\(p(u_i \mid \mathbf{y}) = \int p(\mathbf{u}\mid \theta) d \mathbf{u}_{-i}\)</span>. However, this might not be computationally efficient as the latent filed <span class="math inline">\(\mathbf{u}\)</span> can be very high dimensional. Instead we can compute the posterior marginals by considering that</p>
<p><span class="math display">\[
p(u_i \mid \mathbf{y}) = \int \color{skyblue}{ p(u_i\mid \theta,\mathbf{y})} ~\color{purple}{p(\theta\mid \mathbf{y})} d \theta
\]</span> This possible because the size of <span class="math inline">\(\theta\)</span> is typically small and thus, easier to integrate (this is not a too restrictive assumption since most likelihoods and latent effects will typically depend on a small number of hyperparemeters). This means that, in order to compute the marginal posteriors <span class="math inline">\(p(u_i \mid \mathbf{y})\)</span>, we need to approximate:</p>
<ol type="1">
<li>The posterior of hyperparameters <span class="math inline">\(\color{skyblue}{p(\theta \mid \mathbf{y})}\)</span></li>
<li>The posterior of the latent field <span class="math inline">\(\color{purple}{p(u_i\mid \theta,\mathbf{y})}\)</span></li>
</ol>
<section id="approximating-the-posterior-of-hyperparameters" class="level2">
<h2 class="anchored" data-anchor-id="approximating-the-posterior-of-hyperparameters"><strong>Approximating the posterior of hyperparameters</strong></h2>
<p>To build an approximation to <span class="math inline">\(p(\theta \mid \mathbf{y})\)</span> we can use definition of conditional probability where, conditional on the observations <span class="math inline">\(\mathbf{y}\)</span></p>
<p><span class="math display">\[
\begin{aligned}
p(\mathbf{u}~,\theta\mid \mathbf{y}) &amp;= p(\mathbf{u}\mid\mathbf{y},\theta)~p(\theta\mid \mathbf{y}) \\
\Rightarrow p(\theta \mid \mathbf{y}) &amp;= \dfrac{\color{tomato}{p(\mathbf{u}~,\theta \mid \mathbf{y})}}{p(\mathbf{u} \mid \mathbf{y},\theta)}
\end{aligned}
\]</span></p>
<p>Applying Bayes rules we get that <span class="math inline">\(\color{tomato}{p(\mathbf{u}~,\theta \mid \mathbf{y})} = \frac{p(y \mid \mathbf{u}, \theta)\,p(\mathbf{u}, \theta)} {p(y)}\)</span></p>
<p><span class="math display">\[
\begin{aligned}
p(\mathbf{u}~,\theta\mid \mathbf{y}) &amp;=
\color{tomato}{\frac{p(y \mid \mathbf{u}, \theta)\,
      p(\mathbf{u}, \theta)}
     {p(y)}} \frac{1}{p(\mathbf{u} \mid \theta, \mathbf{y})} ~~\color{grey}{\text{Factorization of the joint prior} ~~p(\mathbf{u},\theta)=p(\mathbf{u}\mid \theta)p(\theta)}\\
&amp;=
\frac{p(y \mid \mathbf{u}, \theta)\,
      p(\mathbf{u} \mid \theta)\,
      p(\theta)}
     {p(y)} \frac{1}{p(\mathbf{u} \mid \theta, \mathbf{y})} \\
&amp;\propto \dfrac{p(\mathbf{y}\mid \mathbf{u},\theta)p(\mathbf{u}\mid \theta)p(\theta)}{p(\mathbf{u}\mid \theta, \mathbf{y})}
\end{aligned}
\]</span></p>
<p>Where</p>
<ul>
<li><span class="math inline">\(p(\mathbf{y}\mid \mathbf{u},\theta)\)</span> is the likelihood function which we know</li>
<li><span class="math inline">\(p(\mathbf{u}\mid \theta) = N(0,Q^{-1})\)</span> is the Gaussian prior for the latent field which we also know</li>
<li><span class="math inline">\(p(\theta)\)</span> is a non-Gaussian prior for the hyperameters (also known)</li>
<li><span class="math inline">\(p(\mathbf{u}\mid \theta, \mathbf{y})\)</span> the non-Gaussian density for the full condition of the latent effects … which we DON’t know</li>
</ul>
<p>In practice we approximate <span class="math inline">\(p(\mathbf{u}\mid \theta, \mathbf{y})\)</span> with a Gaussian distribution <span class="math inline">\(p_G(\mathbf{u}\mid \theta, \mathbf{y})\)</span>. How? we use the Laplace method.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Laplace approximation
</div>
</div>
<div class="callout-body-container callout-body">
<p>The Laplace approximation is an old technique for the approximation of integrals. Suppose the following integral that we want to approximate as <span class="math inline">\(n \to \infty\)</span>:</p>
<p><span class="math display">\[
I_n = \int_x \exp(n f(x))\,dx,
\]</span></p>
<p>We represent <span class="math inline">\(f(x)\)</span> by means of a Taylor series expansion evaluated at a point <span class="math inline">\(x_0\)</span>in which <span class="math inline">\(f(x)\)</span> has its maximum so that</p>
<p><span class="math display">\[
\begin{aligned}
f(x) &amp;\approx f(x_0) + \cancelto{0}{f'(x_0)(x-x_0)} + \frac{1}{2}f''(x_0)(x-x_0)^2 \\
\Rightarrow I_n &amp;\approx \exp\left\{n f(x_0)\right\} \underbrace{\int_x\exp\left\{\frac{n}{2} (x-x_0)^2f''(x_0) \right\}dx}_{\text{Gaussian Kernell}}\\
\therefore \tilde{I}_n &amp;= \exp(n f(x_0))
\sqrt{\frac{2\pi}{-n f''(x_0)}}
\end{aligned}
\]</span> Considering <span class="math inline">\(nf(x)\)</span> as the sum of log-likelihoods and <span class="math inline">\(x\)</span> as the unknown parameter, then we can apply Laplace method to approximate it by evaluating the Taylor series expansion at <span class="math inline">\(x_0\)</span>.</p>
</div>
</div>
<p>The full conditional we want to approximate is given by</p>
<p><span class="math display">\[
\begin{aligned}
p(\mathbf{u}\mid \theta, \mathbf{y}) &amp;\propto \pi(\mathbf{u} \mid \theta)\,\pi(\mathbf{y} \mid \mathbf{u}, \theta) \\
&amp;\propto
\exp\!\left(
-\frac{1}{2}\mathbf{u}^\top \mathbf{Q} \mathbf{u}
+
\sum_i \log \pi(y_i \mid \eta_i, \theta)
\right).
\end{aligned}
\]</span></p>
<p>We can take the second-order Taylor series expansion for the log density <span class="math inline">\(\log p(y_i\mid \eta_i,\theta)\)</span>.</p>
<p>Recall that by letting $ c=-f’’(x_0),;b=cx_0$ the second-order Taylor series expansion for <span class="math inline">\(f(x)\)</span> evaluated at its maximum <span class="math inline">\(x_0\)</span> is:</p>
<p><span class="math display">\[
\begin{aligned}
f(x)&amp;\approx f(x_0) +\tfrac12f''(x_0)(x-x_0)^2 \\
&amp;\approx f(x_0)+\tfrac12f''(x_0) (x^2 -2x_0x +x_0^2)   \\
&amp;\approx\underbrace{\bigl[f(x_0)+\tfrac12f''(x_0)x_0^2\bigr]}_{\text{constant } a}+ \underbrace{\bigl[-f''(x_0)x_0\bigr]}_{b}x+\tfrac12\underbrace{f''(x_0)}_{-c}x^2 \qquad\color{grey}{\text{setting }  c=-f''(x_0),\;b=cx_0}\\
&amp;\approx a+bx-\tfrac12cx^2.
\end{aligned}
\]</span></p>
<p>Thus the second order Taylor expansion for the log density yields to</p>
<p><span class="math display">\[
\begin{aligned}
p(\mathbf{u}\mid \theta, \mathbf{y}) &amp;\propto \exp\!\left\{
-\frac{1}{2}\mathbf{u}^\top \mathbf{Q} \mathbf{u}
+
\sum_i ( a_i + b_i \eta_i - \frac{1}{2}c_i \eta_i^2)
\right\}.\\
&amp;\approx \exp\left\{ \frac{1}{2}\mathbf{u}^\intercal \tilde{Q}\mathbf{u} + \tilde{\mathbf{u}}\right\}
\end{aligned}
\]</span></p>
<p>In short, we approximate <span class="math inline">\(p(\mathbf{u}\theta,\mathbf{y})\)</span> with a Gaussian <span class="math inline">\(N(\tilde{Q}\tilde{\mathbf{b}},\tilde{Q})\)</span> with <span class="math inline">\(\tilde{Q}= Q + \begin{bmatrix}\text{diag}(\mathbf{c}) &amp;0\\0&amp;0 \end{bmatrix}\)</span> and <span class="math inline">\(\tilde{\mathbf{b}} = [\mathbf{b}~ 0]^\intercal\)</span></p>
<ul>
<li><p><span class="math inline">\(\mathbf{b}\)</span>: Contains the first derivatives of the log-likelihood w.r.t. each <span class="math inline">\(\eta_i\)</span>, evaluated at the expansion point (i.e.&nbsp;the mode).</p></li>
<li><p><span class="math inline">\(\mathbf{c}\)</span>: Contains the negative second derivatives (observed Fisher information) of the log-likelihood w.r.t. each <span class="math inline">\(\eta_i\)</span>.</p></li>
</ul>
<p>Thus, the approximation of the joint posterior of the hyperparameters is given by:</p>
<p><span class="math display">\[
\begin{aligned}
p(\theta\mid \mathbf{y}) &amp;\propto \dfrac{p(\mathbf{y}\mid \mathbf{u},\theta)p(\mathbf{u}\mid \theta)p(\theta)}{p(\mathbf{u}\mid \theta, \mathbf{y})} \\
&amp;\approx
\left.
\frac{
p(\mathbf{y} \mid \mathbf{u},\theta)\,
p(\mathbf{u} \mid \theta)\,
p(\theta)
}{
p_G(\mathbf{u} \mid \mathbf{y},\theta)
}
\right|_{u=u^*(\theta)}.
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(p_G(\mathbf{u} \mid \mathbf{y},\theta))\)</span> is the Gaussian approximation – based the Laplace method - of <span class="math inline">\(p(\mathbf{u}\mid \theta, \mathbf{y})\)</span> and <span class="math inline">\(u^*(\theta)\)</span> is the mode for a given <span class="math inline">\(\theta\)</span>. This Gaussian approximation will be exact as <span class="math inline">\(n \to \infty\)</span>.</p>
<p>Different optimization techniques like Newton-type methods can be used to find the mode <span class="math inline">\(\tilde{p}(\theta\mid \mathbf{y})\)</span> and then exploring <span class="math inline">\(\tilde{p}(\theta\mid \mathbf{y})\)</span> to find grid point for numerical integration. <a href="#fig-approx_1" class="quarto-xref">Figure&nbsp;1</a> illustrates the numerical integration approach to compute <span class="math inline">\(p(\theta\mid \mathbf{y})\)</span> for a regular grid series of points <span class="math inline">\(\theta_k, k = 1,\ldots,K\)</span></p>
<div id="fig-approx_1" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-approx_1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="hyperpar_approx.gif" class="img-fluid quarto-figure quarto-figure-center figure-img" width="250">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-approx_1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Illustration of numerical approximation of the hyperparameters posterior density
</figcaption>
</figure>
</div>
<p>Now we just need to approximate <span class="math inline">\(p(u_i\mid \theta\mathbf{y})\)</span>, but unfortunately we cannot use the marginals from the preivous Gaussian approximation <span class="math inline">\(p_G(\mathbf{u} \mid \mathbf{y},\theta)\)</span> directly as there can be errors in the location and/or errors due to the lack of skewness.</p>
</section>
<section id="approximating-the-posterior-for-the-latent-field" class="level2">
<h2 class="anchored" data-anchor-id="approximating-the-posterior-for-the-latent-field"><strong>Approximating the posterior for the latent field</strong></h2>
<p>There are two ways of approximating the marginals for the latent fields. The first one is by simply using the Gaussian approximation and computing the posterior conditional distributions <span class="math inline">\(p(u_i\mid \theta,\mathbf{y})\)</span> directly as marginals of <span class="math inline">\(p_G(\mathbf{u} \mid \mathbf{y},\theta)\)</span> i.e.,</p>
<p><span class="math display">\[
\tilde{p}(u_i\mid \theta,\mathbf{y})=N(u_i;\mu_i(\theta),\sigma_i^2(\theta))
\]</span></p>
<p>with mean <span class="math inline">\(\mu_i(\theta)\)</span> and variance <span class="math inline">\(\sigma^2_i(\theta)\)</span>. However, while computationally cost effective, this approximation is generally not very good specially for non-Gaussian likelihoods due to errors in the location and /or lack of skewness.</p>
<p>Thus, a different approach is to write the vector of hyperparameters <span class="math inline">\(\mathbf{u} = \{u_i,\mathbf{u}_{-i}\}\)</span> and use the Laplace approximation again for:</p>
<p><span class="math display">\[
\begin{aligned}
p(u_i \mid \boldsymbol{\psi}, \mathbf{y}) &amp;= \frac{p(\{u_i,\mathbf{u}_{-i}\} \mid \theta, \mathbf{y})}{p(\mathbf{u}_{-i}\mid u_i, \theta, \mathbf{y})} \\
&amp;= \frac{p(\mathbf{u}, \theta \mid \mathbf{y})}{p(\theta \mid \mathbf{y})} \cdot \frac{1}{p(\mathbf{u}_{-i} \mid u_i, \theta, \mathbf{y})}\\
&amp;\propto \frac{p(\mathbf{u}, \theta \mid \mathbf{y})}{p(\mathbf{u}_{-i} \mid u_i, \theta, \mathbf{y})} \\
&amp;\left.\approx \frac{p(\mathbf{u}, \theta \mid \mathbf{y})}{\widetilde{p}(\mathbf{u}_{-i} \mid u_i, \theta, \mathbf{y})} \right|_{\mathbf{u}_{-i}=\mathbf{u}^*_{-i}(u_i,\theta)}=:\tilde{p}(u_i\mid\theta,\mathbf{y}).
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(\widetilde{p}(\mathbf{u}_{-i} \mid u_i, \theta, \mathbf{y})\)</span> is the Laplace Gaussian approximation to <span class="math inline">\(p(\mathbf{u}_{-i} \mid u_i, \theta, \mathbf{y})\)</span> and <span class="math inline">\(\mathbf{u}_{-i}^*(u_i, \theta)\)</span> is its mode.</p>
<p>This nested extra Laplace approximation step (an hence the Integrated <em>Nested Laplace</em> Approximation name) is sufficiently accurate but computationally more expensive that the simpler Gaussian strategy. Thus, Variational Bayes techniques have been used to improve the mean of the GMRF approximation instead (see <span class="citation" data-cites="vanniekerk2021">(<a href="#ref-vanniekerk2021" role="doc-biblioref">Niekerk and Rue 2021</a>)</span>). This correction is computationally efficient and achieves accuracy for the mean similar to a full integrated nested Laplace approximation.</p>
<p>From this improved GMRF approximation we can then compute the marginals <span class="math inline">\(p_G^*(u_i \mid \mathbf{y},\theta)\)</span> to obtain:</p>
<p><span class="math display">\[
\tilde{p}(u_i\mid\mathbf{y}) = \sum_k p_G^*(u_i \mid \mathbf{y},\theta) \tilde{p}(\theta\mid\mathbf{y})w_k
\]</span></p>



</section>
</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-vanniekerk2021" class="csl-entry" role="listitem">
Niekerk, Janet van, and Haavard Rue. 2021. <span>“Low-Rank Variational Bayes Correction to the Laplace Method.”</span> <a href="https://doi.org/10.48550/ARXIV.2111.12945">https://doi.org/10.48550/ARXIV.2111.12945</a>.
</div>
</div></section></div></main> <!-- /main -->
<script>

/* update total correct if #webex-total_correct exists */
update_total_correct = function() {
  console.log("webex: update total_correct");

  var t = document.getElementsByClassName("webex-total_correct");
  for (var i = 0; i < t.length; i++) {
    p = t[i].parentElement;
    var correct = p.getElementsByClassName("webex-correct").length;
    var solvemes = p.getElementsByClassName("webex-solveme").length;
    var radiogroups = p.getElementsByClassName("webex-radiogroup").length;
    var selects = p.getElementsByClassName("webex-select").length;

    t[i].innerHTML = correct + " of " + (solvemes + radiogroups + selects) + " correct";
  }
}

/* webex-solution button toggling function */
b_func = function() {
  console.log("webex: toggle hide");

  var cl = this.parentElement.classList;
  if (cl.contains('open')) {
    cl.remove("open");
  } else {
    cl.add("open");
  }
}

/* check answers */
check_func = function() {
  console.log("webex: check answers");

  var cl = this.parentElement.classList;
  if (cl.contains('unchecked')) {
    cl.remove("unchecked");
    this.innerHTML = "Hide Answers";
  } else {
    cl.add("unchecked");
    this.innerHTML = "Show Answers";
  }
}

/* function for checking solveme answers */
solveme_func = function(e) {
  console.log("webex: check solveme");

  var real_answers = JSON.parse(this.dataset.answer);
  var my_answer = this.value;
  var cl = this.classList;
  if (cl.contains("ignorecase")) {
    my_answer = my_answer.toLowerCase();
  }
  if (cl.contains("nospaces")) {
    my_answer = my_answer.replace(/ /g, "")
  }

  if (my_answer == "") {
    cl.remove("webex-correct");
    cl.remove("webex-incorrect");
  } else if (real_answers.includes(my_answer)) {
    cl.add("webex-correct");
    cl.remove("webex-incorrect");
  } else {
    cl.add("webex-incorrect");
    cl.remove("webex-correct");
  }

  // match numeric answers within a specified tolerance
  if(this.dataset.tol > 0){
    var tol = JSON.parse(this.dataset.tol);
    var matches = real_answers.map(x => Math.abs(x - my_answer) < tol)
    if (matches.reduce((a, b) => a + b, 0) > 0) {
      cl.add("webex-correct");
    } else {
      cl.remove("webex-correct");
    }
  }

  // added regex bit
  if (cl.contains("regex")){
    answer_regex = RegExp(real_answers.join("|"))
    if (answer_regex.test(my_answer)) {
      cl.add("webex-correct");
    }
  }

  update_total_correct();
}

/* function for checking select answers */
select_func = function(e) {
  console.log("webex: check select");

  var cl = this.classList

  /* add style */
  cl.remove("webex-incorrect");
  cl.remove("webex-correct");
  if (this.value == "answer") {
    cl.add("webex-correct");
  } else if (this.value != "blank") {
    cl.add("webex-incorrect");
  }

  update_total_correct();
}

/* function for checking radiogroups answers */
radiogroups_func = function(e) {
  console.log("webex: check radiogroups");

  var checked_button = document.querySelector('input[name=' + this.id + ']:checked');
  var cl = checked_button.parentElement.classList;
  var labels = checked_button.parentElement.parentElement.children;

  /* get rid of styles */
  for (i = 0; i < labels.length; i++) {
    labels[i].classList.remove("webex-incorrect");
    labels[i].classList.remove("webex-correct");
  }

  /* add style */
  if (checked_button.value == "answer") {
    cl.add("webex-correct");
  } else {
    cl.add("webex-incorrect");
  }

  update_total_correct();
}

window.onload = function() {
  console.log("webex onload");
  /* set up solution buttons */
  var buttons = document.getElementsByTagName("button");

  for (var i = 0; i < buttons.length; i++) {
    if (buttons[i].parentElement.classList.contains('webex-solution')) {
      buttons[i].onclick = b_func;
    }
  }

  var check_sections = document.getElementsByClassName("webex-check");
  console.log("check:", check_sections.length);
  for (var i = 0; i < check_sections.length; i++) {
    check_sections[i].classList.add("unchecked");

    let btn = document.createElement("button");
    btn.innerHTML = "Show Answers";
    btn.classList.add("webex-check-button");
    btn.onclick = check_func;
    check_sections[i].appendChild(btn);

    let spn = document.createElement("span");
    spn.classList.add("webex-total_correct");
    check_sections[i].appendChild(spn);
  }

  /* set up webex-solveme inputs */
  var solveme = document.getElementsByClassName("webex-solveme");

  for (var i = 0; i < solveme.length; i++) {
    /* make sure input boxes don't auto-anything */
    solveme[i].setAttribute("autocomplete","off");
    solveme[i].setAttribute("autocorrect", "off");
    solveme[i].setAttribute("autocapitalize", "off");
    solveme[i].setAttribute("spellcheck", "false");
    solveme[i].value = "";

    /* adjust answer for ignorecase or nospaces */
    var cl = solveme[i].classList;
    var real_answer = solveme[i].dataset.answer;
    if (cl.contains("ignorecase")) {
      real_answer = real_answer.toLowerCase();
    }
    if (cl.contains("nospaces")) {
      real_answer = real_answer.replace(/ /g, "");
    }
    solveme[i].dataset.answer = real_answer;

    /* attach checking function */
    solveme[i].onkeyup = solveme_func;
    solveme[i].onchange = solveme_func;

    solveme[i].insertAdjacentHTML("afterend", " <span class='webex-icon'></span>")
  }

  /* set up radiogroups */
  var radiogroups = document.getElementsByClassName("webex-radiogroup");
  for (var i = 0; i < radiogroups.length; i++) {
    radiogroups[i].onchange = radiogroups_func;
  }

  /* set up selects */
  var selects = document.getElementsByClassName("webex-select");
  for (var i = 0; i < selects.length; i++) {
    selects[i].onchange = select_func;
    selects[i].insertAdjacentHTML("afterend", " <span class='webex-icon'></span>")
  }

  update_total_correct();
}

</script>
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
<p><a href="https://envstats-stats4009-stats503.github.io/Course_Notes/2025_26/Module_3/docs/" style="color:#ccc;">Home</a></p>
</div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




</body></html>