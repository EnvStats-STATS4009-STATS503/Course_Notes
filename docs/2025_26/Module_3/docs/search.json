[
  {
    "objectID": "slides/slides_9.html#point-process-data",
    "href": "slides/slides_9.html#point-process-data",
    "title": "Methods for Point referrenced Data",
    "section": "Point process data",
    "text": "Point process data\n\n\n\nMany of the ecological and environmental processes of interest can be represented by a spatial point process or can be viewed as an aggregation of one.\n\n\nMany contemporary data sources collect georeferenced information about the location where an event has occur (e.g., species occurrence, wildfire, flood events).\nThis point-based information provides valuable insights into ecosystem dynamics."
  },
  {
    "objectID": "slides/slides_9.html#defining-a-point-process",
    "href": "slides/slides_9.html#defining-a-point-process",
    "title": "Methods for Point referrenced Data",
    "section": "Defining a Point Process",
    "text": "Defining a Point Process\n\nConsider a fixed geographical region \\(A\\).\nThe set of locations at which events occur are denoted by \\(\\mathbf{s} = (\\mathbf{s}_1, \\ldots, \\mathbf{s}_n)\\).\nWe let \\(N(A)\\) be a random variable which represents the total number of events in every subset of region \\(A\\).\nOur primary interest is in measuring where events occur, so the locations are our data."
  },
  {
    "objectID": "slides/slides_9.html#spatial-point-patterns",
    "href": "slides/slides_9.html#spatial-point-patterns",
    "title": "Methods for Point referrenced Data",
    "section": "Spatial point patterns",
    "text": "Spatial point patterns\n\nThere are three broad types of spatial Spatial point patterns which can be explored, each representing a different type of spatial dependence.\n\nComplete spatial randomness (CSR) - events occur at random, and independently of each other.\nClustered process - events occur close to existing events.\nRegular process - events occur away from existing events."
  },
  {
    "objectID": "slides/slides_9.html#the-closer-you-look-the-less-you-see",
    "href": "slides/slides_9.html#the-closer-you-look-the-less-you-see",
    "title": "Methods for Point referrenced Data",
    "section": "The closer you look the less you see",
    "text": "The closer you look the less you see\nLocations of isopods burrows in the northern Negev Desert, Israel"
  },
  {
    "objectID": "slides/slides_9.html#the-closer-you-look-the-less-you-see-1",
    "href": "slides/slides_9.html#the-closer-you-look-the-less-you-see-1",
    "title": "Methods for Point referrenced Data",
    "section": "The closer you look the less you see",
    "text": "The closer you look the less you see\nLocations of isopods burrows in the northern Negev Desert, Israel"
  },
  {
    "objectID": "slides/slides_9.html#example-spatial-scales-and-point-patterns",
    "href": "slides/slides_9.html#example-spatial-scales-and-point-patterns",
    "title": "Methods for Point referrenced Data",
    "section": "Example: Spatial scales and point patterns",
    "text": "Example: Spatial scales and point patterns\nLocations of isopods burrows in the northern Negev Desert, Israel.\nIf we look at the complete data set we can see a strong aggregation pattern\n\nIsopod burrow data with \\(n=2015\\) individual burrows in a study area with an extent of 75,600 m\\(^2\\). Inset maps show two subsets with reduced extent of 6.4 m\\(^2\\) each. Source: (Dungan et al. 2002)."
  },
  {
    "objectID": "slides/slides_9.html#summarising-a-point-process",
    "href": "slides/slides_9.html#summarising-a-point-process",
    "title": "Methods for Point referrenced Data",
    "section": "Summarising a point process",
    "text": "Summarising a point process\n\n\nWe can define the (first order) intensity of a point process as the expected number of events per unit area.\nThis can also be thought of as a measure of the density of our points.\nIn some cases, the intensity will be constant over space (homogeneous), while in other cases it can vary by location (inhomogeneous or heterogenous).\nIf our intensity is homogeneous, we can define it as\n\\[\n\\lambda(s) = \\frac{\\mathbb{E}[N(A)]}{|A|} = \\frac{\\lambda |A|}{|A|} \\lambda.\n\\]"
  },
  {
    "objectID": "slides/slides_9.html#complete-spatial-randomness",
    "href": "slides/slides_9.html#complete-spatial-randomness",
    "title": "Methods for Point referrenced Data",
    "section": "Complete spatial randomness",
    "text": "Complete spatial randomness\nWe can use the concept of intensity to help us define complete spatial randomness (CSR).\n\nFor any spatial region \\(A\\), CSR requires that:\n\n\n\nUniformity and Independent scattering : Given the number of events \\(N(A) = n\\) in a region, the \\(n\\) events are independently and uniformly distributed over space (i.e., each event has an equal probability of occurring anywhere in the study area).\nPoisson distribution of point counts: The number of points in any set \\(A_i\\) follows a Poisson distribution with mean \\(\\lambda|A_i|\\), that is \\[N(A_i) \\sim \\text{Poisson}(\\lambda|A_i|).\\]\n\n\n\n\n\nIf these conditions are satisfied, we can describe our process as a homogeneous Poisson process.\n\n\n\n\nCSR describes a point pattern that is “completely random”.\nconditional on the number of points \\(N(A) = n\\), the \\(n\\) points are independently and uniformly distributed over \\(A\\) i.e., each event has an equal probability of occurring anywhere in the study areaNOT that the Point form an uniform spatial patter\nThus, a realization of a HPP is a PP described by CSR."
  },
  {
    "objectID": "slides/slides_9.html#homogeneous-poisson-process",
    "href": "slides/slides_9.html#homogeneous-poisson-process",
    "title": "Methods for Point referrenced Data",
    "section": "Homogeneous Poisson Process",
    "text": "Homogeneous Poisson Process\n\n\n\n\nA simplest point process model is the homogeneous Poisson process (HPP).\nThe likelihood of a point pattern \\(\\mathbf{y} = \\left[ \\mathbf{s}_1, \\ldots, \\mathbf{s}_n \\right]^\\intercal\\) distributed as a HPP with intensity \\(\\lambda\\) and observation window \\(\\Omega\\) is\n\\[\np(\\mathbf{y} | \\lambda) \\propto \\lambda^n e^{ \\left( - |\\Omega| \\lambda \\right)} ,\n\\]\n\n\\(|\\Omega|\\) is the size of the observation window.\n\\(\\lambda\\) is the expected number of points per unit area.\n\\(|\\Omega|\\lambda\\) the total expected number of points in the observation window.\n\n\n\n\n\n\nA key property of a Poisson process is that the number of points within any subset \\(A_i\\) of region \\(A\\) is Poisson distributed with constant rate \\(|A_i|\\lambda\\)."
  },
  {
    "objectID": "slides/slides_9.html#ripleys-k-function",
    "href": "slides/slides_9.html#ripleys-k-function",
    "title": "Methods for Point referrenced Data",
    "section": "Ripley’s K function",
    "text": "Ripley’s K function\n\nWe can contrast the observed point pattern against a point pattern generated from this CSR model to determine whether a HPP is appropriate for our data.\nTo do this, we calculate what is known as Ripley’s K-function.\n\n\n\n\nRipley’s K is a function of distance \\(r\\), and is given by\n\\[\nK(r) = \\frac{E[N(\\mathbf{s}_0, r)]}{\\lambda}\n\\]\n\\(N(\\mathbf{s}_0, r)\\) denotes the number of events that occur within distance \\(r\\) of an event \\(\\mathbf{s}_0\\).\nClearly, as \\(r\\) increases, so too will \\(K(r)\\)."
  },
  {
    "objectID": "slides/slides_9.html#ripleys-k-function-1",
    "href": "slides/slides_9.html#ripleys-k-function-1",
    "title": "Methods for Point referrenced Data",
    "section": "Ripley’s K function",
    "text": "Ripley’s K function\nRipley’s \\(K\\) can be estimated as:\n\\[\n\\hat{K}(r) = \\color{tomato}{\\frac{1}{n}\\sum_{i=1}^n\\sum_{i\\neq j}I(||s_i-s_j||&lt;r)} \\times \\color{purple}{\\lambda^{-1}}\n\\]\n\nThis first part of the equation corresponds to the expected number of events that occur within a buffer of radius \\(r\\)\nThe second part of the equation corresponds to the density of events estimated \\(\\lambda = n/|A|\\).\nThe idea is to compare \\(\\hat{K}(r)\\) against the expected \\(K(r)\\) under CSR, i.e. \\(K_{CSR}(r)\\)"
  },
  {
    "objectID": "slides/slides_9.html#using-the-k-function-to-assess-csr",
    "href": "slides/slides_9.html#using-the-k-function-to-assess-csr",
    "title": "Methods for Point referrenced Data",
    "section": "Using the K function to assess CSR",
    "text": "Using the K function to assess CSR\n\n\nIf we have a homogeneous Poisson process, we would expect that\n\nThe expected number of points in any area is \\(\\lambda \\times |A|\\)\nTherefore: \\(E[\\text{points within } r] = \\lambda \\times \\pi r^2\\)\n\\[\nK_{CSR}(r) = \\lambda \\times (\\pi  r^2) \\times \\lambda^{-1} = \\pi \\times r^2\n\\]\n\nThat is, under CSR we would expect that the K function is equal to the area of the circle with radius \\(r\\)."
  },
  {
    "objectID": "slides/slides_9.html#using-the-k-function-to-assess-csr-1",
    "href": "slides/slides_9.html#using-the-k-function-to-assess-csr-1",
    "title": "Methods for Point referrenced Data",
    "section": "Using the K function to assess CSR",
    "text": "Using the K function to assess CSR\n\nWhen working with real data, some natural variation is to be expected even when CSR holds.\nWe therefore need an approach which accounts for this when assessing for CSR.\n\n\n\nWe can estimate \\(\\hat{K}(r)\\) across a set of distances \\(r\\) for our set of observed events.\nOur \\(\\hat{K}(r)\\) can then be compared to the theoretical function under CSR, \\(K_{CRS}(r) = \\pi r^2\\).\nIf the two functions are similar, then CSR is reasonable."
  },
  {
    "objectID": "slides/slides_9.html#using-the-k-function-to-assess-csr-2",
    "href": "slides/slides_9.html#using-the-k-function-to-assess-csr-2",
    "title": "Methods for Point referrenced Data",
    "section": "Using the K function to assess CSR",
    "text": "Using the K function to assess CSR\n\n\nSpatial clustering\n\n\n\nPeople sitting in a Park\n\n\n\nCSR\n\n\n\nJapanese Pines in a Forest\n\n\n\nRegular pattern\n\n\n\nCells in Petri dish"
  },
  {
    "objectID": "slides/slides_9.html#using-the-k-function-to-assess-csr-3",
    "href": "slides/slides_9.html#using-the-k-function-to-assess-csr-3",
    "title": "Methods for Point referrenced Data",
    "section": "Using the K function to assess CSR",
    "text": "Using the K function to assess CSR\n\n\nSpatial clustering\n\nIf \\(\\hat{K}(r) &gt; K_{CSR}(r)\\) it means that more points are found within a radius \\(r\\) than what would be expected under CSR, suggesting a clustering pattern.\n\nE.g., tree seedlings often cluster near parent trees due to seed dispersal limitations.\n\n\n\n\n\nPeople sitting in a Park\n\n\n\nCSR\n\n\n\nJapanese Pines in a Forest\n\n\n\nRegular pattern\n\n\n\nCells in Petri dish"
  },
  {
    "objectID": "slides/slides_9.html#using-the-k-function-to-assess-csr-4",
    "href": "slides/slides_9.html#using-the-k-function-to-assess-csr-4",
    "title": "Methods for Point referrenced Data",
    "section": "Using the K function to assess CSR",
    "text": "Using the K function to assess CSR\n\n\nSpatial clustering\n\n\n\nPeople sitting in a Park\n\n\n\nCSR\nIf \\(\\hat{K}(r) = K_{CSR}(r)\\) then our PP is a realization of an HPP.\n\n\n\nJapanese Pines in a Forest\n\n\n\nRegular pattern\n\n\n\nCells in Petri dish"
  },
  {
    "objectID": "slides/slides_9.html#using-the-k-function-to-assess-csr-5",
    "href": "slides/slides_9.html#using-the-k-function-to-assess-csr-5",
    "title": "Methods for Point referrenced Data",
    "section": "Using the K function to assess CSR",
    "text": "Using the K function to assess CSR\n\n\nSpatial clustering\n\n\n\nPeople sitting in a Park\n\n\n\nCSR\n\n\n\nJapanese Pines in a Forest\n\n\n\nRegular pattern\n\nIf \\(\\hat{K}(r) &lt; K_{CSR}(r)\\), it indicates that the pattern is more regular since we observe fewer neighboring points within a distance \\(r\\) than expected under CSR.\n\nE.g., territorial animals (e.g., nesting birds) often exhibit regular spacing due to competition for space.\n\n\n\n\n\nCells in Petri dish"
  },
  {
    "objectID": "slides/slides_9.html#limitations",
    "href": "slides/slides_9.html#limitations",
    "title": "Methods for Point referrenced Data",
    "section": "Limitations",
    "text": "Limitations\nWhile Ripley’s \\(K\\) function is widely used in environmental and ecological studies it has some caveats.\n\n\\(K(s)\\) is a cumulative function, where all points less than \\(r\\) are also used.\nEdge effects. This occurs because points near the boundaries of the study area have fewer neighboring points within distance \\(r\\), leading to underestimation of \\(K(r)\\)\n\n\n\nSo, if there is a strong clustering patter at 5m but no pattern at larger distances, then Ripley’s \\(K\\) could still indicate a strong clustering at larger scales due to the data \\(&lt;\\) 5m still being used.\nThe \\(K(r)\\) function can be adjusted for edge effects by including some weights \\(w\\). E.g., if half of a point’s search area lies within the study area then the point will receive a weight \\(w=2\\), meaning it contributes twice as much to compensate for the missing area."
  },
  {
    "objectID": "slides/slides_9.html#inhomogeneous-poisson-process-1",
    "href": "slides/slides_9.html#inhomogeneous-poisson-process-1",
    "title": "Methods for Point referrenced Data",
    "section": "Inhomogeneous Poisson process",
    "text": "Inhomogeneous Poisson process\nSo far we have assumed that the point process is stationary and isotropic.\n\nThese assumption rarely hold true in real-data\ninhomogeneous Poisson process (IPP) models are often used for inference prediction and mapping spatial patterns\n\nThe IPP has a spatially varying intensity \\(\\lambda(\\mathbf{s})\\) defined in terms of spatially varying covariates that are available across the whole study area:\n\\[\n\\lambda(s) = \\mathrm{exp}(\\alpha+\\beta x(s) +\\ldots )\n\\]"
  },
  {
    "objectID": "slides/slides_9.html#inhomogeneous-poisson-process-2",
    "href": "slides/slides_9.html#inhomogeneous-poisson-process-2",
    "title": "Methods for Point referrenced Data",
    "section": "Inhomogeneous Poisson process",
    "text": "Inhomogeneous Poisson process\nLet \\(\\mathbf{y} = s_1,\\ldots,s_n\\) the \\(n\\) number of observed events/points in an observation window \\(\\Omega\\)\nFor an IPP with an intensity \\(\\lambda(s)\\), the likelihood is given by:\n\\[\np(\\mathbf{y} | \\lambda) \\propto \\exp \\left( -\\int_\\Omega \\lambda(\\mathbf{s}) \\mathrm{d}\\mathbf{s} \\right) \\prod_{i=1}^n \\lambda(\\mathbf{s}_i).\n\\]\n\n\nIf the case of an HPP the integral in the likelihood can easily be computed as \\(\\int_\\Omega \\lambda(\\mathbf{s}) \\mathrm{d}\\mathbf{s} =|\\Omega|\\lambda\\)\nFor an HPP with an intensity \\(\\lambda\\), the log-likelihood is given by: \\[\nl(\\beta;\\mathbf{y}) = n\\log(\\lambda) -\\lambda|\\Omega|,\n\\]\nThe maximum likelihood estimators is \\(\\hat{\\lambda} = n/|\\Omega|\\).\nFor IPP, the integral in the likelihood has to be approximateda as a weighted sum."
  },
  {
    "objectID": "slides/slides_9.html#inhomogeneous-poisson-process-3",
    "href": "slides/slides_9.html#inhomogeneous-poisson-process-3",
    "title": "Methods for Point referrenced Data",
    "section": "Inhomogeneous Poisson process",
    "text": "Inhomogeneous Poisson process\nThis integral is approximated as \\(\\int_\\Omega \\lambda(\\mathbf{s}) \\mathrm{d}\\mathbf{s} \\approx \\sum_{j=1}^J w_j \\lambda(\\mathbf{s}_j)\\)\n\n\\(w_j\\) are the integration weights\n\\(\\mathbf{s}_j\\) are the quadrature locations.\n\nThis serves two purposes:\n\nApproximating the integral\nre-writing the inhomogeneous Poisson process likelihood as a regular Poisson likelihood."
  },
  {
    "objectID": "slides/slides_9.html#inhomogeneous-poisson-process-4",
    "href": "slides/slides_9.html#inhomogeneous-poisson-process-4",
    "title": "Methods for Point referrenced Data",
    "section": "Inhomogeneous Poisson process",
    "text": "Inhomogeneous Poisson process\nThe idea behind the trick to rewrite the approximate likelihood is to introduce a dummy vector \\(\\mathbf{z}\\) and an integration weights vector \\(\\mathbf{w}\\) of length \\(J + n\\)\n\n\n\\[\\mathbf{z} = \\left[\\underbrace{0_1, \\ldots,0_J}_\\text{quadrature locations}, \\underbrace{1_1, \\ldots ,1_n}_{\\text{data points}} \\right]^\\intercal\\]\n\n\\[\\mathbf{w} = \\left[ \\underbrace{w_1, \\ldots, w_J}_\\text{quadrature locations}, \\underbrace{0_1, \\ldots, 0_n}_\\text{data points} \\right]^\\intercal\\]\n\nThen the approximate likelihood can be written as\n\\[\n\\begin{aligned}\np(\\mathbf{z} | \\lambda) &\\propto \\prod_{i=1}^{J + n} \\eta_i^{z_i} \\exp\\left(-w_i \\eta_i \\right) \\\\\n\\eta_i &= \\log\\lambda(\\mathbf{s}_i) = \\mathbf{x}(s)'\\beta\n\\end{aligned}\n\\]\n\nThis is similar to a product of Poisson distributions with means \\(\\eta_i\\), exposures \\(w_i\\) and observations \\(z_i\\)."
  },
  {
    "objectID": "slides/slides_9.html#limitations-with-ipp",
    "href": "slides/slides_9.html#limitations-with-ipp",
    "title": "Methods for Point referrenced Data",
    "section": "Limitations with IPP",
    "text": "Limitations with IPP\n\n\n\n\n\nIPP models assume that data points are conditionally independent given the covariates, meaning that any spatial variation is fully explained by environmental and sampling factors.\nUnmeasured endogenous and exogenous factors can create spatial dependence.\nIgnoring them can lead to bias in our conclusions."
  },
  {
    "objectID": "slides/slides_9.html#the-log-gaussian-cox-process",
    "href": "slides/slides_9.html#the-log-gaussian-cox-process",
    "title": "Methods for Point referrenced Data",
    "section": "The Log-Gaussian Cox Process",
    "text": "The Log-Gaussian Cox Process\n\n\n\n\nLog-Gaussian Cox processes (LGCP) extend the IPP by allowing the intensity function to vary spatially according to a structured spatial random effect, i.e. the intensity is random\n\n\\[\n\\log~\\lambda(s)= \\mathbf{x}(s)'\\beta + \\xi(s)\n\\]\n\nThe events are then assumed to be independent given the covariates and \\(\\xi(s)\\) - a GRF with Matérn covariance.\n\n\nHow do we model \\(\\xi(s)\\) ?\n\n\n\nWe use an SPDE model!\nThe software inlabru has implemented some integration schemes that are especially well suited to integrating the intensity in models with an SPDE effect.\n\n\n\n\nSee for further reference: Simpson, Daniel, Janine B. Illian, Finn Lindgren, Sigrunn H. Sørbye, and Håvard Rue. 2016. “Going off grid: computationally efficient inference for log-Gaussian Cox processes.” Biometrika 103 (1): 49–70."
  },
  {
    "objectID": "slides/slides_9.html#example-forest-fires-in-castilla-la-mancha",
    "href": "slides/slides_9.html#example-forest-fires-in-castilla-la-mancha",
    "title": "Methods for Point referrenced Data",
    "section": "Example: Forest fires in Castilla-La Mancha",
    "text": "Example: Forest fires in Castilla-La Mancha\n\nIn this example we model the location of forest fires in the Castilla-La Mancha region of Spain between 1998 and 2007.\nWe are now going to use the elevation as a covariate to explain the variability of the intensity \\(\\lambda(s)\\) over the domain of interest and a spatially structured SPDE model.\n\n\\[\n\\log\\lambda(s) = \\beta_0 + \\beta_1 \\text{elevation}(s) + \\xi(s)\n\\]"
  },
  {
    "objectID": "slides/slides_9.html#example-forest-fires-in-castilla-la-mancha-1",
    "href": "slides/slides_9.html#example-forest-fires-in-castilla-la-mancha-1",
    "title": "Methods for Point referrenced Data",
    "section": "Example: Forest fires in Castilla-La Mancha",
    "text": "Example: Forest fires in Castilla-La Mancha\n\n\nThe IPP Model \\[\n\\begin{aligned}\np(\\mathbf{y} | \\lambda)  & \\propto \\exp \\left( -\\int_\\Omega \\lambda(\\mathbf{s}) \\mathrm{d}\\mathbf{s} \\right) \\prod_{i=1}^n \\lambda(\\mathbf{s}_i) \\\\\n\\eta(s) &  = \\log ~\\lambda(s) = \\beta_0 +  x(s)\n\\end{aligned}\n\\] The code\n\n# define model component\ncmp = ~ Intercept(1) + elev(elev_raster, model = \"linear\") \n\n# define model predictor\neta  = geometry ~ Intercept +  elev \n\n# build the observation model\nlik = bru_obs(\"cp\",\n              formula = eta,\n              data = pp,\n              ips = ips)\n\n# fit the model\nfit = bru(cmp, lik)\n\n\n\nRegular Gridfm_int\n\n\n\nn.int = 1000\nips = st_sf(geometry = st_sample(region,\n            size = n.int,\n            type = \"regular\"))\n\nips$weight = st_area(region) / n.int\n\n\n\n\n\n\n\n\n\n\n\n\n\n# The mesh\nmesh = fm_mesh_2d(boundary = region,\n                  max.edge = c(5, 10),\n                  cutoff = 4, crs = NA)\n\n# build integration scheme\nips = fm_int(mesh,\n             samplers = region)"
  },
  {
    "objectID": "slides/slides_9.html#example-forest-fires-in-castilla-la-mancha-2",
    "href": "slides/slides_9.html#example-forest-fires-in-castilla-la-mancha-2",
    "title": "Methods for Point referrenced Data",
    "section": "Example: Forest fires in Castilla-La Mancha",
    "text": "Example: Forest fires in Castilla-La Mancha\n\n\nThe LGCP Model\n\\[\n\\begin{aligned}\np(\\mathbf{y} | \\lambda)  & \\propto \\exp \\left( -\\int_\\Omega \\lambda(\\mathbf{s}) \\mathrm{d}\\mathbf{s} \\right) \\prod_{i=1}^n \\lambda(\\mathbf{s}_i) \\\\\n\\eta(s) &  = \\log(\\lambda(s)) = \\color{#FF6B6B}{\\boxed{\\beta_0}} + \\color{#FF6B6B}{\\boxed{ x(s)}} + \\color{#FF6B6B}{\\boxed{ \\omega(s)}}\\\\\n\\end{aligned}\n\\]\nThe code\n\n# define model component\ncmp = ~ Intercept(1) + elev(elev_raster, model = \"linear\") +\n  space(geometry, model = spde_model) \n\n\n# define model predictor\neta  = geometry ~ Intercept +  elev + space\n\n\n# build the observation model\nlik = bru_obs(\"cp\",\n              formula = eta,\n              data = pp,\n              ips = ips)\n\n\n# fit the model\nfit = bru(cmp, lik)\n\n\n\n# The mesh\nmesh = fm_mesh_2d(boundary = region,\n                  max.edge = c(5, 10),\n                  cutoff = 4, crs = NA)\n\n# The SPDE model\nspde_model =  inla.spde2.pcmatern(mesh,\n                                  prior.sigma = c(1, 0.5),\n                                  prior.range = c(100, 0.5))\n\n# build integration scheme\nips = fm_int(mesh,\n             samplers = region)"
  },
  {
    "objectID": "slides/slides_9.html#example-forest-fires-in-castilla-la-mancha-3",
    "href": "slides/slides_9.html#example-forest-fires-in-castilla-la-mancha-3",
    "title": "Methods for Point referrenced Data",
    "section": "Example: Forest fires in Castilla-La Mancha",
    "text": "Example: Forest fires in Castilla-La Mancha\n\n\nThe LGCP Model\n\\[\n\\begin{aligned}\np(\\mathbf{y} | \\lambda)  & \\propto \\exp \\left( -\\int_\\Omega \\lambda(\\mathbf{s}) \\mathrm{d}\\mathbf{s} \\right) \\prod_{i=1}^n \\lambda(\\mathbf{s}_i) \\\\\n\\color{#FF6B6B}{\\boxed{\\eta(s)}} &  = \\log(\\lambda(s)) = \\color{#FF6B6B}{\\boxed{\\beta_0 +  x(s) + \\omega(s)}}\\\\\n\\end{aligned}\n\\]\nThe code\n\n# define model component\ncmp = ~ Intercept(1) + elev(elev_raster, model = \"linear\") +\n  space(geometry, model = spde_model) \n\n\n# define model predictor\neta  = geometry ~ Intercept +  elev + space\n\n\n# build the observation model\nlik = bru_obs(\"cp\",\n              formula = eta,\n              data = pp,\n              ips = ips)\n\n\n# fit the model\nfit = bru(cmp, lik)\n\n\n\n# The mesh\nmesh = fm_mesh_2d(boundary = region,\n                  max.edge = c(5, 10),\n                  cutoff = 4, crs = NA)\n\n# The SPDE model\nspde_model =  inla.spde2.pcmatern(mesh,\n                                  prior.sigma = c(1, 0.5),\n                                  prior.range = c(100, 0.5))\n\n# build integration scheme\nips = fm_int(mesh,\n             samplers = region)"
  },
  {
    "objectID": "slides/slides_9.html#example-forest-fires-in-castilla-la-mancha-4",
    "href": "slides/slides_9.html#example-forest-fires-in-castilla-la-mancha-4",
    "title": "Methods for Point referrenced Data",
    "section": "Example: Forest fires in Castilla-La Mancha",
    "text": "Example: Forest fires in Castilla-La Mancha\n\n\nThe LGCP Model\n\\[\n\\begin{aligned}\n\\color{#FF6B6B}{\\boxed{p(\\mathbf{y} | \\lambda)}}  & \\propto \\exp \\left( -\\int_\\Omega \\lambda(\\mathbf{s}) \\mathrm{d}\\mathbf{s} \\right) \\prod_{i=1}^n \\lambda(\\mathbf{s}_i) \\\\\n\\eta(s) &  = \\log(\\lambda(s)) = \\beta_0 +  x(s) + \\omega(s)\\\\\n\\end{aligned}\n\\]\nThe code\n\n# define model component\ncmp = ~ Intercept(1) + elev(elev_raster, model = \"linear\") +\n  space(geometry, model = spde_model) \n\n\n# define model predictor\neta  = geometry ~ Intercept +  elev + space\n\n\n# build the observation model\nlik = bru_obs(\"cp\",\n              formula = eta,\n              data = pp,\n              ips = ips)\n\n\n# fit the model\nfit = bru(cmp, lik)\n\n\n\n# The mesh\nmesh = fm_mesh_2d(boundary = region,\n                  max.edge = c(5, 10),\n                  cutoff = 4, crs = NA)\n\n# The SPDE model\nspde_model =  inla.spde2.pcmatern(mesh,\n                                  prior.sigma = c(1, 0.5),\n                                  prior.range = c(100, 0.5))\n\n# build integration scheme\nips = fm_int(mesh,\n             samplers = region)"
  },
  {
    "objectID": "slides/slides_9.html#example-forest-fires-in-castilla-la-mancha-5",
    "href": "slides/slides_9.html#example-forest-fires-in-castilla-la-mancha-5",
    "title": "Methods for Point referrenced Data",
    "section": "Example: Forest fires in Castilla-La Mancha",
    "text": "Example: Forest fires in Castilla-La Mancha\n\n\nThe LGCP Model\n\\[\n\\begin{aligned}\np(\\mathbf{y} | \\lambda)  & \\propto \\exp \\left( -\\int_\\Omega \\lambda(\\mathbf{s}) \\mathrm{d}\\mathbf{s} \\right) \\prod_{i=1}^n \\lambda(\\mathbf{s}_i) \\\\\n\\eta(s) &  = \\log(\\lambda(s)) = \\beta_0 +  x(s) + \\omega(s)\\\\\n\\end{aligned}\n\\]\nThe code\n\n# define model component\ncmp = ~ Intercept(1) + elev(elev_raster, model = \"linear\") +\n  space(geometry, model = spde_model) \n\n\n# define model predictor\neta  = geometry ~ Intercept +  elev + space\n\n\n# build the observation model\nlik = bru_obs(\"cp\",\n              formula = eta,\n              data = pp,\n              ips = ips)\n\n\n# fit the model\nfit = bru(cmp, lik)\n\n\n\n# The mesh\nmesh = fm_mesh_2d(boundary = region,\n                  max.edge = c(5, 10),\n                  cutoff = 4, crs = NA)\n\n# The SPDE model\nspde_model =  inla.spde2.pcmatern(mesh,\n                                  prior.sigma = c(1, 0.5),\n                                  prior.range = c(100, 0.5))\n\n# build integration scheme\nips = fm_int(mesh,\n             samplers = region)"
  },
  {
    "objectID": "slides/slides_9.html#example-forest-fires-in-castilla-la-mancha-6",
    "href": "slides/slides_9.html#example-forest-fires-in-castilla-la-mancha-6",
    "title": "Methods for Point referrenced Data",
    "section": "Example: Forest fires in Castilla-La Mancha",
    "text": "Example: Forest fires in Castilla-La Mancha\nModel Predictions"
  },
  {
    "objectID": "slides/slides_9.html#incomplete-detection",
    "href": "slides/slides_9.html#incomplete-detection",
    "title": "Methods for Point referrenced Data",
    "section": "Incomplete Detection",
    "text": "Incomplete Detection\nWe do not always observed all the events. Specially in Ecology!\n\n\n\nUneven sampling effort\nObserver errors/detectability\nAccessibility constraints\nPopulation density"
  },
  {
    "objectID": "slides/slides_9.html#incomplete-detection-1",
    "href": "slides/slides_9.html#incomplete-detection-1",
    "title": "Methods for Point referrenced Data",
    "section": "Incomplete Detection",
    "text": "Incomplete Detection\nImplications:\n\nIncomplete detection (points missing from our sample)\nBiased patterns may distort the true distribution.\nIf we account for these biases, then we can estimate the underlying pattern across the entire region by making use of covariates that are available for the whole area"
  },
  {
    "objectID": "slides/slides_9.html#overview-of-distance-sampling",
    "href": "slides/slides_9.html#overview-of-distance-sampling",
    "title": "Methods for Point referrenced Data",
    "section": "Overview of Distance Sampling",
    "text": "Overview of Distance Sampling\n\nDistance sampling is a family of related methods for estimating the abundance and spatial distribution of wild populations.\nDistance sampling is based on the idea that animals further away from observers are harder to detect than animals that are nearer.\n\n\n\n\nThis idea is implemented in the model as a detection function that depends on distance.\n\nAnimals at greater distances are harder to detect and the detection function therefore declines as distance increases."
  },
  {
    "objectID": "slides/slides_9.html#density-surface-models",
    "href": "slides/slides_9.html#density-surface-models",
    "title": "Methods for Point referrenced Data",
    "section": "Density surface models",
    "text": "Density surface models\nCoupling distance sampling data with spatial modelling allows maps of spatially varying density to be produced.\n\nTraditionally, this is achieved in a two-stages approach by (i) using a detectability point estimates to create an offset vector to (ii) use within GLM or GAM for count response data."
  },
  {
    "objectID": "slides/slides_9.html#density-surface-models-1",
    "href": "slides/slides_9.html#density-surface-models-1",
    "title": "Methods for Point referrenced Data",
    "section": "Density surface models",
    "text": "Density surface models\nCoupling distance sampling data with spatial modelling allows maps of spatially varying density to be produced.\n\nTraditionally, this is achieved in a two-stages approach by (i) using a detectability point estimates to create an offset vector to (ii) use within GLM or GAM for count response data.\nThis requires binning the data into counts based on some discretisation of space."
  },
  {
    "objectID": "slides/slides_9.html#density-surface-models-2",
    "href": "slides/slides_9.html#density-surface-models-2",
    "title": "Methods for Point referrenced Data",
    "section": "Density surface models",
    "text": "Density surface models\nCoupling distance sampling data with spatial modelling allows maps of spatially varying density to be produced.\n\nTraditionally, this is achieved in a two-stages approach by (i) using a detectability point estimates to create an offset vector to (ii) use within GLM or GAM for count response data.\nA major downside to this approach is the propagation of uncertainty from the detection model to the second-stage spatial model.\n\n\n\nThe goal: one-stage distance sampling model, simultaneously estimating the detectability and the spatial distribution of animals using a point process framework."
  },
  {
    "objectID": "slides/slides_9.html#thinned-point-process-1",
    "href": "slides/slides_9.html#thinned-point-process-1",
    "title": "Methods for Point referrenced Data",
    "section": "Thinned Point Process",
    "text": "Thinned Point Process\n\n\n\nThe LGCP is a flexible approach that can include spatial covariates to model the mean intensity and a mean-zero spatially structured random effect to account for unexplained heterogeneity not captured by the covariates.\n\nTo account for the imperfect detection of points we specify a thinning probability function \\[g(s) = \\mathbb{P}(\\text{a point at s is detected}|\\text{a point is at s})\\]\nA key property of LGCP is that a realisation of a point process with intensity \\(\\lambda(s)\\) that is thinned by probability function \\(g(s)\\), follows also a LGCP with intensity:\n\n\\[\n\\underbrace{\\tilde{\\lambda}(s)}_{\\text{observed process}} = \\underbrace{\\lambda(s)}_{\\text{true process}} \\times \\underbrace{g(s)}_{\\text{thinning probability}}\n\\]"
  },
  {
    "objectID": "slides/slides_9.html#thinned-point-process-2",
    "href": "slides/slides_9.html#thinned-point-process-2",
    "title": "Methods for Point referrenced Data",
    "section": "Thinned Point Process",
    "text": "Thinned Point Process\nLets visualize this on 1D: Intensity function with points"
  },
  {
    "objectID": "slides/slides_9.html#thinned-point-process-3",
    "href": "slides/slides_9.html#thinned-point-process-3",
    "title": "Methods for Point referrenced Data",
    "section": "Thinned Point Process",
    "text": "Thinned Point Process\nIntensity (density) function with points and transect locations"
  },
  {
    "objectID": "slides/slides_9.html#thinned-point-process-4",
    "href": "slides/slides_9.html#thinned-point-process-4",
    "title": "Methods for Point referrenced Data",
    "section": "Thinned Point Process",
    "text": "Thinned Point Process\n\n\nDetection function \\(\\color{red}{g(s)}\\)\nHere \\(\\color{red}{g(s) =1}\\) on the transects (at x = 10,30 and 50)."
  },
  {
    "objectID": "slides/slides_9.html#thinned-point-process-5",
    "href": "slides/slides_9.html#thinned-point-process-5",
    "title": "Methods for Point referrenced Data",
    "section": "Thinned Point Process",
    "text": "Thinned Point Process\n\nDetection function \\(\\color{red}{g(s)}\\) and detected points"
  },
  {
    "objectID": "slides/slides_9.html#thinned-point-process-6",
    "href": "slides/slides_9.html#thinned-point-process-6",
    "title": "Methods for Point referrenced Data",
    "section": "Thinned Point Process",
    "text": "Thinned Point Process"
  },
  {
    "objectID": "slides/slides_9.html#thinned-point-process-7",
    "href": "slides/slides_9.html#thinned-point-process-7",
    "title": "Methods for Point referrenced Data",
    "section": "Thinned Point Process",
    "text": "Thinned Point Process\n The detection function describes the probability \\(\\color{red}{p(s)}\\) that an point is detected"
  },
  {
    "objectID": "slides/slides_9.html#thinned-point-process-8",
    "href": "slides/slides_9.html#thinned-point-process-8",
    "title": "Methods for Point referrenced Data",
    "section": "Thinned Point Process",
    "text": "Thinned Point Process"
  },
  {
    "objectID": "slides/slides_9.html#thinned-point-process-9",
    "href": "slides/slides_9.html#thinned-point-process-9",
    "title": "Methods for Point referrenced Data",
    "section": "Thinned Point Process",
    "text": "Thinned Point Process\n\nObservations are from a thinned Poisson process with intensity \\(\\lambda(s) \\color{red}{p(s)}\\)"
  },
  {
    "objectID": "slides/slides_9.html#detection-function",
    "href": "slides/slides_9.html#detection-function",
    "title": "Methods for Point referrenced Data",
    "section": "Detection Function",
    "text": "Detection Function\n\n\n\n\nStandard distance sampling approaches specify \\(g(s)\\) as a function that declines with increasing distance\n\nhorizontal distance to the observer for point transects\nperpendicular distance to the transect line for line transects"
  },
  {
    "objectID": "slides/slides_9.html#detection-function-1",
    "href": "slides/slides_9.html#detection-function-1",
    "title": "Methods for Point referrenced Data",
    "section": "Detection Function",
    "text": "Detection Function\n\n\n\n\nStandard distance sampling approaches specify \\(g(s)\\) as a function that declines with increasing distance\n\nhorizontal distance to the observer for point transects\nperpendicular distance to the transect line for line transects"
  },
  {
    "objectID": "slides/slides_9.html#detection-function-2",
    "href": "slides/slides_9.html#detection-function-2",
    "title": "Methods for Point referrenced Data",
    "section": "Detection Function",
    "text": "Detection Function\n\n\n\n\nStandard distance sampling approaches specify \\(g(s)\\) as a function that declines with increasing distance\n\nhorizontal distance to the observer for point transects\nperpendicular distance to the transect line for line transects\n\nThe thinning probability function is specified as a parametric family of functions.\n\n\n\nHalf-normal: \\(g(\\mathbf{s}|\\sigma) = \\exp(-0.5 (d(\\mathbf{s})/\\sigma)^2)\\)\nHazard-rate :\\(g(\\mathbf{s}|\\sigma) = 1 - \\exp(-(d(\\mathbf{s})/\\sigma)^{-1})\\)"
  },
  {
    "objectID": "slides/slides_9.html#detection-function-3",
    "href": "slides/slides_9.html#detection-function-3",
    "title": "Methods for Point referrenced Data",
    "section": "Detection Function",
    "text": "Detection Function\n\n\n\n\nStandard distance sampling approaches specify \\(g(s)\\) as a function that declines with increasing distance\n\nhorizontal distance to the observer for point transects\nperpendicular distance to the transect line for line transects\n\nThe thinning probability function is specified as a parametric family of functions.\nThe thinned-LGCP likelihood is given by:\n\n\\[\n\\pi(\\mathbf{s_1},\\ldots,\\mathbf{s_m}) = \\exp\\left( |\\Omega| - \\int_{\\mathbf{s}\\in\\Omega}\\lambda(s)g(s)\\text{d}s \\right) \\prod_{i=1}^m \\lambda(\\mathbf{s}_i)g(\\mathbf{s}_i)\n\\]\n\nTo make \\(g(s)\\) and \\(\\lambda(s)\\) identifiable, we assume intensity is constant with respect to distance from the observer.\n\nIn practice this means we assume animals are uniformly distributed with respect to distance from the line"
  },
  {
    "objectID": "slides/slides_9.html#putting-all-the-pieces-together",
    "href": "slides/slides_9.html#putting-all-the-pieces-together",
    "title": "Methods for Point referrenced Data",
    "section": "Putting all the pieces together",
    "text": "Putting all the pieces together\n\n\n\n\n\n\n\n\n\nThe true point pattern \\(Y = \\mathbf{s}_1,\\ldots,\\mathbf{s}_n\\) are a realization of a Point process with intensity \\(\\lambda(s)\\)"
  },
  {
    "objectID": "slides/slides_9.html#putting-all-the-pieces-together-1",
    "href": "slides/slides_9.html#putting-all-the-pieces-together-1",
    "title": "Methods for Point referrenced Data",
    "section": "Putting all the pieces together",
    "text": "Putting all the pieces together\n\n\n\n\n\n\n\n\n\nThe true point pattern \\(Y = \\mathbf{s}_1,\\ldots,\\mathbf{s}_n\\) are a realization of a Point process with intensity \\(\\lambda(s)\\)\nWe design a sampling survey to collect the data along transects"
  },
  {
    "objectID": "slides/slides_9.html#putting-all-the-pieces-together-2",
    "href": "slides/slides_9.html#putting-all-the-pieces-together-2",
    "title": "Methods for Point referrenced Data",
    "section": "Putting all the pieces together",
    "text": "Putting all the pieces together\n\n\n\n\n\n\n\n\n\nThe true point pattern \\(Y = \\mathbf{s}_1,\\ldots,\\mathbf{s}_n\\) are a realization of a Point process with intensity \\(\\lambda(s)\\)\nWe design a sampling survey to collect the data along transects\ndetected points are generated from the thinned PP with intensity \\(\\color{red}{\\tilde{\\lambda}(s)}= \\lambda(s)\\color{red}{g(d(s))}\\)\n\nThe log intensity \\(\\log \\color{red}{\\tilde{\\lambda}(s)} = \\overbrace{\\log \\lambda (s)}^{\\mathbf{x}'\\beta + \\xi(s)} + \\overbrace{\\log \\color{red}{g(d(s))}}^{-0.5~d(\\mathbf{s})^2\\sigma^{-2}}\\)"
  },
  {
    "objectID": "slides/slides_9.html#putting-all-the-pieces-together-3",
    "href": "slides/slides_9.html#putting-all-the-pieces-together-3",
    "title": "Methods for Point referrenced Data",
    "section": "Putting all the pieces together",
    "text": "Putting all the pieces together\n\n\n\n\n\n\n\n\n\nThe true point pattern \\(Y = \\mathbf{s}_1,\\ldots,\\mathbf{s}_n\\) are a realization of a Point process with intensity \\(\\lambda(s)\\)\nWe design a sampling survey to collect the data along transects\ndetected points are generated from the thinned PP with intensity \\(\\color{red}{\\tilde{\\lambda}(s)}= \\lambda(s)\\color{red}{g(d(s))}\\)\n\nThe log intensity \\(\\log \\color{red}{\\tilde{\\lambda}(s)} = \\overbrace{\\log \\lambda (s)}^{\\mathbf{x}'\\beta + \\xi(s)} + \\overbrace{\\log \\color{red}{g(d(s))}}^{-0.5~d(\\mathbf{s})^2\\sigma^{-2}}\\)\nThe encounter rate, i.e. the number of observed animals within a distance \\(W\\) follows \\(m \\sim \\text{Poisson} \\left(\\int_0^W \\tilde{\\lambda}(d)\\text{d}d\\right)\\)\n\n\n\n\nThe pdf of detected distances is \\(\\pi(d_1,\\ldots,d_m|m) \\propto \\prod_{i=1}^m\\dfrac{\\tilde{\\lambda}(d_i)}{\\int_0^W \\tilde{\\lambda}(d)\\text{d}d}\\)"
  },
  {
    "objectID": "slides/slides_9.html#putting-all-the-pieces-together-4",
    "href": "slides/slides_9.html#putting-all-the-pieces-together-4",
    "title": "Methods for Point referrenced Data",
    "section": "Putting all the pieces together",
    "text": "Putting all the pieces together\n\n\n\n\n\n\n\n\n\nThe true point pattern \\(Y = \\mathbf{s}_1,\\ldots,\\mathbf{s}_n\\) are a realization of a Point process with intensity \\(\\lambda(s)\\)\nWe design a sampling survey to collect the data along transects\ndetected points are generated from the thinned PP with intensity \\(\\color{red}{\\tilde{\\lambda}(s)}= \\lambda(s)\\color{red}{g(d(s))}\\)\n\nThe log intensity \\(\\log \\color{red}{\\tilde{\\lambda}(s)} = \\overbrace{\\log \\lambda (s)}^{\\mathbf{x}'\\beta + \\xi(s)} + \\overbrace{\\log \\color{red}{g(d(s))}}^{-0.5~d(\\mathbf{s})^2\\sigma^{-2}}\\)\nThe encounter rate, i.e. the number of observed animals within a distance \\(W\\) follows \\(m \\sim \\text{Poisson} \\left(\\int_0^W \\tilde{\\lambda}(d)\\text{d}d\\right)\\)\n\n\n\n\nThe pdf of detected distances is \\(\\pi(d_1,\\ldots,d_m|m) \\propto \\prod_{i=1}^m\\dfrac{ g(d_i)}{\\int_o^W g(d) \\text{d}d}\\) if \\(\\color{red}{\\tilde{\\lambda}(d_i)} = \\lambda \\color{red}{g(d_i)}\\)"
  },
  {
    "objectID": "slides/slides_9.html#putting-all-the-pieces-together-5",
    "href": "slides/slides_9.html#putting-all-the-pieces-together-5",
    "title": "Methods for Point referrenced Data",
    "section": "Putting all the pieces together",
    "text": "Putting all the pieces together\n\nIf the strips width ( \\(2W\\) ) is narrow compared to study region (\\(\\Omega\\)) we can treat them as lines.\nDefine the Poisson process likelihood along the kronecker spaces (line \\(\\times\\) distance)\nAccounting for imperfect detection the thinned Poisson process model on (space, distance) along the transects becomes:\n\n\\[\n\\begin{aligned}\n\\log \\tilde{\\lambda}(s,\\text{distance}) &= \\overbrace{\\mathbf{x}'\\beta + \\xi(s)}^{\\log \\lambda(s)} + \\log \\mathbb{P}(\\text{detection at }s|\\text{distance},\\sigma) + \\log(2)\\\\\n\\mathbb{P}(\\text{detection}) &=1-\\exp\\left(-\\frac{\\sigma}{\\text{distance}}\\right)\n\\end{aligned}\n\\]\n\n\nHere \\(\\log 2\\) accounts for the two-sided detection.\nTypically \\(\\mathbb{P}(distance)\\) is a non-linear function, that is where inlabru can help via a Fixed point iteration scheme (further details available in this vignette)\nwe define \\(\\log (\\sigma)\\) as a latent Gaussian variable and iteratively linearise it."
  },
  {
    "objectID": "slides/slides_9.html#example-dolphins-in-the-gulf-of-mexico",
    "href": "slides/slides_9.html#example-dolphins-in-the-gulf-of-mexico",
    "title": "Methods for Point referrenced Data",
    "section": "Example: Dolphins in the Gulf of Mexico",
    "text": "Example: Dolphins in the Gulf of Mexico\nIn the next example, we will explore data from a combination of several NOAA shipboard surveys conducted on pan-tropical spotted dolphins in the Gulf of Mexico.\n\nA total of 47 observations of groups of dolphins were detected. The group size was recorded, as well as the Beaufort sea state at the time of the observation.\nTransect width is 16 km, i.e. maximal detection distance 8 km (transect half-width 8 km)."
  },
  {
    "objectID": "slides/slides_9.html#step-1-define-the-spde-representation-the-mesh",
    "href": "slides/slides_9.html#step-1-define-the-spde-representation-the-mesh",
    "title": "Methods for Point referrenced Data",
    "section": "Step 1: Define the SPDE representation: The mesh",
    "text": "Step 1: Define the SPDE representation: The mesh\nFirst, we need to create the mesh used to approximate the random field. We can either:\n\nUse a pre-define sf boundary and specify this directly into the mesh construction via the fm_mesh_2d function\n\n\nlibrary(fmesher)\n\nmesh_1 = fm_mesh_2d(boundary = mexdolphin$ppoly,\n                    max.edge = c(30, 150),\n                    cutoff = 15,\n                    crs = fm_crs(mexdolphin$points))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmax.edge for maximum triangle edge lengths\ncutoff to avoid overly small triangles in clustered areas"
  },
  {
    "objectID": "slides/slides_9.html#step-1-define-the-spde-representation-the-spde",
    "href": "slides/slides_9.html#step-1-define-the-spde-representation-the-spde",
    "title": "Methods for Point referrenced Data",
    "section": "Step 1: Define the SPDE representation: The SPDE",
    "text": "Step 1: Define the SPDE representation: The SPDE\nWe use the inla.spde2.pcmatern to define the SPDE model using PC priors through the following probability statements\n\n\n\n\\(P(\\rho &lt; 50) = 0.1\\)\n\\(P(\\sigma &gt; 2) = 0.1\\)\n\n\n\nspde_model =  inla.spde2.pcmatern(\n  mexdolphin$mesh,\n  prior.sigma = c(2, 0.1),\n  prior.range = c(50, 0.1)\n)"
  },
  {
    "objectID": "slides/slides_9.html#step-2-define-the-detection-function",
    "href": "slides/slides_9.html#step-2-define-the-detection-function",
    "title": "Methods for Point referrenced Data",
    "section": "Step 2: Define the Detection function",
    "text": "Step 2: Define the Detection function\nWe start by plotting the distances and histogram of frequencies in distance intervals.\n\nThen, we need to define a half-normal detection probability function. This must take distance as its first argument and the linear predictor of the sigma parameter as its second:\n\n# define detection function\nhn &lt;- function(distance, sigma) {\n  exp(-0.5 * (distance / sigma)^2)\n}"
  },
  {
    "objectID": "slides/slides_9.html#example-dolphins-in-the-gulf-of-mexico-1",
    "href": "slides/slides_9.html#example-dolphins-in-the-gulf-of-mexico-1",
    "title": "Methods for Point referrenced Data",
    "section": "Example: Dolphins in the Gulf of Mexico",
    "text": "Example: Dolphins in the Gulf of Mexico\n\n\nThe LGCP Model\n\\[\n\\begin{aligned}\np(\\mathbf{y} | \\lambda)  & \\propto \\exp \\left( -\\int_\\Omega \\lambda(\\mathbf{s}) p(\\mathbf{s}) \\mathrm{d}\\mathbf{s} \\right) \\prod_{i=1}^n \\lambda(\\mathbf{s}_i) p(\\mathbf{s}_i)) \\\\\n\\eta(s) &  = \\color{#FF6B6B}{\\boxed{\\beta_0}} + \\color{#FF6B6B}{\\boxed{ \\omega(s)}} + \\color{#FF6B6B}{\\boxed{ \\log p(s)}} \\\\\n\\end{aligned}\n\\]\nThe code\n\n# define model component\ncmp = ~ Intercept(1) + \n  space(main = geometry, model = spde_model) +\n  sigma(1,\n    prec.linear = 1,\n    marginal = bru_mapper_marginal(qexp, pexp, dexp, rate = 1 / 8)\n  ) \n\n# define model predictor\neta  = geometry + distance ~ space +\n  log(hn(distance, sigma)) +\n  Intercept + log(2)\n\n# build the observation model\nlik = bru_obs(\"cp\",\n              formula = eta,\n              data = mexdolphin$points,\n              ips = ips)\n# fit the model\nfit = bru(cmp, lik)\n\n\nThe integration scheme\n\n# build integration scheme\ndistance_domain &lt;-  fm_mesh_1d(seq(0, 8,\n                              length.out = 30))\nips = fm_int(list(geometry = mesh,\n                  distance = distance_domain),\n             samplers = mexdolphin$samplers)\n\n\n\n\n\n\n\n\n\n\n\n\nThe samplers in this dataset are lines, not polygons, so we need to tell inlabru about the strip half-width, W, which in the case of these data is 8.\nTo control the prior distribution for the \\(\\sigma\\) parameter, we use a transformation mapper that converts a latent variable into an exponentially distributed variable with expectation 8\nThe marginal argument in the sigma component specifies the transformation function taking N(0,1) to Exponential(1/8)."
  },
  {
    "objectID": "slides/slides_9.html#example-dolphins-in-the-gulf-of-mexico-2",
    "href": "slides/slides_9.html#example-dolphins-in-the-gulf-of-mexico-2",
    "title": "Methods for Point referrenced Data",
    "section": "Example: Dolphins in the Gulf of Mexico",
    "text": "Example: Dolphins in the Gulf of Mexico\n\n\nThe LGCP Model\n\\[\n\\begin{aligned}\np(\\mathbf{y} | \\lambda)  & \\propto \\exp \\left( -\\int_\\Omega \\lambda(\\mathbf{s}) p(\\mathbf{s}) \\mathrm{d}\\mathbf{s} \\right) \\prod_{i=1}^n \\lambda(\\mathbf{s}_i) p(\\mathbf{s}_i)) \\\\\n\\color{#FF6B6B}{\\boxed{\\eta(s)}} &  = \\color{#FF6B6B}{\\boxed{\\beta_0 +  \\omega(s) +  \\log p(s)}}\\\\\n\\end{aligned}\n\\]\nThe code\n\n# define model component\ncmp = ~ Intercept(1) + \n  space(main = geometry, model = spde_model) +\n  sigma(1,\n    prec.linear = 1,\n    marginal = bru_mapper_marginal(qexp, pexp, dexp, rate = 1 / 8)\n  ) \n\n# define model predictor\neta  = geometry + distance ~ space +\n  log(hn(distance, sigma)) +\n  Intercept + log(2)\n\n# build the observation model\nlik = bru_obs(\"cp\",\n              formula = eta,\n              data = mexdolphin$points,\n              ips = ips)\n\n# fit the model\nfit = bru(cmp, lik)\n\n\nThe integration scheme\n\n# build integration scheme\ndistance_domain &lt;-  fm_mesh_1d(seq(0, 8,\n                              length.out = 30))\nips = fm_int(list(geometry = mesh,\n                  distance = distance_domain),\n             samplers = mexdolphin$samplers)\n\n\n\n\n\n\n\n\n\n\n\n\nwe need an offset due to the unknown direction of the detections"
  },
  {
    "objectID": "slides/slides_9.html#example-dolphins-in-the-gulf-of-mexico-3",
    "href": "slides/slides_9.html#example-dolphins-in-the-gulf-of-mexico-3",
    "title": "Methods for Point referrenced Data",
    "section": "Example: Dolphins in the Gulf of Mexico",
    "text": "Example: Dolphins in the Gulf of Mexico\n\n\nThe LGCP Model\n\\[\n\\begin{aligned}\n\\color{#FF6B6B}{\\boxed{p(\\mathbf{y} | \\lambda)}} & \\propto \\exp \\left( -\\int_\\Omega \\lambda(\\mathbf{s}) p(\\mathbf{s}) \\mathrm{d}\\mathbf{s} \\right) \\prod_{i=1}^n \\lambda(\\mathbf{s}_i) p(\\mathbf{s}_i)) \\\\\n\\eta(s) &  = \\beta_0 +  \\omega(s) +  \\log p(s) \\\\\n\\end{aligned}\n\\]\nThe code\n\n# define model component\ncmp = ~ Intercept(1) + \n  space(main = geometry, model = spde_model) +\n  sigma(1,\n    prec.linear = 1,\n    marginal = bru_mapper_marginal(qexp, pexp, dexp, rate = 1 / 8)\n  ) \n\n# define model predictor\neta  = geometry + distance ~ space +\n  log(hn(distance, sigma)) +\n  Intercept + log(2)\n\n# build the observation model\nlik = bru_obs(\"cp\",\n              formula = eta,\n              data = mexdolphin$points,\n              ips = ips)\n\n# fit the model\nfit = bru(cmp, lik)\n\n\nThe integration scheme\n\n# build integration scheme\ndistance_domain &lt;-  fm_mesh_1d(seq(0, 8,\n                              length.out = 30))\nips = fm_int(list(geometry = mesh,\n                  distance = distance_domain),\n             samplers = mexdolphin$samplers)"
  },
  {
    "objectID": "slides/slides_9.html#example-dolphins-in-the-gulf-of-mexico-4",
    "href": "slides/slides_9.html#example-dolphins-in-the-gulf-of-mexico-4",
    "title": "Methods for Point referrenced Data",
    "section": "Example: Dolphins in the Gulf of Mexico",
    "text": "Example: Dolphins in the Gulf of Mexico\n\n\nThe LGCP Model\n\\[\n\\begin{aligned}\n\\color{#FF6B6B}{\\boxed{p(\\mathbf{y} | \\lambda)}} & \\propto \\exp \\left( -\\int_\\Omega \\lambda(\\mathbf{s}) p(\\mathbf{s}) \\mathrm{d}\\mathbf{s} \\right) \\prod_{i=1}^n \\lambda(\\mathbf{s}_i) p(\\mathbf{s}_i)) \\\\\n\\eta(s) &  = \\beta_0 +  \\omega(s) +  \\log p(s) \\\\\n\\end{aligned}\n\\]\nThe code\n\n# define model component\ncmp = ~ Intercept(1) + \n  space(main = geometry, model = spde_model) +\n  sigma(1,\n    prec.linear = 1,\n    marginal = bru_mapper_marginal(qexp, pexp, dexp, rate = 1 / 8)\n  ) \n\n# define model predictor\neta  = geometry + distance ~ space +\n  log(hn(distance, sigma)) +\n  Intercept + log(2)\n\n# build the observation model\nlik = bru_obs(\"cp\",\n              formula = eta,\n              data = mexdolphin$points,\n              ips = ips)\n\n# fit the model\nfit = bru(cmp, lik)\n\n\nThe integration scheme\n\n# build integration scheme\ndistance_domain &lt;-  fm_mesh_1d(seq(0, 8,\n                              length.out = 30))\nips = fm_int(list(geometry = mesh,\n                  distance = distance_domain),\n             samplers = mexdolphin$samplers)"
  },
  {
    "objectID": "slides/slides_9.html#results-posterior-summaries",
    "href": "slides/slides_9.html#results-posterior-summaries",
    "title": "Methods for Point referrenced Data",
    "section": "Results: posterior summaries",
    "text": "Results: posterior summaries\n\n\nWe can to obtain posterior summaries of the model parameters.\n\n\n\n\n\n\n\n\n\nmean\n0.025quant\n0.975quant\n\n\n\n\nIntercept\n−8.41\n−9.47\n−7.62\n\n\nsigma\n−0.05\n−0.46\n0.36\n\n\nRange for space\n131.74\n41.79\n320.28\n\n\nStdev for space\n1.17\n0.72\n1.78\n\n\n\n\n\n\n\n\nWe can also to plot the posterior density of the Matérn field parameters\n\nspde.posterior(fit, \"space\", what = \"range\") %&gt;% plot()"
  },
  {
    "objectID": "slides/slides_9.html#results-posterior-summaries-1",
    "href": "slides/slides_9.html#results-posterior-summaries-1",
    "title": "Methods for Point referrenced Data",
    "section": "Results: posterior summaries",
    "text": "Results: posterior summaries\n\n\nWe can to obtain posterior summaries of the model parameters.\n\n\n\n\n\n\n\n\n\nmean\n0.025quant\n0.975quant\n\n\n\n\nIntercept\n−8.41\n−9.47\n−7.62\n\n\nsigma\n−0.05\n−0.46\n0.36\n\n\nRange for space\n131.74\n41.79\n320.28\n\n\nStdev for space\n1.17\n0.72\n1.78\n\n\n\n\n\n\n\n\nWe can also to plot the posterior density of the Matérn field parameters\n\nspde.posterior(fit, \"space\", what = \"log.variance\") %&gt;% plot()"
  },
  {
    "objectID": "slides/slides_9.html#results-predicted-densities",
    "href": "slides/slides_9.html#results-predicted-densities",
    "title": "Methods for Point referrenced Data",
    "section": "Results: predicted densities",
    "text": "Results: predicted densities\n\n\nTo map the spatial intensity we first need to define a grid of points where we want to predict.\n\nWe do this using the function fm_pixel() which creates a regular grid of points covering the mesh\nThen, we use the predict function which takes as input\n\nthe fitted model (fit)\nthe prediction points (pxl)\nthe model components we want to predict (e.g., \\(e^{\\beta_0 + \\xi(s)}\\))\n\nTo plot this you can use ggplot and add a gg() layer with your output of interest (E.g., pr.int$spatial)\n\n\n\nlibrary(patchwork)\npxl &lt;- fm_pixels(mesh, dims = c(200, 100), mask = mexdolphin$ppoly)\npr.int &lt;- predict(fit, pxl, ~ data.frame(spatial = space,\n                                      loglambda = Intercept + space,\n                                      lambda = exp(Intercept + space)))"
  },
  {
    "objectID": "slides/slides_9.html#results-predicted-densities-1",
    "href": "slides/slides_9.html#results-predicted-densities-1",
    "title": "Methods for Point referrenced Data",
    "section": "Results: predicted densities",
    "text": "Results: predicted densities\nWe can also use the predict function to predict the detection probabilities:\n\ndistdf &lt;- data.frame(distance = seq(0, 8, length.out = 100))\ndfun &lt;- predict(fit, distdf, ~ hn(distance, sigma))\nplot(dfun)"
  },
  {
    "objectID": "slides/slides_9.html#results-data-level-prediction",
    "href": "slides/slides_9.html#results-data-level-prediction",
    "title": "Methods for Point referrenced Data",
    "section": "Results: Data level prediction",
    "text": "Results: Data level prediction\n47 groups were seen. How many would be seen along the transects under perfect detection?\n\npredpts_transect &lt;- fm_int(mexdolphin$mesh, mexdolphin$samplers)\nLambda_transect &lt;- predict(fit,\n                           predpts_transect,~ 16 * sum(weight * exp(space + Intercept)))\n\n\n\n\n\n\n\n\n\nmean\nsd\nq0.025\nq0.5\nq0.975\nmedian\nsd.mc_std_err\nmean.mc_std_err\n\n\n\n\n96.15\n28.65\n57.27\n88.26\n160.56\n88.26\n2.24\n3.31\n\n\n\n\n\n\n\nHow many would be seen under perfect detection across the whole study area (i.e., the mean expected number of dolphins)?\n\npredpts &lt;- fm_int(mexdolphin$mesh, mexdolphin$ppoly)\nLambda &lt;- predict(fit, predpts, ~ sum(weight * exp(space + Intercept)))\n\n\n\n\n\n\n\n\n\nmean\nsd\nq0.025\nq0.5\nq0.975\nmedian\nsd.mc_std_err\nmean.mc_std_err\n\n\n\n\n325.12\n95.72\n199.67\n302.79\n557.35\n302.79\n8.53\n11.28"
  },
  {
    "objectID": "slides/slides_9.html#results-predicted-expected-counts",
    "href": "slides/slides_9.html#results-predicted-expected-counts",
    "title": "Methods for Point referrenced Data",
    "section": "Results: predicted expected counts",
    "text": "Results: predicted expected counts\nWhat’s the predictive distribution of group counts?\nWe can also get Monte Carlo samples for the expected number of dolphins as follows:\n\n\n\nNs &lt;- seq(50, 450, by = 1)\n\nNest &lt;- predict(fit, predpts,\n  ~ data.frame(\n    N = Ns,\n    density = dpois(\n      Ns,\n      lambda = sum(weight * exp(space + Intercept))\n    )\n  ),\n  n.samples = 2000\n)\n\nNest &lt;- dplyr::bind_rows(\n  cbind(Nest, Method = \"Posterior\"),\n  data.frame(\n    N = Nest$N,\n    mean = dpois(Nest$N, lambda = Lambda$mean),\n    mean.mc_std_err = 0,\n    Method = \"Plugin\"\n  )\n)"
  },
  {
    "objectID": "slides/slides_9.html#summary-of-points",
    "href": "slides/slides_9.html#summary-of-points",
    "title": "Methods for Point referrenced Data",
    "section": "Summary of points",
    "text": "Summary of points\n\n\nPoint process are a stochastic processes that describe the locations where events occur\nUnlike geostatistical data where the locations are fixed, here the locations have a stochastic nature the locations are our data!\nCSR as a realisation of an HPP that describe events that occur independently and uniformly at random across space, such that the number of events in any region follows a Poisson distribution with mean \\(\\lambda \\times \\text{area}\\).\nK functions can be used to distinguish between CSR, spatial clustering or regular point patterns."
  },
  {
    "objectID": "slides/slides_9.html#summary-of-points-1",
    "href": "slides/slides_9.html#summary-of-points-1",
    "title": "Methods for Point referrenced Data",
    "section": "Summary of points",
    "text": "Summary of points\n\n\nIPP allows the intensity of the point process to vary across space through spatially varying covariates.\nNumerical integration schemes are required to estimate the parameters of an IPP\nLGCP are a double stochastic process that extend IPP models by allowing the intensity function to vary spatially according to a structured spatial random effect\nThinned Point Processes offer improved accuracy by accounting the observational process of how individuals are detected"
  },
  {
    "objectID": "slides/slides_9.html#the-end",
    "href": "slides/slides_9.html#the-end",
    "title": "Methods for Point referrenced Data",
    "section": "The End",
    "text": "The End\n\n\n“It is that range of biodiversity that we must care for — the whole thing — rather than just one or two stars”\nSir David Attenborough"
  },
  {
    "objectID": "slides/slides_7.html#section",
    "href": "slides/slides_7.html#section",
    "title": "Modelling Areal Data",
    "section": "",
    "text": "“doing what little one can to increase the general stock of knowledge is as respectable an object of life, as one can in any likelihood pursue.”\nCharles Darwin"
  },
  {
    "objectID": "slides/slides_7.html#section-1",
    "href": "slides/slides_7.html#section-1",
    "title": "Modelling Areal Data",
    "section": "",
    "text": "Space is inherent to all ecological processes, influencing dynamics such as migration, dispersal, and species interactions. A primary goal in ecology is to understand how these processes shape species distributions and dynamics across space."
  },
  {
    "objectID": "slides/slides_7.html#spatial-data-1",
    "href": "slides/slides_7.html#spatial-data-1",
    "title": "Modelling Areal Data",
    "section": "Spatial data",
    "text": "Spatial data\n\nMany natural processes take place in space\nSpatial data are data which have any form of geographical information attached to them.\nThe emergence of modelling frameworks for spatial data in ecology and environmental sciences has been facilitated by the rise of new technologies."
  },
  {
    "objectID": "slides/slides_7.html#spatial-data-2",
    "href": "slides/slides_7.html#spatial-data-2",
    "title": "Modelling Areal Data",
    "section": "Spatial data",
    "text": "Spatial data\nThe overall goal of any piece of spatial analysis is to understand the spatial patterns in our data. This could involve:\n\n\nEstimating differences in mean, variance, or some other summary statistic over space.\nPredicting the value at some unobserved location.\nIdentifying hotspots with high (or low) values compared to the rest of the region.\n\n\n\naims also vary with different types of spatial data…"
  },
  {
    "objectID": "slides/slides_7.html#types-of-spatial-data",
    "href": "slides/slides_7.html#types-of-spatial-data",
    "title": "Modelling Areal Data",
    "section": "Types of spatial data",
    "text": "Types of spatial data\nDiscrete space:\n\nData on a spatial grid (areal data)\n\nContinuous space:\n\nGeostatistical (geo-referenced) data\nSpatial point pattern data\n\nThe components of the models we will cover are used to reflect spatial dependence structures in discrete and continuous space."
  },
  {
    "objectID": "slides/slides_7.html#spatial-data-structures",
    "href": "slides/slides_7.html#spatial-data-structures",
    "title": "Modelling Areal Data",
    "section": "Spatial data structures",
    "text": "Spatial data structures\nWe can distinguish three types of spatial data structures\n\n\nAreal data\n\n\n\nMap of bird conservation regions (BCRs) showing the proportion of bird species within each region showing a declining trend\n\n\n\nGeostatistical data\n\n\n\nScotland river temperature monitoring network\n\n\n\nPoint pattern data\n\n\n\nOccurrence records of four ungulate species in Tibet,"
  },
  {
    "objectID": "slides/slides_7.html#spatial-data-structures-1",
    "href": "slides/slides_7.html#spatial-data-structures-1",
    "title": "Modelling Areal Data",
    "section": "Spatial data structures",
    "text": "Spatial data structures\nWe can distinguish three types of spatial data structures\n\n\nAreal data\nIn areal data our measurements are summarised across a set of discrete, non-overlapping spatial units.\n\n\n\nMap of bird conservation regions (BCRs) showing the proportion of bird species within each region showing a declining trend\n\n\n\nGeostatistical data\n\n\n\nScotland river temperature monitoring network\n\n\n\nPoint pattern data\n\n\n\nOccurrence records of four ungulate species in Tibet"
  },
  {
    "objectID": "slides/slides_7.html#spatial-data-structures-2",
    "href": "slides/slides_7.html#spatial-data-structures-2",
    "title": "Modelling Areal Data",
    "section": "Spatial data structures",
    "text": "Spatial data structures\nWe can distinguish three types of spatial data structures\n\n\nAreal data\n\n\n\nMap of bird conservation regions (BCRs) showing the proportion of bird species within each region showing a declining trend\n\n\n\nGeostatistical data\nIn geostatistical data, measurements of a continuous process are taken at a set of fixed locations.\n\n\n\nScotland river temperature monitoring network\n\n\n\nPoint pattern data\n\n\n\nOccurrence records of four ungulate species in Tibet,"
  },
  {
    "objectID": "slides/slides_7.html#spatial-data-structures-3",
    "href": "slides/slides_7.html#spatial-data-structures-3",
    "title": "Modelling Areal Data",
    "section": "Spatial data structures",
    "text": "Spatial data structures\nWe can distinguish three types of spatial data structures\n\n\nAreal data\n\n\n\nMap of bird conservation regions (BCRs) showing the proportion of bird species within each region showing a declining trend\n\n\n\nGeostatistical data\n\n\n\nScotland river temperature monitoring network\n\n\n\nPoint pattern data\nIn point pattern data we record the locations where events occur (e.g. trees in a forest, earthquakes) and the coordinates of such occurrences are our data.\n\n\n\nOccurrence records of four ungulate species in Tibet,"
  },
  {
    "objectID": "slides/slides_7.html#spatial-scales",
    "href": "slides/slides_7.html#spatial-scales",
    "title": "Modelling Areal Data",
    "section": "Spatial Scales",
    "text": "Spatial Scales\nScales describe the spatial and temporal dimensions at which an ecological or environmental process occurs. The spatial scale is often described by:\n\n\ngrain or spatial resolution: finest spatial unit of measurement for a given process\nextent: total length or area of the study\n\n\n\nThere is typically a trade-off between the grain and the extent, mainly due to practicality.\nThe scale at which an ecological or environmental process occur can have a major impact on the interpretation of our analysis."
  },
  {
    "objectID": "slides/slides_7.html#modifiable-areal-unit-problem--maup",
    "href": "slides/slides_7.html#modifiable-areal-unit-problem--maup",
    "title": "Modelling Areal Data",
    "section": "Modifiable Areal Unit Problem -MAUP",
    "text": "Modifiable Areal Unit Problem -MAUP\n\nThe spatial scale of analysis is crucial in ecological and environmental studies, as it can significantly influence the patterns we observe and the conclusions we draw.\nChanging the grain of the study (while holding the extent constant) can introduce bias and uncertainty in the observed ecological patterns. E.g, when aggregating individual level data\nmodifiable areal unit relates to the assumption that a relationships exists one level of aggregation does not necessarily hold at another level of aggregation"
  },
  {
    "objectID": "slides/slides_7.html#modifiable-areal-unit-problem--maup-1",
    "href": "slides/slides_7.html#modifiable-areal-unit-problem--maup-1",
    "title": "Modelling Areal Data",
    "section": "Modifiable Areal Unit Problem -MAUP",
    "text": "Modifiable Areal Unit Problem -MAUP\n\nThe spatial scale of analysis is crucial in ecological and environmental studies, as it can significantly influence the patterns we observe and the conclusions we draw.\nChanging the grain of the study (while holding the extent constant) can introduce bias and uncertainty in the observed ecological patterns. E.g, when aggregating individual level data\nmodifiable areal unit relates to the assumption that a relationships exists one level of aggregation does not necessarily hold at another level of aggregation"
  },
  {
    "objectID": "slides/slides_7.html#modifiable-areal-unit-problem--maup-2",
    "href": "slides/slides_7.html#modifiable-areal-unit-problem--maup-2",
    "title": "Modelling Areal Data",
    "section": "Modifiable Areal Unit Problem -MAUP",
    "text": "Modifiable Areal Unit Problem -MAUP\n\nThe spatial scale of analysis is crucial in ecological and environmental studies, as it can significantly influence the patterns we observe and the conclusions we draw.\nChanging the grain of the study (while holding the extent constant) can introduce bias and uncertainty in the observed ecological patterns. E.g, when aggregating individual level data\nmodifiable areal unit relates to the assumption that a relationships exists one level of aggregation does not necessarily hold at another level of aggregation"
  },
  {
    "objectID": "slides/slides_7.html#ecological-fallacy",
    "href": "slides/slides_7.html#ecological-fallacy",
    "title": "Modelling Areal Data",
    "section": "Ecological fallacy",
    "text": "Ecological fallacy\n\nThe assumption that a relationships exists one level of aggregation does not necessarily hold at another level of aggregation.\nIn ecology this is known as ecological fallacy, which arises when incorrect inferences about individual sample units are made based on aggregated\nWhen individual-level data is grouped, the variability within units is lost, which can mask finer-scale patterns or exaggerate certain trends"
  },
  {
    "objectID": "slides/slides_7.html#spatial-modelling-why-necessary",
    "href": "slides/slides_7.html#spatial-modelling-why-necessary",
    "title": "Modelling Areal Data",
    "section": "Spatial modelling: why necessary?",
    "text": "Spatial modelling: why necessary?\nTobler’s first law of geography states that:\n\nEverything is related to everything else, but near things are more related than distant things”\n\nStandard models assume independent observations\n\nSpatial data are often not independent\nTwo nearby observations are similar \\(\\rightarrow\\) not providing independent info\n\nSpatial models include special components to explicitly model dependence."
  },
  {
    "objectID": "slides/slides_7.html#spatial-dependence",
    "href": "slides/slides_7.html#spatial-dependence",
    "title": "Modelling Areal Data",
    "section": "Spatial dependence",
    "text": "Spatial dependence\n\nInference and prediction are essential in ecology and conservation, but spatial dependency can complicate statistical analysis.\nUnderstanding how spatial dependence structures impact our ecological analyses is crucial to address many ecological problems.\nSpatial dependence may be caused by:\n\n\nEndogenous processes inherent to the system\n(e.g., localized dispersal leading to organism clustering or social and grouping behaviors)\nExogenous factors such as spatially dependent environmental gradients used by the organism of interest\nModelling mis-specifications, including the omission of key covariates or incorrect functional assumptions"
  },
  {
    "objectID": "slides/slides_7.html#areal-data-1",
    "href": "slides/slides_7.html#areal-data-1",
    "title": "Modelling Areal Data",
    "section": "Areal Data",
    "text": "Areal Data\n\n\nAreal data are data which come from well-defined geographical units such as postcode areas, health boards, or pixels on a satellite image.\nAs with other types of spatial modelling, our goal is to observe and explain spatial variation in our data.\nAgain, we are trying to understand and account for spatial dependence.\nGenerally, we aim to produce a smoothed map that summarises the spatial patterns observed in our data."
  },
  {
    "objectID": "slides/slides_7.html#overview-of-areal-processes",
    "href": "slides/slides_7.html#overview-of-areal-processes",
    "title": "Modelling Areal Data",
    "section": "Overview of Areal processes",
    "text": "Overview of Areal processes\n\nAn areal process (or lattice process) is a stochastic process defined on a set of regions that form a partition of our region of interest \\(D\\).\n\nLet \\(B_1, \\ldots B_m\\) be our set of \\(m\\) distinct regions such that: \\[\\bigcup\\limits_{i=1}^m \\hspace{1mm}B_i = D.\\]\nHere we require that our regions are non-overlapping, with \\[B_i \\cap B_j = \\emptyset.\\]\nThen our areal process is simply the stochastic process \\[\\{Z(B_i); i=1,\\ldots,m\\}.\\]"
  },
  {
    "objectID": "slides/slides_7.html#how-do-we-model-this",
    "href": "slides/slides_7.html#how-do-we-model-this",
    "title": "Modelling Areal Data",
    "section": "How do we model this?",
    "text": "How do we model this?\nImagine we have animal counts in each region. We can model them as Poisson\n\\[\ny_i \\sim \\mathrm{Poisson}(e^{\\eta_i})\n\\] How do we model the linear predictor \\(\\eta_i\\)?\n\n\nWe could model the number of animals in each region independently\n\n\\[\n\\eta_i = \\beta_0 + \\mathbf{x}'\\beta + u_i ~~~ u_i \\overset{iid}{\\sim} N(0,\\sigma^2_i)\n\\]\n\nregional differences are accounted for through a random effect"
  },
  {
    "objectID": "slides/slides_7.html#how-do-we-model-this-1",
    "href": "slides/slides_7.html#how-do-we-model-this-1",
    "title": "Modelling Areal Data",
    "section": "How do we model this?",
    "text": "How do we model this?\nImagine we have animal counts in each region. We can model them as Poisson\n\\[\ny_i \\sim \\mathrm{Poisson}(e^{\\eta_i})\n\\] How do we model the linear predictor \\(\\eta_i\\)?\n\n\nBut… what if the distribution varies across space, i.e. is structured in space?\nDo the covariates account for those structures?\nIf there’s an area where the animal is rare, we’ll get lots of zero counts"
  },
  {
    "objectID": "slides/slides_7.html#how-do-we-model-this-2",
    "href": "slides/slides_7.html#how-do-we-model-this-2",
    "title": "Modelling Areal Data",
    "section": "How do we model this?",
    "text": "How do we model this?\nImagine we have animal counts in each region. We can model them as Poisson\n\\[\ny_i \\sim \\mathrm{Poisson}(e^{\\eta_i})\n\\] How do we model the linear predictor \\(\\eta_i\\)?\n\n\nWe could model some dependence across regions:\n\nNearby regions should have similar counts\n\n\n\\[\n\\eta_i = \\beta_0 + \\mathbf{x}'\\beta + u_i ~~~ u_i \\overset{iid}{\\sim} N(0,\\Sigma)\n\\]\n\nNow the random effect \\(u_i \\sim N(0, \\Sigma)\\) is correlated.\n\n\n\n\nHow do we do this?"
  },
  {
    "objectID": "slides/slides_7.html#sparse-matrixes",
    "href": "slides/slides_7.html#sparse-matrixes",
    "title": "Modelling Areal Data",
    "section": "Sparse Matrixes",
    "text": "Sparse Matrixes\n\\[\n\\mathcal{u}\\sim\\mathcal{N}(0,\\Sigma)\n\\]\n\nForce the covariance matrix \\(\\Sigma\\) to be sparse\nForce the precision matrix \\(\\Sigma^{-1}\\) to be sparse"
  },
  {
    "objectID": "slides/slides_7.html#sparse-matrixes-1",
    "href": "slides/slides_7.html#sparse-matrixes-1",
    "title": "Modelling Areal Data",
    "section": "Sparse Matrixes",
    "text": "Sparse Matrixes\n\\[\n\\mathcal{u}\\sim\\mathcal{N}(0,\\Sigma)\n\\]\n\nForce the covariance matrix \\(\\Sigma\\) to be sparse\n\n\\(\\Sigma_{ij} = \\text{Cov}(u_i, u_j)\\) : Covariance between \\(u_i\\) and \\(u_j\\)\n\\(\\Sigma_{ij} = 0\\) \\(\\longrightarrow\\) \\(u_i\\) and \\(u_j\\) are independent\nA sparse covariance matrix implies that many elements of \\(\\mathbf{u}\\) are mutually independent…..is this desirable?\n\nForce the precision matrix \\(\\mathbf{Q} = \\Sigma^{-1}\\) to be sparse"
  },
  {
    "objectID": "slides/slides_7.html#sparse-matrixes-2",
    "href": "slides/slides_7.html#sparse-matrixes-2",
    "title": "Modelling Areal Data",
    "section": "Sparse Matrixes",
    "text": "Sparse Matrixes\n\\[\n\\mathcal{u}\\sim\\mathcal{N}(0,\\Sigma)\n\\]\n\nForce the covariance matrix \\(\\Sigma\\) to be sparse\n\n\\(\\Sigma_{ij} = \\text{Cov}(u_i, u_j)\\) : Covariance between \\(u_i\\) and \\(u_j\\)\n\\(\\Sigma_{ij} = 0\\) \\(\\longrightarrow\\) \\(u_i\\) and \\(u_j\\) are independent\nA sparse covariance matrix implies that many elements of \\(\\mathbf{u}\\) are mutually independent…..is this desirable?\n\nForce the precision matrix \\(\\mathbf{Q} = \\Sigma^{-1}\\) to be sparse\n\nWhat does \\(Q_{ij}\\) represents?\nWhat does a sparse precision matrix implies?"
  },
  {
    "objectID": "slides/slides_7.html#example-the-ar1-proces",
    "href": "slides/slides_7.html#example-the-ar1-proces",
    "title": "Modelling Areal Data",
    "section": "Example: The AR(1) proces",
    "text": "Example: The AR(1) proces\nDefinition\n\\[\n\\begin{aligned}\n\\mathbf{i=1}&:  x_1 \\sim N\\left(0, \\frac{1}{1-\\phi^2}\\right)\\\\\n\\mathbf{i=2,\\dots,T}&:  x_i  = \\phi\\  x_{i-1} +\\epsilon_i,\\ \\epsilon_i\\sim\\mathcal{N}(0,1)\n\\end{aligned}\n\\]\n\nVery common to model dependence in time\nThe joint distribution of \\(\\mathbf{x}=x_1,x_2,\\dots\\) is Gaussian\nHow do covariance and precision matrices look?"
  },
  {
    "objectID": "slides/slides_7.html#covariance-and-precision-matrix-for-ar1",
    "href": "slides/slides_7.html#covariance-and-precision-matrix-for-ar1",
    "title": "Modelling Areal Data",
    "section": "Covariance and Precision Matrix for AR1",
    "text": "Covariance and Precision Matrix for AR1\n\n\n\n\n\nCovariance Matrix\n\\[\n\\Sigma = \\frac{1}{1-\\phi^2}  \\begin{bmatrix}\n1& \\phi & \\phi^2  & \\dots& \\phi^N \\\\\n\\phi & 1& \\phi  & \\dots& \\phi^{N-1} \\\\\n\\phi^2 & \\phi & 1 & \\dots& \\phi^{N-2} \\\\\n\\dots& \\dots& \\dots& \\dots& \\dots& \\\\\n\\phi^{N} & \\phi^{N-1}& \\phi^{N-2}  & \\dots& 1\\\\\n\\end{bmatrix}\n\\]\n\nThis is a dense matrix.\nAll elements of the \\(\\mathbf{x}\\) vector are dependent."
  },
  {
    "objectID": "slides/slides_7.html#covariance-and-precision-matrix-for-ar1-1",
    "href": "slides/slides_7.html#covariance-and-precision-matrix-for-ar1-1",
    "title": "Modelling Areal Data",
    "section": "Covariance and Precision Matrix for AR1",
    "text": "Covariance and Precision Matrix for AR1\n\n\n\nPrecision Matrix\n\\[\n\\mathbf{Q} = \\Sigma^{-1} =  \\begin{bmatrix}\n1& -\\phi & 0  & 0 &\\dots& 0 \\\\\n-\\phi & 1 + \\phi^2& -\\phi  & 0 & \\dots& 0 \\\\\n0 & -\\phi & 1-\\phi^2 &-\\phi &  \\dots& 0 \\\\\n0 & 0 & -\\phi &1-\\phi^2 & \\dots & \\dots \\\\\n\\dots& \\dots& \\dots& \\dots& \\dots& \\dots& \\\\\n0 &0 & 0 & \\dots  & -\\phi& 1\\\\\n\\end{bmatrix}\n\\]\n\nThis is a tridiagonal matrix, it is sparse\nThe tridiagonal form of \\(\\mathbf{Q}\\) can be exploited for quick calculations.\n\n\nWhat is the key property of this example that causes \\(\\mathbf{Q}\\) to be sparse?"
  },
  {
    "objectID": "slides/slides_7.html#conditional-independence",
    "href": "slides/slides_7.html#conditional-independence",
    "title": "Modelling Areal Data",
    "section": "Conditional independence",
    "text": "Conditional independence\nThe key lies in the full conditionals\n\\[\nx_t|\\mathbf{x}_{-t}\\sim\\mathcal{N}\\left(\\frac{\\phi}{1-\\phi^2}(x_{t-1}+x_{t+1}), \\frac{1}{1+\\phi^2}\\right)\n\\]\n\n\n\n\n\n\n\nThe circles represent the values of \\(x\\) at individual time points\nThere is a line between them if they are conditionally dependent\nEach timepoint is only conditionally dependent on the two closest neighbours\nThe nonzero pattern in the precision matrix is given by the neighborhood structure of the process"
  },
  {
    "objectID": "slides/slides_7.html#conditional-autoregressive-models",
    "href": "slides/slides_7.html#conditional-autoregressive-models",
    "title": "Modelling Areal Data",
    "section": "Conditional autoregressive models",
    "text": "Conditional autoregressive models\n\n\nMarkov in Space:\nFirst order conditional autoregressive model or a CAR(1) model.\n\nEvery node is conditionally dependent on its four nearest neighbours\n\n\n\n\n\n\n\n\n\nModels based on neighbourhood have a name in statistics: they are Markovian models\nMarkovian models are specified entirely through “neighbourhood structures”\nRecall our first Model:\n\\[\n\\begin{aligned}\ny_i &\\sim \\mathrm{Poisson}(e^{\\eta_i}) \\\\\n\\eta_i &= \\beta_0 + \\mathbf{x}'\\beta + u_i ~~~ u_i \\sim N(0,Q^{-1})\n\\end{aligned}\n\\]\nThis mean \\(u_i\\) is independent of all the other parameters \\(\\mathbf{u}_{-i}\\), given the set of its neighbors.\nfor any pair of elements (\\(i,j\\)) in \\(\\mathbf{u}\\), \\(u_i\\perp u_j|\\mathbf{u}_{-ij}\\Longleftrightarrow Q{ij} =0\\)\n\\(Q{ij} \\neq 0\\) only if \\(j\\in \\{i,\\mathcal{N}(i)\\}\\)"
  },
  {
    "objectID": "slides/slides_7.html#informal-definition-of-a-gmrf",
    "href": "slides/slides_7.html#informal-definition-of-a-gmrf",
    "title": "Modelling Areal Data",
    "section": "(Informal) definition of a GMRF",
    "text": "(Informal) definition of a GMRF\nGauss Markov random field (GMRF) is\n\n\na Gaussian distribution where the non-zero elements of the precision matrix are defined by a neighbourhood matrix (or graph structure)\neach region conditionally has a Gaussian distribution with\nmean equal to the average of the neighbours and\nprecision proportional to the number of neighbours\n\n\nGMRFs are key to the many inferential approaches:\n\n\ncomputationally efficient\nother data structures can be approximated by a GMRF in a clever way"
  },
  {
    "objectID": "slides/slides_7.html#formal-definition-of-a-gmrf",
    "href": "slides/slides_7.html#formal-definition-of-a-gmrf",
    "title": "Modelling Areal Data",
    "section": "Formal definition of a GMRF",
    "text": "Formal definition of a GMRF\nLet \\(\\mathbf{u}\\) be a GMRF wrt \\(\\mathcal{G} = (\\mathcal{V},\\mathcal{E})\\)).\n\\(\\mathcal{V}\\) Vertices: \\(1,2,\\ldots,n\\). \\(\\mathcal{E}\\) Edges \\(\\{i,j\\}\\)\n\nNo edge between \\(i\\) and \\(j\\) if \\(u_i \\perp u_j \\mid \\mathbf{u}_{ij}.\\)\nAn edge between \\(i\\) and \\(j\\) if \\(u_i \\not\\perp u_j \\mid \\mathbf{u}_{ij}.\\)\n\nKey point: A graph defines the sparsity structure of Q\n\n\n\nDefinition\n\n\nA random variable \\(\\mathbf{u}\\) is said to be a GMRF wrt to the graph \\(\\mathcal{G}\\) with vertices \\(\\{1, 2,\\dots , n\\}\\) and edges \\(\\mathcal{E}\\) , with mean \\(\\mu\\) and precision matrix \\(\\mathbf{Q}\\) if its probability distribution is given by\n\\[\n\\mathbf{u} \\sim N(\\mu,Q^{-1})\n\\] and\n\\(Q_{ij} \\neq 0\\Longleftrightarrow \\{i,j\\}\\in\\mathcal{E} ~~\\forall~~i\\neq j\\)"
  },
  {
    "objectID": "slides/slides_7.html#modelling-spatial-similarity",
    "href": "slides/slides_7.html#modelling-spatial-similarity",
    "title": "Modelling Areal Data",
    "section": "Modelling spatial similarity",
    "text": "Modelling spatial similarity\n\n\n\nAn example of a GMRF is the the Besag model a.k.a. Intrinsic Conditional Autoregressive (ICAR) model. The conditional distribution for \\(u_i\\) is\n\\[\nu_i|\\mathbf{u}_{-i},\\tau_u, \\sim N\\left(\\frac{1}{d_i}\\sum_{j\\sim i}u_j,\\frac{1}{d_i\\tau_u}\\right)\n\\]\n\n\n\\(\\mathbf{u}_{-i} = (u_i,\\ldots,u_{i-1},u_{i+1},\\ldots,u_n)^T\\)\n\\(\\tau_u\\) is the precision parameter (inverse variance).\n\\(d_i\\) is the number of neighbours\nThe mean of \\(u_i\\) is equivalent to the the mean of the effects over all neighbours, and the precision is proportional to the number of neighbours (e.g., if an area has many neighbors then its variance will be smaller)"
  },
  {
    "objectID": "slides/slides_7.html#modelling-spatial-similarity-1",
    "href": "slides/slides_7.html#modelling-spatial-similarity-1",
    "title": "Modelling Areal Data",
    "section": "Modelling spatial similarity",
    "text": "Modelling spatial similarity\n\n\n\nThe joint distribution is given by:\n\\[\n\\mathbf{u}|\\tau_u \\sim N\\left(0,\\frac{1}{\\tau_u}Q^{-1}\\right),\n\\]\nWhere \\(Q\\) denotes the precision matrix defined as\n\\[\nQ_{i,j} = \\begin{cases}\nd_i, & i = j \\\\\n-1, & i \\sim j \\\\\n0, &\\text{otherwise}\n\\end{cases}\n\\]\nThis structure matrix directly defines the neighbourhood structure and is sparse."
  },
  {
    "objectID": "slides/slides_7.html#how-does-it-looks",
    "href": "slides/slides_7.html#how-does-it-looks",
    "title": "Modelling Areal Data",
    "section": "How does it looks?",
    "text": "How does it looks?\nLarynx cancer relative risk map"
  },
  {
    "objectID": "slides/slides_7.html#how-does-it-looks-1",
    "href": "slides/slides_7.html#how-does-it-looks-1",
    "title": "Modelling Areal Data",
    "section": "How does it looks?",
    "text": "How does it looks?\nConnecting all the neighbouring areas give the following graph"
  },
  {
    "objectID": "slides/slides_7.html#how-does-it-looks-2",
    "href": "slides/slides_7.html#how-does-it-looks-2",
    "title": "Modelling Areal Data",
    "section": "How does it looks?",
    "text": "How does it looks?\n\nLet us focus on one small part of the graph"
  },
  {
    "objectID": "slides/slides_7.html#how-does-it-looks-3",
    "href": "slides/slides_7.html#how-does-it-looks-3",
    "title": "Modelling Areal Data",
    "section": "How does it looks?",
    "text": "How does it looks?\n\nWe apply an ICAR model where each region conditionally has a Gaussian distribution with mean equal to the average of the neighbours and a precision proportional to the number of neighbour\n\\[\nx_9\\mid\\mathbf{x}_{-9}\\sim N\\left(\\frac{1}{6}(x_7+x_{11}+x_{12}+x_{13}+x_{14}+x_{15},\\frac{1}{6\\tau}\\right)\n\\]"
  },
  {
    "objectID": "slides/slides_7.html#how-does-it-looks-4",
    "href": "slides/slides_7.html#how-does-it-looks-4",
    "title": "Modelling Areal Data",
    "section": "How does it looks?",
    "text": "How does it looks?\nThe sub graph leads to a precision matrix with 21.6% non-zero elements."
  },
  {
    "objectID": "slides/slides_7.html#how-does-it-looks-like",
    "href": "slides/slides_7.html#how-does-it-looks-like",
    "title": "Modelling Areal Data",
    "section": "How does it looks like?",
    "text": "How does it looks like?\nThe full graph leads to a precision matrix with 0.1% non-zero elements.\n\n\n\nHow do we do choose the neighborhood structure?"
  },
  {
    "objectID": "slides/slides_7.html#neighbourhood-structures",
    "href": "slides/slides_7.html#neighbourhood-structures",
    "title": "Modelling Areal Data",
    "section": "Neighbourhood structures",
    "text": "Neighbourhood structures\n\n\nEach of our regions \\(B_i\\) has a set of other nearby which can be considered neighbours\nWe might expect that areas have more in common with their neighbours.\nTherefore, we can construct dependence structures based on the principle that neighbours are correlated and non-neighbours are uncorrelated.\nHowever, we need to come up with a sensible way of defining what a neighbour is in this context."
  },
  {
    "objectID": "slides/slides_7.html#defining-a-neighbourhood",
    "href": "slides/slides_7.html#defining-a-neighbourhood",
    "title": "Modelling Areal Data",
    "section": "Defining a Neighbourhood",
    "text": "Defining a Neighbourhood\n\nThere are many different ways to define a region’s neighbours.\nThe most common ones fall into two main categories - those based on borders, and those based on distance.\n\n\n\nCommon borders\n\nAssume that regions which share a border on a map are neighbours.\n\nSimple and easy to implement\nTreats all borders the same, regardless of length, which can be unrealistic.\nAreas very close together are not neighbours if there is even a small gap between them.\n\n\n\n\n\n\n\n\nDistance-based\n\nAssume that regions which are a within a certain distance of each other area neighbours.\n\nWhat distance do you choose? How do you decide that?\nWhere do you measure from? (e.g., nearest border or a central point)."
  },
  {
    "objectID": "slides/slides_7.html#neighbourhood-matrix",
    "href": "slides/slides_7.html#neighbourhood-matrix",
    "title": "Modelling Areal Data",
    "section": "Neighbourhood matrix",
    "text": "Neighbourhood matrix\n\n\nOnce we have identified a set of neighbours using our chosen method, we can use this to account for spatial correlation.\nWe construct a neighbourhood matrix (or proximity matrix), which defines how each of our \\(m\\) regions relate to each other.\nLet \\(W\\) denote an \\(m \\times m\\) matrix where the \\((i,j)\\)th entry, \\(w_{ij}\\) denotes the proximity between regions \\(B_i\\) and \\(B_j\\).\n\n\n\n\n\n\nNote\n\n\nThe values of this matrix can be discrete (which regions are neighbours) or continuous (how far apart are the regions)."
  },
  {
    "objectID": "slides/slides_7.html#binary-neighbourhood-matrix",
    "href": "slides/slides_7.html#binary-neighbourhood-matrix",
    "title": "Modelling Areal Data",
    "section": "Binary Neighbourhood matrix",
    "text": "Binary Neighbourhood matrix\nBy far the most common approach is to use a binary neighbourhood matrix, \\(W\\), denoted by\n\\[\n\\begin{aligned}\nw_{ij} &= 1 \\hspace{2mm} \\mbox{ if areas} (B_i, B_j) \\mbox{ are neighbours.}\\\\\nw_{ij} &= 0 \\hspace{2mm} \\mbox{ otherwise.}\n\\end{aligned}\n\\]\n\n\n\nDependencies structures are described through this spatial weights matrix\nBinary matrices are used for their simplicity"
  },
  {
    "objectID": "slides/slides_7.html#how",
    "href": "slides/slides_7.html#how",
    "title": "Modelling Areal Data",
    "section": "How?",
    "text": "How?\n\n\nNow that we have defined a measure of spatial proximity for areal data, we can use this to assess spatial dependence.\nEssentially, we can now ask the question of how similar a region is to its neighbours.\nWe can consider global correlation, measured across the entire region, and local correlation which allows for regional variation.\nIn this course, we will focus on modelling global autocorrelation using Moran’s I."
  },
  {
    "objectID": "slides/slides_7.html#morans-i",
    "href": "slides/slides_7.html#morans-i",
    "title": "Modelling Areal Data",
    "section": "Moran’s \\(I\\)",
    "text": "Moran’s \\(I\\)\n\nMoran’s \\(I\\) is a measure of global spatial autocorrelation and can be considered an extension of the Pearson correlation coefficient.\nFor a set of data \\(Z_1, \\ldots, Z_m\\) measured on regions \\(B_1, \\ldots, B_m\\), with neighbourhood matrix \\(W\\), Moran’s \\(I\\) is given by:\n\n\\[\nI = \\frac{m}{\\sum_{i=1}^m \\sum_{j=1}^m w_{ij}}\n\\frac{\\sum_{i=1}^m \\sum_{j=1}^m w_{ij}(Z_i - \\bar{Z})(Z_j - \\bar{Z})}\n{\\sum_{i=1}^m (Z_i - \\bar{Z})^2}\n\\]\n\nThis is essentially a function of differences in values between neighbouring areas."
  },
  {
    "objectID": "slides/slides_7.html#morans-i-1",
    "href": "slides/slides_7.html#morans-i-1",
    "title": "Modelling Areal Data",
    "section": "Moran’s \\(I\\)",
    "text": "Moran’s \\(I\\)\n\nMoran’s \\(I\\) ranges between \\(-1\\) and \\(1\\) and can be interpreted similarly to a standard correlation coefficient.\n\\(I = 1\\) implies perfect spatial correlation.\n\\(I = 0\\) implies complete spatial randomness.\n\\(I = -1\\) implies perfect dispersion (negative correlation).\nOur observed \\(I\\) is a point estimate, and we may also wish to assess whether it is significantly different from zero."
  },
  {
    "objectID": "slides/slides_7.html#assessing-significance",
    "href": "slides/slides_7.html#assessing-significance",
    "title": "Modelling Areal Data",
    "section": "Assessing significance",
    "text": "Assessing significance\n\nWe can test for statistically significant spatial correlation using a permutation test, with hypotheses:\n\n\\[\n\\begin{aligned}\nH_0 &: \\text{no spatial association } (I = 0) \\\\\nH_1 &: \\text{some spatial association } (I \\neq 0)\n\\end{aligned}\n\\]\n\nWe carry out \\(k\\) random permutations of our data (reassigning each data value to a random location) and compute Moran’s \\(I\\) for each permutation\n\\((I_{\\text{perm}} = I_1, \\ldots, I_k)\\).\nWe reject the null hypothesis if the observed Moran’s \\(I\\) (\\(I_{\\text{obs}}\\)) could not plausibly have arisen from the distribution of \\(I_{\\text{perm}}\\)."
  },
  {
    "objectID": "slides/slides_7.html#examples",
    "href": "slides/slides_7.html#examples",
    "title": "Modelling Areal Data",
    "section": "Examples",
    "text": "Examples\n\n\nPerfect dispersion\n\n\n\n\n\n\\[\n\\begin{aligned}\n\\text{Estimate} &\\ -1 \\\\\n\\text{St. Dev.} &\\ 0.095 \\\\\n\\text{p-value}  &&lt; 0.05\n\\end{aligned}\n\\]\n\nNo spatial correlation\n\n\n\n\n\n\\[\n\\begin{aligned}\n\\text{Estimate} &\\ 0.001 \\\\\n\\text{St. Dev.} &\\ 0.095 \\\\\n\\text{p-value}  &\\ 0.865\n\\end{aligned}\n\\]\n\nHigh correlation\n\n\n\n\n\n\\[\n\\begin{aligned}\n\\text{Estimate} &\\ 0.865 \\\\\n\\text{St. Dev.} &\\ 0.095 \\\\\n\\text{p-value}  &&lt; 0.05 \\times 10^{25}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/slides_7.html#fitting-a-car-model",
    "href": "slides/slides_7.html#fitting-a-car-model",
    "title": "Modelling Areal Data",
    "section": "Fitting a CAR model",
    "text": "Fitting a CAR model\nSuppose we identified spatial autocorrelation in our data… what do we do next?\n\n\nFitting autoregressive models to non-normal data—such as presence/absence responses—can be challenging due to the lack of simple distributional assumptions.\nCAR models are often implemented within a Bayesian framework, which provides a flexible approach for handling spatially structured ecological & environmental data.\nA key advantage of Bayesian hierarchical modelling is its ability to properly incorporate uncertainty and facilitate inference in complex spatial models."
  },
  {
    "objectID": "slides/slides_7.html#bayesian-inference",
    "href": "slides/slides_7.html#bayesian-inference",
    "title": "Modelling Areal Data",
    "section": "Bayesian Inference",
    "text": "Bayesian Inference\n\nIn the Bayesian framework all unknown quantities in the model are treated as random variables, and the aim is to estimate the joint posterior distribution of the unknown parameters \\(\\theta\\) given the observed data \\(\\mathbf{y}\\).\nWe obtain this distribution through Bayes’ theorem:\n\n\\[\n\\pi(\\theta \\mid \\mathbf{y}) = \\frac{\\pi(\\mathbf{y} \\mid \\theta)\\pi(\\theta)}{\\pi(\\mathbf{y})}\n\\]\n\nComponents:\n\n\\(\\pi(\\mathbf{y} \\mid \\theta)\\) is the likelihood of \\(\\mathbf{y}\\) given parameters \\(\\theta\\)\n\\(\\pi(\\theta)\\) is the prior distribution of the parameters\n\\(\\pi(\\mathbf{y})\\) is the marginal likelihood (normalizing constant):\n\n\\[\n\\pi(\\mathbf{y}) = \\int_\\Theta \\pi(\\mathbf{y} \\mid \\theta) \\pi(\\theta) d\\theta\n\\]\n\nmarginalizing \\(\\pi(\\mathbf{y})\\) means integrating out all the uncertainty on \\(\\theta\\)"
  },
  {
    "objectID": "slides/slides_7.html#bayesian-inference-computation",
    "href": "slides/slides_7.html#bayesian-inference-computation",
    "title": "Modelling Areal Data",
    "section": "Bayesian Inference (Computation)",
    "text": "Bayesian Inference (Computation)\n\n\n\n\nWhen the posterior distribution lacks a closed-form solution, computational methods must be used to approximate it.\nTo efficiently estimate spatially structured spatial models, particularly those incorporating Conditional Autoregressive (CAR) priors, the Integrated Nested Laplace Approximation (INLA) framework (Van Niekerk et al., 2023) provides a powerful alternative to traditional Markov chain Monte Carlo (MCMC) methods.\n\n\n\n\n\n\n\n\n\n\nposterior\n\n\n\nA\n\nPrior\n belief\n\n\n\nC\n\nBayes Theorem\n &\n Bayesian Computations\n\n\n\nA-&gt;C\n\n\n\n\n\nB\n\nObservation\n model\n\n\n\nB-&gt;C\n\n\n\n\n\nD\n\nPosterior\n distribution\n\n\n\nC-&gt;D\n\n\n\n\n\n\n\n\n\n\n\\[\n\\color{purple}{\\pi(\\mathbf{u},\\theta|\\mathbf{y})}\\propto \\color{#FF6B6B}{\\pi(\\mathbf{y}|\\mathbf{u},\\theta)}\\color{#0066CC}{\\pi(\\mathbf{u}|\\theta)\\pi(\\theta)}\n\\]"
  },
  {
    "objectID": "slides/slides_7.html#the-inla-framework",
    "href": "slides/slides_7.html#the-inla-framework",
    "title": "Modelling Areal Data",
    "section": "The INLA Framework",
    "text": "The INLA Framework\nWhat is INLA?\n\nThe short answer:\n\nINLA is a fast and accurate method to do Bayesian inference with latent Gaussian models."
  },
  {
    "objectID": "slides/slides_7.html#the-inla-framework-1",
    "href": "slides/slides_7.html#the-inla-framework-1",
    "title": "Modelling Areal Data",
    "section": "The INLA Framework",
    "text": "The INLA Framework\nWhat is INLA?\nThe (much) longer answer:\n\n\nRue, H., Martino, S. and Chopin, N. (2009), Approximate Bayesian inference for latent Gaussian models by using integrated nested Laplace approximations. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 71: 319-392.\nVan Niekerk, J., Krainski, E., Rustand, D., & Rue, H. (2023). A new avenue for Bayesian inference with INLA. Computational Statistics & Data Analysis, 181, 107692.\nLindgren, F., Bachl, F., Illian, J., Suen, M. H., Rue, H., & Seaton, A. E. (2024). inlabru: software for fitting latent Gaussian models with non-linear predictors. arXiv preprint arXiv:2407.00791.\nLindgren, F., Bolin, D., & Rue, H. (2022). The SPDE approach for Gaussian and non-Gaussian fields: 10 years and still running. Spatial Statistics, 50, 100599."
  },
  {
    "objectID": "slides/slides_7.html#the-inla-framework-2",
    "href": "slides/slides_7.html#the-inla-framework-2",
    "title": "Modelling Areal Data",
    "section": "The INLA Framework",
    "text": "The INLA Framework\nINLA can be used with Bayesian hierarchical models where we model in different stages or levels:\n\n\nStage 1: What is the distribution of the responses?\nStage 2: What is the distribution of the underlying latent components?\nStage 3: What are our prior beliefs about the hyperparameters?"
  },
  {
    "objectID": "slides/slides_7.html#stage-1-data-generating-process",
    "href": "slides/slides_7.html#stage-1-data-generating-process",
    "title": "Modelling Areal Data",
    "section": "Stage 1: Data Generating Process",
    "text": "Stage 1: Data Generating Process\nOn Stage 1, we look at how is our data (\\(\\mathbf{y}\\)) generated from the underlying components \\(\\mathbf{x}\\) and hyperparameters \\(\\theta\\) in the model\nResponse types:\n\nGaussian (temperature, rainfall, weight)\nCount data (disease cases, species counts)\nPoint patterns (tree locations)\nBinary data (yes/no responses)\nSurvival data (time to event)\n\nIt is also important how data are collected!\nAll of this information is placed into our likelihood \\(\\pi(\\mathbf{y}|\\theta)\\)."
  },
  {
    "objectID": "slides/slides_7.html#stage-1-data-generating-process-1",
    "href": "slides/slides_7.html#stage-1-data-generating-process-1",
    "title": "Modelling Areal Data",
    "section": "Stage 1: Data Generating Process",
    "text": "Stage 1: Data Generating Process\nWe assume that given the underlying components (\\(\\mathbf{x}\\)) and hyperparameter (\\(\\theta\\)), the data are independent on each other\n\\[\n\\pi(y|x,\\theta) = \\prod_{i\\in\\mathcal{I}}\\pi(y_i|\\mathbf{x}_{\\mathcal{I}},\\theta)\n\\]\nThis implies that all the dependence structure in the data is explained on Stage 2."
  },
  {
    "objectID": "slides/slides_7.html#stage-2-dependence-structure",
    "href": "slides/slides_7.html#stage-2-dependence-structure",
    "title": "Modelling Areal Data",
    "section": "Stage 2: Dependence Structure",
    "text": "Stage 2: Dependence Structure\nThe underlying unobserved components \\(\\mathbf{x}\\) are called latent components and can be:\n\nFixed effects of covariates\nUnstructured random effects (individual effects, group effects)\nStructured random effects (AR(1), regional effects, \\(\\ldots\\) )\n\nThese are linked to the responses in the likelihood through linear predictors."
  },
  {
    "objectID": "slides/slides_7.html#stage-2-dependence-structure-1",
    "href": "slides/slides_7.html#stage-2-dependence-structure-1",
    "title": "Modelling Areal Data",
    "section": "Stage 2: Dependence Structure",
    "text": "Stage 2: Dependence Structure\n\nThe latent field \\(\\mathbf{u}\\) is a GMRF \\[\n\\pi(\\mathbf{u}|\\theta)\\propto\\exp\\left\\{-\\frac{1}{2}\\mathbf{u}^T\\mathbf{Q}\\mathbf{u}\\right\\}\n\\]\n\nThe precision matrix \\(\\mathbf{Q}\\) is sparse."
  },
  {
    "objectID": "slides/slides_7.html#stage-2-dependence-structure-2",
    "href": "slides/slides_7.html#stage-2-dependence-structure-2",
    "title": "Modelling Areal Data",
    "section": "Stage 2: Dependence Structure",
    "text": "Stage 2: Dependence Structure\n\nThe latent field \\(\\mathbf{u}\\) is a GMRF \\[\n\\pi(\\mathbf{u}|\\theta)\\propto\\exp\\left\\{-\\frac{1}{2}\\mathbf{u}^T\\mathbf{Q}\\mathbf{u}\\right\\}\n\\]\nWe can factorize the likelihood as: \\(\\pi(\\mathbf{y}|\\mathbf{u},\\theta) = \\prod_i\\pi(y_i|\\eta_i,\\theta)\\)\n\nData are conditional independent give \\(\\mathbf{u}\\) and \\(\\theta\\)\nEach data point depends on only 1 element of the latent field: the predictor \\(\\eta_i\\)\n\\(\\eta\\) is a linear combination of other elements of \\(\\mathbf{u}\\): \\(\\eta = \\mathbf{A}^T\\mathbf{u}\\)"
  },
  {
    "objectID": "slides/slides_7.html#stage-2-dependence-structure-3",
    "href": "slides/slides_7.html#stage-2-dependence-structure-3",
    "title": "Modelling Areal Data",
    "section": "Stage 2: Dependence Structure",
    "text": "Stage 2: Dependence Structure\n\nThe latent field \\(\\mathbf{u}\\) is a GMRF \\[\n\\pi(\\mathbf{u}|\\theta)\\propto\\exp\\left\\{-\\frac{1}{2}\\mathbf{u}^T\\mathbf{Q}\\mathbf{u}\\right\\}\n\\]\nWe can factorize the likelihood as: \\(\\pi(\\mathbf{y}|\\mathbf{u},\\theta) = \\prod_i\\pi(y_i|\\eta_i,\\theta)\\)\nThe vector of hyperparameters \\(\\theta\\) is low dimensional."
  },
  {
    "objectID": "slides/slides_7.html#stage-3-hyperparameters",
    "href": "slides/slides_7.html#stage-3-hyperparameters",
    "title": "Modelling Areal Data",
    "section": "Stage 3: Hyperparameters",
    "text": "Stage 3: Hyperparameters\nThe likelihood and the latent model typically have hyperparameters that control their behavior.\nThe hyperparameters \\(\\theta\\) can include:\n\nExamples likelihood:\n\nVariance of observation noise\nDispersion parameter in the negative binomial model -\nProbability of a zero (zero-inflated models)\n\nExamples latent model:\n\nVariance of unstructured effects\nCorrelation of multivariate effects\nRange and variance of spatial effects\nAutocorrelation parameter \\(\\rho\\)"
  },
  {
    "objectID": "slides/slides_7.html#latent-gaussian-models-lgms",
    "href": "slides/slides_7.html#latent-gaussian-models-lgms",
    "title": "Modelling Areal Data",
    "section": "Latent Gaussian Models (LGMs)",
    "text": "Latent Gaussian Models (LGMs)\nThese three stages constitute the basis of latent Gaussian models (LGMs).\n\nA LGM consists of three elements:\n\na likelihood model\na latent Gaussian field (i.e., the latent components of our model)\na vector of non-Gaussian hyperparameters\n\nThe characteristic property is that the latent part of the hierarchical model is Gaussian, \\(\\mathbf{u}|\\theta \\sim N(0,Q^{-1})\\), where the expected value is \\(0\\) and the precision matrix is \\(Q\\)."
  },
  {
    "objectID": "slides/slides_7.html#inla-strategy-in-short",
    "href": "slides/slides_7.html#inla-strategy-in-short",
    "title": "Modelling Areal Data",
    "section": "INLA strategy in short",
    "text": "INLA strategy in short\nAssumption\nWe are mainly interested in posterior marginals \\(p(u_i|\\mathbf{y})\\) and \\(p(\\theta_j|\\mathbf{y})\\)\nStrategy\n\nWe use numerical integration in a smart way\nWe approximate what we do not know analytically exploiting the Gaussian structure of \\(\\mathbf{u}|y\\)."
  },
  {
    "objectID": "slides/slides_7.html#example-lip-cancer-rates-in-scotland",
    "href": "slides/slides_7.html#example-lip-cancer-rates-in-scotland",
    "title": "Modelling Areal Data",
    "section": "Example: Lip cancer rates in Scotland",
    "text": "Example: Lip cancer rates in Scotland\n\nIn this example we model the number of lip cancer rates in Scotland in the years 1975–1980 at the county level in order to evaluate the presence of an association between sun exposure and lip cancer.\n\n\n\n\nIn epidemiology, disease risk is assessed using Standardized Mortality Ratios (SMR):\n\\[ SMR_i = \\dfrac{Y_i}{E_i} \\]\n\nA value \\(SMR &gt; 1\\) indicates a high risk area.\nA value \\(SMR&lt;1\\) suggests a low risk area.\nSIRs may be misleading in counties with small populations.\nmodel-based approaches enable to incorporate covariates and borrow information from neighboring counties to improve local estimates,"
  },
  {
    "objectID": "slides/slides_7.html#do-we-need-to-model-spatial-dependence",
    "href": "slides/slides_7.html#do-we-need-to-model-spatial-dependence",
    "title": "Modelling Areal Data",
    "section": "Do we need to model spatial dependence?",
    "text": "Do we need to model spatial dependence?\n\nRecall that, Moran’s \\(I\\) ranges between -1 and 1, and can be interpreted in a similar way to a standard correlation coefficient.\n\n\\(I=1\\) implies that we have perfect spatial correlation.\n\\(I=0\\) implies that we have complete spatial randomness.\n\\(I=-1\\) implies that we have perfect dispersion (negative correlation).\n\nOur observed \\(I\\) is a point estimate, and we may also wish to assess whether it is significantly different from zero. We can test for a statistically significant spatial correlation using a permutation test, with hypotheses:\n\\[\n\\begin{aligned}\nH_0&: \\text{ negative or no spatial association } (I \\leq 0)\\\\\nH_1&: \\text{ positive spatial association } (I &gt; 0)\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/slides_7.html#do-we-need-to-model-spatial-dependence-1",
    "href": "slides/slides_7.html#do-we-need-to-model-spatial-dependence-1",
    "title": "Modelling Areal Data",
    "section": "Do we need to model spatial dependence?",
    "text": "Do we need to model spatial dependence?\nWe can use moran.test() to test this hypothesis by setting alternative = \"greater\". To do so, we need to supply list containing the neighbors via the nb2listw() function from the spdep package. Lets assess now the spatial autocorrelation of the SMR in 2011:\n\n\n\n    Moran I test under randomisation\n\ndata:  scotland_sf$SMR  \nweights: R  \nn reduced by no-neighbour observations  \n\nMoran I statistic standard deviate = 5.3678, p-value = 3.984e-08\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.437119239      -0.019230769       0.007227652 \n\n\n\nSince have set the alternative hypothesis to be \\(I &gt; 0\\) and have a p-value \\(&lt;0.05\\), we then reject the null hypothesis and conclude there is evidence for positive spatial autocorrelation."
  },
  {
    "objectID": "slides/slides_7.html#bym-model",
    "href": "slides/slides_7.html#bym-model",
    "title": "Modelling Areal Data",
    "section": "BYM model",
    "text": "BYM model\n\n\n\nThe ICAR model accounts only for spatially structured variability and does not include a limiting case where no spatial structure is present.\n\n\nWe typically add an unstructured random effect \\(z_i\\mid \\tau_z \\sim N(0,\\tau_{z}^{-1})\\)\nThe resulting model \\(v_i = u_i + z_i\\) is known as the Besag-York-Mollié model (BYM)\nThe structured spatial effect is controlled by \\(\\tau_u\\) which control the degree of smoothing:\n\nHigher \\(\\tau_u\\) values lead to stronger smoothing (less spatial variability).\nLower \\(\\tau_u\\) values allow for greater local variation."
  },
  {
    "objectID": "slides/slides_7.html#example-lip-cancer-rates-in-scotland-1",
    "href": "slides/slides_7.html#example-lip-cancer-rates-in-scotland-1",
    "title": "Modelling Areal Data",
    "section": "Example: Lip cancer rates in Scotland",
    "text": "Example: Lip cancer rates in Scotland\n\n\n\nStage 1: We assume the responses are Poisson distributed: \\[          \n\\begin{aligned}y_i|\\eta_i & \\sim \\text{Poisson}(E_i\\lambda_i)\\\\\\text{log}(\\lambda_i) = \\color{#FF6B6B}{\\boxed{\\eta_i}} & = \\color{#FF6B6B}{\\boxed{\\beta_0 + \\beta_1 \\mathrm{AFF} + u_i + z_i} }\\end{aligned}\n\\]\n\n\n\n\n\n\nStage 2: \\(\\eta_i\\) is a linear function of four components: an intercept, proportion of population engaged in agriculture, fishing, or forestry (AFF) effect , a spatially structured effect \\(u\\) and an unstructured iid random effect \\(z\\): \\[\n\\eta_i = \\beta_0 + \\beta_1\\mathrm{AFF}_i + u_i + z_i\n\\]\n\n\n\n\n\n\nStage 3: \\(\\{\\tau_{z},\\tau_u\\}\\): Precision parameters for the random effects\n\n\n\nThe latent field is \\(\\mathbf{x}= (\\beta_0, \\beta_1, u_1, u_2,\\ldots, u_n,z_1,...)\\), the hyperparameters are \\(\\boldsymbol{\\theta} = (\\tau_u,\\tau_z)\\), and must be given a prior."
  },
  {
    "objectID": "slides/slides_7.html#example-lip-cancer-rates-in-scotland-2",
    "href": "slides/slides_7.html#example-lip-cancer-rates-in-scotland-2",
    "title": "Modelling Areal Data",
    "section": "Example: Lip cancer rates in Scotland",
    "text": "Example: Lip cancer rates in Scotland\n\n\nThe Model\n\\[\n\\begin{aligned}\ny_i|\\eta_t & \\sim \\text{Poisson}(E_i\\lambda_i)\\\\\n\\text{log}(\\lambda_i) = \\eta_i & = \\beta_0 + \\beta_1 \\mathrm{AFF}_i + u_i + v_i\n\\end{aligned}\n\\]\n\n\\(\\beta_0\\) is the intercept that represents the overall risk\n\\(\\beta_1\\) is the coefficient of the AFF covariate\n\\(u_i\\) is a spatial structured component$\n\\(v_i\\) is a spatial unstructured component\n\n\nneighbourhood structure\n\n\n\n\n\n\nThe precision matrix \\(Q\\) depends on \\(\\tau_u\\) and the neighboring structure\nTo make the precision parameters of models with different intrinsic Gaussian random field comparable we add a sum-to-zero constrain \\(\\sum_i^n u_i = 0\\)."
  },
  {
    "objectID": "slides/slides_7.html#example-lip-cancer-rates-in-scotland-3",
    "href": "slides/slides_7.html#example-lip-cancer-rates-in-scotland-3",
    "title": "Modelling Areal Data",
    "section": "Example: Lip cancer rates in Scotland",
    "text": "Example: Lip cancer rates in Scotland\n\ncmp = ~ Intercept(1) + beta_1(AFF, model = \"linear\") +\n  u_i(region_id, model = \"besag\", graph = Q,scale.model = TRUE) + \n  z_i(region_id , model = \"iid\")\n\nformula = cases ~ Intercept + beta_1 + u_i + z_i\n\nlik = bru_obs(formula = formula, \n              family = \"poisson\",\n              E = expected,\n              data = scotland_sf)\n\nfit = bru(cmp, lik)\n\n\n\n\n\n\n\n\n\nINLA Model Results\n\n\nPosterior summaries of fixed effects and hyperparameters\n\n\nParameter\nMean\n2.5% Quantile\n97.5% Quantile\n\n\n\n\nIntercept\n−0.306\n−0.538\n−0.069\n\n\nbeta_1\n4.317\n1.758\n6.761\n\n\nPrecision for u_i\n4.139\n2.023\n7.597\n\n\nPrecision for z_i\n22,037.716\n1,474.245\n86,082.203\n\n\n\n\n\n\n\n\nThe model revealed a strong positive association between sun exposure and lip cancer risk"
  },
  {
    "objectID": "slides/slides_7.html#compare-relative-risk-against-sir",
    "href": "slides/slides_7.html#compare-relative-risk-against-sir",
    "title": "Modelling Areal Data",
    "section": "Compare Relative Risk against SIR",
    "text": "Compare Relative Risk against SIR\nThe relative risks \\(\\lambda_i\\) can be obtained as follows:\n\\[\n\\begin{aligned}\n\\lambda_i &= \\exp\\left(\\eta_i\\right) \\\\\n&= \\exp\\Bigl(\\beta_0 + \\beta_1 \\times \\mathrm{AFF}_i + u_i + z_i\\Bigr)\n\\end{aligned}\n\\] We see RR values are shrunk towards 1 compared to the SMR values because each area’s spatial effect is pulled toward its neighbors’ average."
  },
  {
    "objectID": "slides/slides_7.html#bym2-model",
    "href": "slides/slides_7.html#bym2-model",
    "title": "Modelling Areal Data",
    "section": "BYM2 Model",
    "text": "BYM2 Model\nIn the original BYM, the spatially structured component must be scaled so that \\(\\tau_u\\) produces consistent smoothness across different neighborhood structures.\nSimpson et al. (2017) proposed a new parametrization of the BYM model that improves parameter interpretability.\n\\[\n    \\mathbf{b} = \\dfrac{1}{\\sqrt{\\tau_b}} \\left(\\sqrt{1-\\phi}v^*+\\sqrt{\\phi}u^*\\right).\n\\]\n\nThe precision \\(\\tau_b&gt;0\\) controls the marginal variance contribution of the weighted sum \\(u^*\\) and \\(v^*\\).\nThe mixing parameter \\(0 \\leq \\phi \\leq 1\\) measures the proportion of the marginal variance explained by the structured effect \\(u^*\\)\n\nif \\(\\phi =1\\) the model captures only spatially structured variability\nif \\(\\phi = 0\\) it accounts solely for unstructured spatial noise."
  },
  {
    "objectID": "slides/slides_7.html#pc-priors",
    "href": "slides/slides_7.html#pc-priors",
    "title": "Modelling Areal Data",
    "section": "PC priors",
    "text": "PC priors\nThe BYM2 model allows the specification of Penalized Complexity (PC) priors to control the amount of spatial smoothing and avoid overfitting.\n\nPC priors shrink the model towards a simpler baseline unless the data provide strong evidence for a more complex structure.\nTo define the prior for the marginal precision \\(\\tau_b\\) and the mixing parameter \\(\\phi\\), we use the probability statements:\n\n\n\\[\n\\begin{aligned}\nP(1/\\sqrt{\\tau_b} &gt; U) &= \\alpha\\\\\nP(\\phi &lt; U) &= \\alpha\n\\end{aligned}\n\\]\n\n\n\\[\n\\begin{aligned}\nP((1/\\sqrt{\\tau_b}) &gt; 0.5/0.31) &\\equiv  P(\\sigma &gt; 1.61) = 0.01\\\\\nP(\\phi &lt; 0.5) &= 2/3 \\approx 0.66\n\\end{aligned}\n\\]\n\n\nInterpretation:\n\nPrior on \\(\\tau_b\\): low probability of large values\nPrior on \\(\\phi\\) assumes that the unstructured random effect accounts for more of the variability than the spatially structured effect."
  },
  {
    "objectID": "slides/slides_7.html#summary-of-points",
    "href": "slides/slides_7.html#summary-of-points",
    "title": "Modelling Areal Data",
    "section": "Summary of points",
    "text": "Summary of points\n\n\nSpace is inherent to many ecological and environmental phenomena\nWe can distinguish between three main types of spatial data:\n\nAreal data - Data summaried across discrete non-overlapping regions\nGeostatistical - Measurements of a continuous process measured at fixed location\nPoint process data - locations where events occur as realization of a continuous process\n\nThe spatial scale defined by the grain and extent determine the conclusion we can drawn from our analysis (MAUP and ecological fallacy).\nSpatial data are often not independent \\(\\rightarrow\\) spatial dependency can complicate statistical analysis."
  },
  {
    "objectID": "slides/slides_7.html#summary-of-points-1",
    "href": "slides/slides_7.html#summary-of-points-1",
    "title": "Modelling Areal Data",
    "section": "Summary of points",
    "text": "Summary of points\n\n\nIncorporate spatial dependence through sparse precision matrices\nSpatial dependence can be incorporated through Gaussian Markov random fields (GMRF\nGMRF conditionally dependent structures enforces sparsity into the precision matrix.\nBesag mode ( or ICAR) is a GMRF used to model spatial dependence in areal data based on a neighborhood structure.\nDifferent way of defining the neighborhood structure and matrices."
  },
  {
    "objectID": "slides/slides_7.html#summary-of-points-2",
    "href": "slides/slides_7.html#summary-of-points-2",
    "title": "Modelling Areal Data",
    "section": "Summary of points",
    "text": "Summary of points\n\n\nCAR models can be fitted using Bayesian methods\nINLA is an efficient Bayesian efficient approximation methods for LGMs.\nLGMs are defined based on:\n\nModel likelihood \\(p(y_i|u_i\\theta)\\)\nLatent GMRF \\(p(\\mathbf{u}\\mid \\theta)\\).\nVector of hyper parameter \\(p(\\theta)\\)\n\nUse Moran’s I to assess spatial dependence.\nUse BYM to extend ICAR model to account for unstructured spatial variation.\nBYM2 model provides further flexibility and interpretability\nPC priors enable intuitive construction of prios based on probability statements."
  },
  {
    "objectID": "notes/notes_8.html",
    "href": "notes/notes_8.html",
    "title": "Modelling Geostatistical Data",
    "section": "",
    "text": "Geostatistical data are the most common form of spatial data found in environmental setting. In these data we regularly take measurements of a spatially continuous ecological or process at a set of fixed locations. This could be data from transects (e.g, where the height of trees is recorded), samples taken across a region (e.g., water depth in a lake) or from monitoring stations as part of a network (e.g., air pollution). In each of these cases, our goal is to estimate the value of our variable across the entire space.\nLet \\(D\\) be our two-dimensional region of interest. In principle, there are infinite locations within \\(D\\), each of which can be represented by mathematical coordinates (e.g., latitude and longitude). We then can identify any individual location as \\(s_i = (x_i, y_i)\\), where \\(x_i\\) and \\(y_i\\) are their coordinates.\nWe can treat our variable of interest as a random variable, \\(Z\\) which can be observed at any location as \\(Z(\\mathbf{s}_i)\\).\nOur geostatistical process can therefore be written as: \\[\\{Z(\\mathbf{s}); \\mathbf{s} \\in D\\}\\]\nIn practice, our data are observed at a finite number of locations, \\(m\\), and can be denoted as:\n\\[z = \\{z(\\mathbf{s}_1), \\ldots z(\\mathbf{s}_m) \\}\\]\nWe have observed our data at \\(m\\) locations, but often want to predict this process at a set of unknown locations. For example, what is the value of \\(z(\\mathbf{s}_0)\\), where \\(\\mathbf{s}_0\\) is an unobserved site?\nThere are two main steps in classical geostatistical analysis.\nHow do I produce a statistical model for the data?\nHow do I use my model to estimate quantities of interest?\nThe first part requires us to think about how our measured data points relate to each other - in other words, to understand spatial autocorrelation. The second part requires us to use that information to predict the value at unmeasured locations, and then to produce maps or summary statistics based on this."
  },
  {
    "objectID": "notes/notes_8.html#variograms",
    "href": "notes/notes_8.html#variograms",
    "title": "Modelling Geostatistical Data",
    "section": "\n2.1 Variograms",
    "text": "2.1 Variograms\nThe first step is to assess whether there is any evidence of spatial dependency in our data.\nThe function describing the dependence between values of our process \\(Z\\) separated by different lags is known as the autocovariance function. This is similar to the autocorrelation function (ACF) used for temporal data.\nIn geostatistical models, we can summarise the covariance structure of a spatial Gaussian random field with its variogram \\(2\\gamma(\\cdot)\\) (or semivariogram \\(\\gamma(\\cdot)\\)). The variogram measures the variance of the difference in the process at two spatial locations \\(\\mathbf{s}\\) and \\(\\mathbf{s+h}\\) and is defined as (under weakly stationary): \\[\\mathrm{Var}[Z(\\mathbf{s}) - Z(\\mathbf{s} + \\mathbf{h})] = E[(Z(\\mathbf{s}) - Z(\\mathbf{s} + \\mathbf{h}))^2] = 2\\gamma_z(\\mathbf{h}).\\]\n\\[\n\\begin{aligned}\n\\mathrm{Var}[Z(\\mathbf{s}) - Z(\\mathbf{s} + \\mathbf{h})]& =  E[Z(\\mathbf{s}) - Z(\\mathbf{s} + \\mathbf{h})^2] - \\{E[Z(\\mathbf{s}) - Z(\\mathbf{s} + \\mathbf{h})]\\}^2\\\\\n&= E[Z(\\mathbf{s}) - Z(\\mathbf{s} + \\mathbf{h})^2] - \\{E[Z(\\mathbf{s})] - E[ Z(\\mathbf{s} + \\mathbf{h})]\\}^2\\\\\n&= E[Z(\\mathbf{s}) - Z(\\mathbf{s} + \\mathbf{h})^2] - \\{\\mu-\\mu\\}^2 =  E[Z(\\mathbf{s}) - Z(\\mathbf{s} + \\mathbf{h})^2] \\\\\n\\end{aligned}\n\\]\nHere, \\(2\\gamma_z(\\mathbf{h})\\) is the variogram, but in practice we use the semi-variogram, \\(\\gamma_z(\\mathbf{h})\\). We use the semi-variogram because our points come in pairs, and the semi-variance is equivalent to the variance per point at a given lag.\n\nWhen the variance of the difference \\(Z(\\mathbf{s}) - Z(\\mathbf{t})\\) is relatively small, then \\(Z(\\mathbf{s})\\) and \\(Z(\\mathbf{t})\\) are similar (spatially correlated).\nWhen the variance of the difference \\(Z(\\mathbf{s}) - Z(\\mathbf{t})\\) is relatively large, then \\(Z(\\mathbf{s})\\) and \\(Z(\\mathbf{t})\\) are less similar (closer to independence).\n\nThe variogram is a function of the underlying geostatistical process \\(Z\\). In practice, we only have access to \\(m\\) realisations of this process, and therefore we have to estimate the variogram. This is known as the empirical variogram.\nWe obtain this by computing the semi-variance for all possible pairs of observations: \\(\\gamma(\\mathbf{s}, \\mathbf{t}) = 0.5(Z(\\mathbf{s}) - Z(\\mathbf{t}))^2\\).\n\n2.1.1 Example: Constructing a Variogram\nThe data parana from the geoR Package contains the average rainfall over different years for the period May to June at 123 monitoring stations in Paraná state, Brazil.\nOur goal is to model the asses if there is spatial correlation in the data so that we can predict rainfall levels at unsampled locations. Our data consist of location coordinates and rainfall levels.\n\n\nExploratory plots\nR Code\n\n\n\n\n\n\n\nRainfall values measured at 143 recording stations in Paraná state, Brazil withlow values being represented in blue and high values in red.\n\n\n\n\n\nWe can use ggplot to map the rainfall levels as follows:\n\nCodelibrary(geoR)\nlibrary(ggplot2)\nlibrary(sf)\nlibrary(patchwork)\nlibrary(gstat)\n\ndata.frame(cbind(parana$coords, Rainfall = parana$data)) %&gt;%\nggplot()+\n  geom_point(aes(east, north, color = Rainfall), size = 2) +\n  coord_fixed(ratio = 1) +\n  scale_color_gradient(low = \"blue\", high = \"orange\") +\n  geom_path(data = data.frame(parana$border), aes(east, north)) +\n  theme_bw()\n\n\n\n\n\nTo illustrate how an empirical variogram is computed, consider the two highlighted locations below.\n\n\n\n\n\nWe can first compute the distance between the two locations using the Euclidean distance formula \\[h = \\sqrt{(475.1 - 403)^2 + (83.6 - 164.5)^2} = 108.36\\]\nNext, we compute the semi-variance between the points using their observed values as \\[\\begin{aligned}\\gamma(\\mathbf{s}, \\mathbf{s}+\\mathbf{h}) &= 0.5(Z(\\mathbf{s}) - Z(\\mathbf{s}+\\mathbf{h}))^2 \\\\ &= 0.5(315.33 - 306.9)^2 = 35.53\\end{aligned}\\]\nWe repeat this process for every possible pair of points, and plot \\(h\\) against \\(\\gamma(\\mathbf{s}, \\mathbf{t})\\) for each.\n\nWe can calculate the empirical variogram for the data using the variogram function from the gstat library. (we first need to convert our data to a sf spatial object). This plot shows the semi-variances for each pair of points.\n\n\n\n\nEmpirical variogram values corresponding to the rainfall data in Paraná state, Brazil.\n\n\n\nNotice that when we truncate the range of lag distances at which we consider spatial dependence to occur (see max.dist or cutoff for geoR and gstat respectivey in the R code above). Typically you would like set this value to approximately \\(1/2\\) to \\(2/3\\) of the total distance observed to ensure reliable estimates of spatial dependence. Beyond this, the number of point pairs available to estimate spatial dependence decreases (many points lack enough distant neighbors) leading to biased estimates of spatial dependence.\n\n\n\n\n\n\nDirectional variograms\n\n\n\nThe variograms assumed isotropy - no directionality in spatial dependence. We can subset our data based on direction (e.g., calculating four variograms for the \\(0^\\circ\\) , \\(45^\\circ\\) , \\(90^\\circ\\) , \\(135^\\circ\\) directions where \\(0^\\circ\\) cover the range from \\(-22.5 \\text{ to }22.5 ^\\circ\\)) to visually consider whether there might be evidence for anisotropy in spatial dependence.\n\n\n\n\nA strong difference in the empirical directional variograms indicate that anisotropy might be occurring in the data. Typically only 4 directions are considered (with windows \\(22.5^\\circ\\)) since any larger values (e.g., between 180 and 360 ) will provide the same patterns because the semivariance formula is symmetric.\n\n emp4.geoR &lt;- variogram(value~1, \n               data = parana_sf,\n               cutoff=400,\n               alpha = c(0, 45, 90, 135)) # specify the degree to subset the data\n \n plot(emp4.geoR,multipanel=FALSE)\n\n\n\n\n\n\n\nThe directional variograms suggests anisotropy in the process of interest.\n\n\nTo make the variogram easier to use and interpret, we divide the distances into a set of discrete bins, and compute the average semi-variance in each. We compute this binned empirical variogram as: \\[\\gamma(\\mathbf{h}) = \\frac{1}{2N(h_k)}\\sum_{(\\mathbf{s},\\mathbf{t}) \\in N(h_k)}[z(\\mathbf{s}) - z(\\mathbf{t})]^2\\]\nIngstat, we simply set the option cloud = FALSE (which is the default setting):\n\nvario_binned &lt;- gstat::variogram(value ~ 1, \n                                 data = parana_sf,\n                                 cloud = FALSE,\n                                 cutoff = 400)\nplot(vario_binned)\n\n\n\nAveraged empirical variogram values corresponding to the rainfall data in Paraná state, Brazil.\n\n\n\nOnce we have computed an empirical variogram, we can constuct null envelope based on permutations of the data values across the locations, i.e. envelopes built under the assumption of no spatial correlation. By overlapping these envelopes with the empirical variograms we can determine whether there is some spatial dependence in our data ,e.g. if our observed variograms falls outside of the envelopes constructed under spatial randomness.\nWe can construct permutation envelopes on the gstat empirical variogram using the envelope function from the variosig R package. Then we can visualize the results using the envplot function:\n\nCodelibrary(variosig)\n\nvarioEnv &lt;- envelope(vario_binned,\n                     data = parana_sf,\n                     locations = st_coordinates(parana_sf),\n                     formula = value ~ 1,\n                     nsim = 499)\n\nenvplot(varioEnv)\n\n\n\n\n\n\n\n[1] \"There are 13 out of 15 variogram estimates outside the 95% envelope.\"\n\n\nIn this example, we observe that the variogram falls outside of the null envelope at distances \\(&lt;200\\)m and also at distances above 300m."
  },
  {
    "objectID": "notes/notes_8.html#gaussian-random-field",
    "href": "notes/notes_8.html#gaussian-random-field",
    "title": "Modelling Geostatistical Data",
    "section": "\n3.1 Gaussian random field",
    "text": "3.1 Gaussian random field\nA Gaussian random field (GRF) is a collection of random variables, where observations occur in a continuous domain, and where every finite collection of random variables has a multivariate normal distribution\n\\[\n\\mathbf{z} = (z(\\mathbf{s}_1),\\ldots,z(\\mathbf{s}_m)) \\sim N(\\mu(\\mathbf{s}_1),\\ldots,\\mu(\\mathbf{s}_m),\\Sigma),\n\\]\nwhere \\(\\Sigma_{ij} = \\mathrm{Cov}(z(\\mathbf{s}_i),z(\\mathbf{s}_j))\\) is a dense \\(m \\times m\\) matrix and measures the strength of the linear dependence between \\(Z(\\mathbf{s}_i)\\) and \\(Z(\\mathbf{s}_j)\\) (As usual, we can compute the variance of \\(Z(\\mathbf{s}_i)\\) as a special case of the covariance where \\(\\mathbf{s}_i = \\mathbf{s}_j\\)).\nWe then need to use a covariance function \\(C_z(\\cdot,\\cdot)\\) that depends on the distance (\\(\\Sigma_{ij} = C_z(\\mathbf{s}_i,\\mathbf{s}_j)\\)) between two points and that\n\nhas no negative variances\nis symmetric\nis decreasing, with maximum at distance = 0\n\n\n\n\n\n\n\nRandom fields as random functions\n\n\n\nOne way to think about random fields is as a way of defining a probability distribution on functions. Given our spatial region of interest \\(D\\), We want to include a spatial random effect in the model because we believe there is some relationship between location and our response variable, but we don’t know exactly what that relationship is. Since there are an infinite number of locations in \\(D\\), there are an infinite number of possible values that \\(f\\) could take. A random field can be thought of as defining a distribution on what functions \\(f\\) can be. Recall that a GRF is essentially nothing more than a rule to define the joint distribution of the values of \\(f\\) for any (finite) collection of locations.\n\n\n\n3.1.1 Stationary and isotropy\nOur geostatistical process can be described as weakly stationary if the following criteria are met:\n\n\\(E[{Z(\\mathbf{s})}] = \\mu_z(\\mathbf{s}) = \\mu_z\\) - a finite constant which does not depend on \\(\\mathbf{s}\\).\n\\(C_z(\\mathbf{s}, \\mathbf{s+h}) = C_z(\\mathbf{h})\\) - a function that depends only on the lag \\(\\mathbf{h}\\) and not on the absolute location.\n\n\nCondition 1 states that our mean function must be constant in space, with no overall spatial trend. We typically assume that after accounting for the deterministic part of the model \\(\\mathbb{E}[Z(\\mathbf{s})] = 0\\)\nCondition 2 states that for any two locations, their covariance depends only on how far apart they are (their spatial lag, \\(h\\)), not their absolute position.\n\nA geostatistical process is said to be isotropic if the covariance function is directionally invariant. This means that the covariance between two points a distance \\(h\\) apart is the same no matter which direction you travel in.\nMathematically, this can be denoted by \\[C_z(\\mathbf{h}) = C_z(||\\mathbf{h}||).\\]"
  },
  {
    "objectID": "notes/notes_8.html#defining-the-gaussian-field-in-a-model",
    "href": "notes/notes_8.html#defining-the-gaussian-field-in-a-model",
    "title": "Modelling Geostatistical Data",
    "section": "\n3.2 Defining the Gaussian Field in a model",
    "text": "3.2 Defining the Gaussian Field in a model\nThe first step in defining a model for a random field in a hierarchical framework is to identify a probability distribution for the observations available at \\(m\\) sampled spatial locations and represented by the vector \\(\\mathbf{y} = y_1,\\ldots,y_m\\).\nFor example, if we assume our observations follow a Gaussian distribution then\n\\[\n\\begin{aligned}\nY_i &\\sim N(\\mu_i,\\tau_e^{-1}) \\text{ or } Y_i = \\mu_i + \\varepsilon_i ~~ \\varepsilon \\sim N(0,\\tau_e^{-1})\\\\\n\\eta_i &=\\mu_i = \\beta_0 + \\ldots + z(\\mathbf{s}_i)\n\\end{aligned}\n\\]\n\n\\(\\tau_e^{-1} = \\sigma^2_e\\) represents the variance of the zero-mean measurement error (equivalent to the nugget effect)\n\nThe response mean \\(\\mu_i\\) which coincides with the linear predictor \\(\\eta_i\\) is defined based on:\n\nthe intercept \\(\\beta_0\\) and any additional covariates\nthe realization of the latent (unobservable) GF \\(Z(\\mathbf{s}) \\sim \\mathrm{MVN}(0,\\Sigma)\\) which accounts for the spatial correlation through \\(\\Sigma = C_z(\\cdot,\\cdot)\\).\n\n\n\n\n3.2.1 Finite basis representation\nAt first glance, implementing such a model in practice can seem challenging. We operate in a world with limited computational power and finite memory, so working with fully general function spaces is not feasible. To make the problem manageable, we restrict the function space for \\(f\\). In other words, we define a set of possible functions using a finite collection of basic “building blocks.”\nIn practice, for Gaussian random fields (GRFs), we instead choose a finite basis and write\n\\[\nf(\\mathbf{s}) = \\sum_{j=1}^M z_j \\phi_j(\\mathbf{s}),\n\\]\nwhere \\(\\phi_1, \\ldots, \\phi_M\\) known (non-random) functions and coefficients \\(\\mathbf{z}_1, \\ldots, \\mathbf{z}_M\\) that we wish to estimate. Under this setup, defining a GRF is equivalent to specifying a joint probability distribution for the coefficient vector \\(\\mathbf{z}= [z_1, \\ldots, z_M]^\\intercal\\). In summary, practical modelling requires restricting functions to finite bases for computational tractability, which leads us to place distributions on coefficients and carefully manage covariance or precision structures. This naturally highlights the importance of selecting a sensible covariance function, as it determines the dependence structure, computational efficiency, and ultimately how well the model captures the underlying spatial process."
  },
  {
    "objectID": "notes/notes_8.html#the-matérn-field",
    "href": "notes/notes_8.html#the-matérn-field",
    "title": "Modelling Geostatistical Data",
    "section": "\n3.3 The Matérn Field",
    "text": "3.3 The Matérn Field\nA commonly used covariance function is the Matérn covariance function. The covariance of two points which are a distance \\(h\\) apart is:\n\\[\n    \\Sigma =C_{\\nu}(h) = \\sigma^2 \\frac{2^{1-\\nu}}{\\Gamma(\\nu)} \\left( \\frac{\\sqrt{2\\nu} h}{\\rho} \\right)^{\\nu} K_{\\nu} \\left( \\frac{\\sqrt{2\\nu} h}{\\rho} \\right)\n\\tag{1}\\]\n\n\\(\\Gamma(\\cdot)\\) is the gamma function\n\\(K_{\\nu}(\\cdot)\\) is the modified Bessel function of the second kind.\n\nParameters \\(\\sigma^2\\), \\(\\rho\\) and \\(\\nu\\) are non-negative values of the covariance function.\n\n\\(\\sigma^2\\) is the spatially structured variance component\n\\(\\rho\\) is the range of the spatial process\n\\(\\nu\\) controls smoothness of the spatial process.\n\n\n\nBig n problem!\nThe disadvantage of the modelling approach involving the spatial covariance function is known as “big n problem” and concerns the computational costs required for algebra operations with dense covariance matrices (such as \\(\\Sigma\\)).\nIn particular dense matrix operations scale cubically with the matrix size, given by the number of locations where the process is observed. Various approximation techniques have therefore been developed to address this issue; for example, the mgcv framework often uses low-rank approximations for smoothing splines.\nThe key to the approach in INLA is to avoid using the covariance matrix as much as possible and to instead work with the precision matrix. INLA deals with the computational challenges of GRFs by considering fields with sparse precision matrices. Such fields are Gaussian Markov random fields!. This is the basis for the main approach we will discuss, where a computationally efficient alternative is provided by the SPDE approach for representing GRFs as GMRFs."
  },
  {
    "objectID": "notes/notes_8.html#the-spde-approach",
    "href": "notes/notes_8.html#the-spde-approach",
    "title": "Modelling Geostatistical Data",
    "section": "\n3.4 The SPDE approach",
    "text": "3.4 The SPDE approach\nThe SPDE approach to modelling GMRFs was introduced by Lindgren et. al. (2011), and defines a (Matérn) GRF as the solution of a stochastic partial differential equation (SPDE)\n\\[\n(\\kappa^2-\\Delta)^{\\alpha/2}Z(s) = W(s)\n\\]\nWhat is this?\n\n\n\\(W(s)\\) is random noise\n\n\\(Z(s)\\) is the smooth process we want\n\n\\((\\kappa^2-\\Delta)^{\\alpha/2}\\) is an operator that “smooths” the white noise.\n\n\\(\\kappa\\) and \\(\\alpha\\) are parameters\n\n“solving the SPDE” means Find a random function \\(Z(t)\\) such that the equality \\((\\kappa^2-\\Delta)^{\\alpha/2}Z(t) = W(t)\\) holds in distribution. Lindgren et. al. (2011) represent the solution as\n\\[\nZ(t) = \\sum_{i = 1}^T\\psi_i(t)w_i\n\\]\nWhere\n\n\n\\(\\psi_i(s)\\) are (known) basis functions for nodes \\(i=1,\\ldots,T\\)\n\n\\(\\psi_i(s_i)= 1\\)\n\\(\\psi_i(s_j) = 0 ~~\\forall~~i \\neq j\\)\nLinear between neighboring nodes\n\n\n\n\\(w_i\\) are (unknown) weights\n\nthe field value \\(Z(s)\\) is a linear interpolation between the two neighboring weights\n\n\n\nThis solution, which completely defined by a Gaussian vector of weights with zero mean and a sparse precision matrix, is approximated using a finite combination of piece-wise linear basis functions.\nIn practice, we approximate the GRF using a triangulated mesh. Then, the SPDE approach represents the continuous spatial process as a continuously indexed Gaussian Markov Random Field (GMRF). In other words, we construct an appropriate lower-resolution approximation of the surface by sampling it in a set of well designed points and constructing a piece-wise linear interpolant.\n\n\n\n\n\n\nNote\n\n\n\nThe solution, as presented in Lindgren et. al. (2011) is a Matérn field Equation 1 where \\(\\nu = \\alpha - d/2\\). For example, when \\(\\alpha=2 \\Rightarrow \\nu= 1\\) since \\(d=2\\) we have that:\n\\[\n\\begin{aligned}\nZ(s) &= \\sum_{i = 1}^K\\psi_i(s)w_i \\\\\n\\mathbf{w} &\\sim N(\\mathbf{0},Q^{-1}) \\leftarrow \\text{GMRF}\\\\\nQ^{-1} &= \\tau^2(\\kappa^4 \\mathbf{C} + 2\\kappa^2 \\mathbf{G}+\\mathbf{G}\\mathbf{C}^{-1}\\mathbf{G})\n\\end{aligned}\n\\]\n\n\\(\\mathbf{C}\\) is diagonal with entries \\(C_{ii} =\\int \\psi_i(s)\\mathrm{d}s\\) and measures how much of the domain each basis function covers.\n\\(G_{ij} = \\int \\nabla \\psi_i(s) \\nabla \\psi_j(s) \\mathrm{d}s\\) reflects the connectivity of the mesh nodes.\nbecause each basis function overlaps only with nearby ones, the resulting precision matrix is sparse, meaning each coefficient depends directly only on its neighbors\n\n\n\nIn summary\n\nThe continuous stationary and isotropic Matérn GRF is the solution of a SPDE and is represented as\n\n\\[\nZ(s) = \\sum_{i = 1}^K\\psi_i(s)w_i\n\\]\n\nThe weights vector \\(\\mathbf{w} = (w_1,\\dots,w_K)\\) is Gaussian with a sparse precision matrix \\(\\longrightarrow\\) Computational convenience\n\nThe field has two parameters\n\nThe range \\(\\rho\\)\n\nThe marginal variance \\(\\sigma^2\\)\n\n\n\nThese parameters are linked to the parameters of the SPDE\nWe need to assign prior to them"
  },
  {
    "objectID": "notes/notes_8.html#penalized-complexity-priors",
    "href": "notes/notes_8.html#penalized-complexity-priors",
    "title": "Modelling Geostatistical Data",
    "section": "\n3.5 Penalized Complexity Priors",
    "text": "3.5 Penalized Complexity Priors\nPenalized Complexity (PC) priors proposed by Simpson et al. (2017) allow us to control the amount of spatial smoothing and avoid overfitting.\n\nPC priors shrink the model towards a simpler baseline unless the data provide strong evidence for a more complex structure.\nTo define the prior for the marginal precision \\(\\sigma^{-2}\\) and the range parameter \\(\\rho\\), we use the probability statements:\n\nDefine the prior for the range \\(\\text{Prob}(\\rho&lt;\\rho_0) = p_{\\rho}\\)\n\nDefine the prior for the range \\(\\text{Prob}(\\sigma&gt;\\sigma_0) = p_{\\sigma}\\)"
  },
  {
    "objectID": "notes/notes_8.html#the-model-and-spde-representation",
    "href": "notes/notes_8.html#the-model-and-spde-representation",
    "title": "Modelling Geostatistical Data",
    "section": "\n4.1 The Model and SPDE representation",
    "text": "4.1 The Model and SPDE representation\n\nStage 1 Model for the response \\[\ny(s)|\\eta(s)\\sim\\text{Normal}(\\mu(s),\\sigma^2_e)\n\\]\n\nStage 2 Latent field model \\[\n\\eta(s) = \\mu(s) = \\beta_0 + Z(s)\n\\]\n\nA global intercept \\(\\beta_0\\)\n\nA Gaussian field \\(Z(s)\\)\n\n\n\n\nStage 3 Hyperparameters\n\nPrecision for the observational error \\(\\tau_e = 1/\\sigma^{2}_e\\)\n\nRange and sd in the Gaussian field \\(\\sigma_{\\omega}, \\tau_{\\omega}\\)"
  },
  {
    "objectID": "notes/notes_8.html#the-workflow",
    "href": "notes/notes_8.html#the-workflow",
    "title": "Modelling Geostatistical Data",
    "section": "\n4.2 The workflow",
    "text": "4.2 The workflow\nWhen fitting a geostatistical model we need to go through the following steps:\n\nBuild the mesh\nDefine the SPDE representation of the spatial GF. This includes defining the priors for the range and sd of the spatial GF\nDefine the components of the linear predictor. This includes the spatial GF and all potential covariates\nDefine the observation model\n\n1. The mesh\nFirst, we need to create the mesh used to approximate the random field. For this purpose we use the function fm_mesh_2d from the fmesher package. One way to build the mesh is to start from the locations where we have observations, these are contained in the dataset.\n\nCodelibrary(fmesher)\nlibrary(inlabru)\nlibrary(INLA)\n\nmesh &lt;- fm_mesh_2d(\n  loc = parana_sf,\n  offset = c(50, 100),\n  cutoff = 1,\n  max.edge = c(30, 60)\n)\n\n\nHere\n\n\nmax.edge for maximum triangle edge lengths\n\noffset for inner and outer extensions (to prevent edge effects)\n\ncutoff to avoid overly small triangles in clustered areas\n\nYou can use the plot(mesh) function to visualize the mesh.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nGeneral guidelines for creating the mesh\n\nCreate triangulation meshes with fm_mesh_2d()\n\nMove undesired boundary effects away from the domain of interest by extending to a smooth external boundary\nUse a coarser resolution in the extension to reduce computational cost (max.edge=c(inner, outer))\nUse a fine resolution (subject to available computational resources) for the domain of interest (inner correlation range) and filter out small input point clusters (0 &lt; cutoff &lt; inner)\nCoastlines and similar can be added to the domain specification in fm_mesh_2d() through the boundary argument.\n\n\n\n\n\n\n\n\n\n Task\n\n\n\nLook at the documentation for the fm_mesh_2d function typing\n\nCode?fm_mesh_2d\n\n\nplay around with the different options and create different meshes.\nThe rule of thumb is that your mesh should be:\n\nfine enough to well represent the spatial variability of your process, but not too fine in order to avoid computation burden\nthe triangles should be regular, avoid long and thin triangles.\nThe mesh should contain a buffer around your area of interest (this is what is defined in the offset option) in order to avoid boundary artefact in the estimated variance.\n\n\n\n2. Define the SPDE representation of the spatial GF\nTo define the SPDE representation of the spatial GF we use the function inla.spde2.pcmatern. This takes as input the mesh we have defined and the PC-priors definition for \\(\\rho\\) and \\(\\sigma\\) (the range and the marginal standard deviation of the field).\nPC priors Gaussian Random field are defined in (Fuglstad et al. 2018). From a practical perspective for the range \\(\\rho\\) you need to define two paramters \\(\\rho_0\\) and \\(p_{\\rho}\\) such that you believe it is reasonable that\n\\[\nP(\\rho&lt;\\rho_0)=p_{\\rho}\n\\]\nwhile for the marginal variance \\(\\sigma\\) you need to define two parameters \\(\\sigma_0\\) and \\(p_{\\sigma}\\) such that you believe it is reasonable that\n\\[\nP(\\sigma&lt;\\sigma_0)=p_{\\sigma}\n\\]\nThe Paraná state is around 663.8711 kilometers width by 464.7481 kilometers height. The PC-prior for the practical range is built considering the probability of the practical range being less than a chosen distance. We chose to set the prior considering the median as 100 kilometers.\n\nCodespde_model =  inla.spde2.pcmatern(mesh,\n                                  prior.sigma = c(1, 0.5),\n                                  prior.range = c(100, 0.5))\n\n\n3. Define the components of the linear predictor\nWe have now defined a mesh and a SPDE representation of the spatial GF. We now need to define the model components:\n\nCodecmp = ~ Intercept(1) + space(geometry, model = spde_model)\n\n\nNOTE since the data frame we use (parana_sf) is an sf object the input in the space() component is the geometry of the dataset.\n4. Define the observation model\nOur data are Gaussian distributed so we can define the observation model as:\n\nCode# define model predictor formula\neta = value ~ Intercept  + space\n\n# build the observation model\nlik = bru_obs(formula = eta,\n              data = parana_sf,\n              family = \"gaussian\")\n\n\n5. Run the model\nFinally we are ready to run the model\n\nCodefit = bru(cmp,lik)"
  },
  {
    "objectID": "notes/notes_8.html#explore-the-results-posterior-summaries",
    "href": "notes/notes_8.html#explore-the-results-posterior-summaries",
    "title": "Modelling Geostatistical Data",
    "section": "\n4.3 Explore the results: Posterior summaries",
    "text": "4.3 Explore the results: Posterior summaries\nThe results of fitting the mode above are:\n\n\n\n\n\n\n\n\n\nMean\n2.5%\n97.5%\n\n\n\nIntercept\n249.714\n232.748\n264.983\n\n\nPrecision for the Gaussian observations\n4.482\n3.197\n5.511\n\n\nRange for space\n57.328\n46.234\n70.330\n\n\nStdev for space\n46.654\n41.222\n52.736\n\n\n\nHere,\n\n\nIntercept represents the average rainfall values\n\nPrecision for the Gaussian observations are the observational errors\n\nRange for space is the correlation of the Matérn field\n\nStdev for space is the marginal std deviation of the Matérn field"
  },
  {
    "objectID": "notes/notes_8.html#spatial-predictions",
    "href": "notes/notes_8.html#spatial-predictions",
    "title": "Modelling Geostatistical Data",
    "section": "\n4.4 Spatial Predictions",
    "text": "4.4 Spatial Predictions\nIn geostatistical applications, the main interest resides in the spatial prediction of the spatial latent field or of the response variable in new locations\n\nSuppose we observe a spatial process \\({Z(s): s \\in \\mathcal{D}}\\) at locations \\(s_1,\\dots,s_n\\).\n\nOur goal: predict the variable of interest at an unobserved location \\(s_0 \\in \\mathcal{D}\\).\n\ngiven the data \\(y = (y_1,\\dots,y_n)\\), what can we say about \\(Z(s_0)\\)?\n\n\nRather than a single guess, we want a full uncertainty-aware prediction.\nIn a Bayesian setting, prediction is a probabilistic task.\n\nThe key lies in the posterior predictive distribution\n\\[\n  \\pi(\\tilde{Y} \\mid y)\n  = \\int \\pi(\\tilde{Y} \\mid \\Theta, y)\\, \\pi(\\Theta \\mid y)\\, d\\Theta,\n\\]\nwhere \\(\\Theta\\) denotes all latent components and hyperparameters.\n\nThe prediction likelihood \\(\\pi(\\tilde{Y} \\mid \\Theta, y)\\) depends on the task:\n\nextrapolation(e.g. forecasting of an AR(1)): \\(\\pi(Y_{n+1} \\mid \\Theta, y_n)\\),\n\ninterporlation: \\(\\pi(Y_i \\mid y_{i-1}, y_{i+1}, \\Theta)\\),\n\n\nSpatial prediction fits naturally into this framework:\n\n\n\\(\\tilde{Y}\\) may represent \\(Z(s_0)\\), \\(\\eta_0\\), or the response at \\(y(s_0)\\),\nconditioning reflects the assumed spatial dependence.\n\n\nINLA approximates \\(\\pi(\\Theta \\mid y)\\) efficiently, enabling full uncertainty propagation when predicting over \\(s_0 \\in \\mathcal{D}\\).\n\nWe now want to extract the estimated posterior mean and sd of spatial GF. To do this we first need to define a grid of points where we want to predict. We do this using the function fm_pixel() which creates a regular grid of points covering the mesh\n\nCode# mask layer to the state border\nborder &lt;- st_as_sf(data.frame(parana$borders),coords =c(\"east\",\"north\")) %&gt;%\n   summarise(geometry = st_combine(geometry)) %&gt;%\n  st_cast(\"POLYGON\")\n# resolution of our predictions\ndims = c(150, 150)\n# data frame for predictions\npred.df &lt;- fm_pixels(mesh,dims = dims,mask =border,  format = \"sf\")\n\n\nthen compute the prediction for both the spatial GF and the linear predictor (spatial GF + intercept)\n\nCodepred &lt;- predict(fit,pred.df,\n                formula ~ data.frame(\n    spde =  space,\n    eta = Intercept + space\n  )\n)\n\n\nFinally, we can plot the maps\n\n\n\n\nNow lets look at a more complex example (we will dicuss it in more detail during the lecture and the lab session)"
  },
  {
    "objectID": "notes/notes_8.html#exploring-the-data",
    "href": "notes/notes_8.html#exploring-the-data",
    "title": "Modelling Geostatistical Data",
    "section": "\n5.1 Exploring the Data",
    "text": "5.1 Exploring the Data\nIn the next example, we will explore data on the Pacific Cod (Gadus macrocephalus) from a trawl survey in Queen Charlotte Sound.\n\nThe dataset the biomass density (kg/km\\(^2\\)) of Pacific cod in the area swept for a given survey in 2003 as well as depth covariate information.\n\n\n\n\n\n\n\n\nFigure 1: Map of the locations where Pacfic Cod were caught and the depth if the study area\n\n\n\n\nFigure 2 A suggests some sort of quadratic effect of depth values, while Figure 2 B indicates a near Gaussian distribution for the log biomass density values and an important amount of zeros in our data.\n\n\n\n\n\n\n\nFigure 2: plot A shows the relationship bewteen the biomass density and the squared-depth values (previously scaled). Plot B shows the distirbution of log biomass density\n\n\n\n\nIf we consider only the locations where biomass \\(&gt;\\) 0, the Envelope Variogram ( Figure 3) of the log-density some unaccounted spatial variation at larger distances, which is not too bad. However, we then have a dilemma:\n\nIf we omit the zeros, we’ll get a good, accurate model fit for non-zero data, but we’ll be throwing away all the data with zeros\nIf we include the zeros, we won’t be throwing any data away, but we’ll get a strange-fitting model that both under- and over-predicts values.\nSo what do we do?\n\n\n\n[1] \"There are 2 out of 15 variogram estimates outside the 95% envelope.\"\n\n\n\n\n\n\n\nFigure 3: Monte Carlo Envelop for log biomass density values at locations where biomass in &gt; 0."
  },
  {
    "objectID": "notes/notes_8.html#a-first-non-spatial-model",
    "href": "notes/notes_8.html#a-first-non-spatial-model",
    "title": "Modelling Geostatistical Data",
    "section": "\n5.2 A first non-spatial model",
    "text": "5.2 A first non-spatial model\nA sensible non-spatial hierarchical Bayesian LGM for these that can be represented as follows:\nFirst, we need to specify an observational model for our data:\n\n\nStage 1 Model for the response \\[\ny(s)|\\eta(s)\\sim \\text{Tweedie}(p,\\mu_i,\\phi)\n\\]\n\nHere, the Tweedie distribution account for positive continuous density values that also contain zeros\n\\(p\\) determines the shape of the variance function (shifts from a Poisson distribution at \\(p=1\\) to a gamma distribution at \\(p=2\\))\n\\(\\mu(s) = \\exp⁡\\eta (i)\\) is the mean linked to linear predictor by the log link.\n\\(\\phi\\) = dispersion parameter .\n\n\n\nThen, we need to specify the components of the latent field:\n\n\nStage 2 Latent field model \\[\n\\eta(s) = \\exp(\\mu(s)) = \\beta_0 + \\beta_1 x(s) + \\beta_2 x(s)^2\n\\]\n\nA global intercept \\(\\beta_0\\)\n\nA effect of covariate \\(x(s)\\) (depth)\nA quadratic effect of covariate \\(x(s)\\) (depth)\n\n\n\nAnd finally, state what the hyperparameters of our model are:\n\n\nStage 3 Hyperparameters\n\ndispersion parameter \\(\\phi\\)\n\npower parameter \\(p\\)\n\n\n\n\nNotice that these hyperparameter are associated with our observational model only as we did include any random effect on the linear predictor. The results for fitting this model are summarised in Table 1. We can see that \\(\\beta\\) coefficients suggest log-biomass peaks at an intermediate depth within the study range and decreases toward both shallower and deeper extremes. Then, \\(\\phi&gt;1\\) indicates overdispersion relative to the variance function. Potentially caused by unobserved heterogeneity. This suggest a spatially explicit model might be more appropriate here. However, Tweedie models fitted to biomass usually have convergence issues when there are large spatial areas with many zeros.\n\n\n\nTable 1: Non spatial Tweedie Model posterior summaries\n\n\n\n\n\n\n\nINLA Model Results\n\n\nPosterior summaries of fixed effects and hyperparameters\n\n\nParameter\nMean\n2.5% Quantile\n97.5% Quantile\n\n\n\n\nIntercept\n3.866\n3.690\n4.043\n\n\ndepth\n−1.269\n−1.480\n−1.059\n\n\ndepth2\n−1.077\n−1.256\n−0.898\n\n\np parameter for Tweedie\n1.643\n1.617\n1.668\n\n\nDispersion parameter for Tweedie\n3.804\n3.530\n4.093"
  },
  {
    "objectID": "notes/notes_8.html#a-multilikelihood-hurdle-geostatistical-model",
    "href": "notes/notes_8.html#a-multilikelihood-hurdle-geostatistical-model",
    "title": "Modelling Geostatistical Data",
    "section": "\n5.3 A multilikelihood Hurdle Geostatistical Model",
    "text": "5.3 A multilikelihood Hurdle Geostatistical Model\nHere we present a multilikelihood approach to jointly model the the log-biomass density while accounting for the presence of zeros in our data. A two-part model can be constructed to accommodate zero-inflated continuous data by combining separate likelihoods: one for the occurrence (e.g., Bernoulli) and one for the conditional positive amount (e.g., log-normal). The primary advantage of this framework is the ability to model the probability of an event and its magnitude independently.\n\n\nStage 1 Model for the response(s)\\[\n\\begin{aligned}\ny_i|\\eta^{(1)}_i&\\sim \\text{Binomial}(1,\\pi_i)\\\\\n\\log(z_i)|\\eta^{(2)}_i&\\sim \\text{Normal}(\\mu_i,\\tau_e^{-1})\\\\\n\\end{aligned}\n\\]\n\n\nWe then define a likelihood for each outcome.\n\n\\(y_i =\\begin{cases} 1 &\\text{if fishes have been caught at location } \\mathbf{s}_i  \\\\ 0 &\\text{otherwise}\\end{cases}\\)\n\\(z_i =\\begin{cases} NA &\\text{if no fish were  caught at location } \\mathbf{s}_i  \\\\ \\text{biomass density at location } \\mathbf{s}_i &\\text{otherwise}\\end{cases}\\)\n\n\n\n\n\nThis structure is equivalent to a Hurdle-log-Normal model, where the overall expected value of log biomass is given by the product \\(\\pi_{i} * \\mu_{i}\\), with \\(\\mu_{i}\\) representing the conditional expectation from the log-normal component.\nNext we define the components of our linear predictor. Notice how we are defining this model in a LGM framework:\n\n\nStage 2 Latent field model \\[\n\\begin{aligned}\n\\eta^{(1)}_i &= \\text{logit}(\\pi_i) = X'\\beta + \\xi_i\\\\\n\\eta^{(2)}_i &= \\mu_i = X'\\alpha + \\omega_i\n\\end{aligned}\n\\]\n\n\n\\(\\{\\alpha,\\beta\\}\\) = Intercepts + covariate effects.\n\n\\(\\{\\xi,\\omega\\}\\) = are the Gaussian fields with Matérn covariance (separate for each outcome).\n\n\n\nFor the occurrence of fish, the linear predictor gets mapped to the logit of the probability of the Bernoulli model while the linear predictor for the biomass density is mapped to the mean of a log normal distribution.\n\n\nStage 3 Hyperparameters\n\nThe hyperparameter for the model are:\n\nobservational error (nugget) \\(\\tau_e\\)\n\nMatérn field(s) parameters \\(\\{\\rho^{(1)},\\rho^{(2)},\\tau_{d}^{(1)},\\tau_{d}^{(2)}\\}\\)"
  },
  {
    "objectID": "notes/notes_8.html#posterior-summaries",
    "href": "notes/notes_8.html#posterior-summaries",
    "title": "Modelling Geostatistical Data",
    "section": "\n5.4 Posterior summaries",
    "text": "5.4 Posterior summaries\nTable 2 shows the posterior summaries of the Hurdle model fitted to the biomass density data. Here,\n\n\n\\(\\alpha_0\\) is the baseline catching probability on the logit scale\n\n\\(\\beta_0\\) is the predicted log(biomass density at the average depth (since these have been scaled)\nCoefficients \\(\\alpha_1,\\alpha_2\\) refer to the change in the log-odds of catching fish as we increase 1 depth unit and unit\\(^2\\) respectively.\nCoefficients \\(\\beta_1, \\beta_2\\) indicate that the log-biomass decreases with depth.\n\n\\(\\rho^{[1]},\\rho^{[2]}\\), suggest spatial correlation decays at 36.66 and 158.36 Km respectively (the extension of the study is approx 46,000 km\\(^2\\))\nunstructured variability is given by \\(\\tau^{-1}_e\\) while ,\\(\\{\\tau_{\\delta,1}^{-1},\\tau^{-1}_{d,2}\\}\\) represent the spatially structured variability.\n\n\n\n\nTable 2: Posterior summaries of Hurdle model fitted to the biomass density data\n\n\n\n\n\n\nParameter\nMean\n2.5% Quantile\n97.5% Quantile\n\n\n\n\\(\\alpha_0\\)\n3.498\n2.885\n4.227\n\n\n\\(\\alpha_1\\)\n−0.509\n−1.098\n0.058\n\n\n\\(\\alpha_2\\)\n−0.339\n−0.789\n0.106\n\n\n\\(\\beta_0\\)\n1.761\n−3.216\n7.268\n\n\n\\(\\beta_1\\)\n−2.564\n−3.626\n−1.694\n\n\n\\(\\beta_2\\)\n−1.494\n−2.143\n−0.944\n\n\n\\(\\tau_\\epsilon^2\\)\n1.151\n0.464\n2.571\n\n\n\\(\\rho^{[1]}\\)\n36.656\n6.882\n103.808\n\n\n\\(\\tau_{d,1}\\)\n1.001\n0.571\n1.657\n\n\n\\(\\rho^{[2]}\\)\n158.355\n56.995\n398.848\n\n\n\\(\\tau_{d,2}\\)\n2.201\n1.192\n3.769"
  },
  {
    "objectID": "notes/notes_8.html#model-comparison",
    "href": "notes/notes_8.html#model-comparison",
    "title": "Modelling Geostatistical Data",
    "section": "\n5.5 Model comparison",
    "text": "5.5 Model comparison\nNote that in the hurdle model there is no direct link between the parameters of the two observation parts. However, the two likelihoods could share some of the components; for example the Matérn field could be used for both predictors. Thus, we will fit a model that estimates this field jointly and compare it with our two previous models (we will cover how to fit this model on the lab)\n\n\n\n\n\n\nModel\nDIC\nWAIC\nMLIK\n\n\n\nTweedie\n1,599.690\n1,612.043\n−979.532\n\n\nHurdle\n1,185.786\n1,200.688\n−660.938\n\n\nHurdle 2\n1,227.140\n1,227.351\n−666.295"
  },
  {
    "objectID": "notes/notes_8.html#spatial-prediction",
    "href": "notes/notes_8.html#spatial-prediction",
    "title": "Modelling Geostatistical Data",
    "section": "\n5.6 Spatial prediction",
    "text": "5.6 Spatial prediction\nLastly we can compute the spatial prediction for the biomass density. To do so we need to compute:\n\n\\(\\pi(s)\\) = Catching probability\n\\(\\mathbb{E}[Z(s)|Y(s)] = \\exp\\left(\\mu(s) + \\dfrac{1}{2\\tau_{e}}\\right)\\)\n\\(\\mathbb{E}(Z(s)) =\\pi(s)\\times \\mathbb{E}[Z(s)|Y(s)]\\)"
  },
  {
    "objectID": "notes/tutorial_sheet_4_solutions.html",
    "href": "notes/tutorial_sheet_4_solutions.html",
    "title": "Tutorial Sheet 4",
    "section": "",
    "text": "1 Areal processes\nThe figure below shows coloured dissolved organic matter (CDOM) across a lake along with the corresponding Moran’s I plot (Figure 1).\n\n\n\n\n\n\nFigure 1: Lake CDOM plotted over space (left) and the corresponding Moran’s I plot (right) based on Queen’s distance\n\n\n\nThe estimated Global Moran’s I is 0.02 with a variance of 0.02.\n\n\n\n\n\n\nInterpreting Moran’s I plot\n\n\n\nA Moran’s I scatter plot visualizes spatial autocorrelation by plotting the values of observations in different areas against their spatially lagged values, which represent the weighted average of neighboring values. A positive correlation in the plot suggests spatial clustering (similar values are located near each other), while a negative correlation indicates spatial dispersion (neighboring values tend to be dissimilar). The slope of the regression line through the points provides an estimate of Moran’s I, a statistic that quantifies the degree of spatial autocorrelation.\n\n\n\n\n\n\n\n\n Task 1\n\n\n\nDefine what is meant by the term neighbourhood matrix and discuss two approaches for defining it, giving a drawback of each\n\n\nSolution\n\n\nBinary: Set the ijth element \\(w_{ij}\\) of \\(W\\) equal to 1 if areas (i; j) share a common border and equal to zero otherwise.\n\nDisadvantages: For small geographical units this can lead to two units being close together but not treated as being spatially close. Length of shared border is not accounted for.\nAdvantage: Don’t need to specify a distance (but there are options of what constitutes a common border if regular shaped grid data, like pixels are used)\n\nDistance: Set \\(w_{ij} = 1\\) if the distance between the central points of each unit is less than a pre-specified threshold \\(d\\), and \\(w_{ij} = 0\\) otherwise.\n\nDisadvantages; Threshold \\(d\\) is arbitrary and needs to be chosen by the user, how do we choose d?\nAdvantages: small geographical units can be neighbours if close together but do not share a common border.\n\n\n\n\n\n\n\n\n\n\n\n Task 2\n\n\n\nComment on the spatial variability in CDOM for this lake, with specific reference to the Moran’s I plot (Figure 1).\n\n\nSolution\n\nNo evidence of spatial correlation in the dataset using the Moran’s I plot. There are points in each quadrant of the plot, and the slope of the line, which represents the Moran’s I value, is very flat (close to 0).\n\n\n\n\n\n\n\n\n\n Task 3\n\n\n\nIn your own words what does the Modifiable Areal Unit Problem (MAUP) refers to?\n\n\nSolution\n\nMAUP is a statistical bias that occurs when data are aggregated into different areal units. The MAUP can lead to different results depending on the scale of the analysis. For example, if you aggregate data at the county level, you may get different results than if you aggregate data at the state level.\n\n\n\n\n\n\n\n\n\n Task 4\n\n\n\nFigure 2 shows the number of lung cancer cases in Pennsylvania per county, in 2002.\n\n\n\n\n\n\n\n\nFigure 2: Number of Lung Cancer cases in Pennsylavnia counties in 2002\n\n\n\n\n\n\nInterpret the map , what does this tells you about the number of lung cancer cases?\nIn addition to the number of cases,,, the data set contains county-level information of the following variables:\n\n\nExpected number of cases \\(E_i\\) computed as \\(E_i = \\sum_{j=1}^{m} r_j \\times n_j\\) ( \\(r_j\\) is rate of disease and \\(n_j\\) the population in stratum \\(j\\))\nOverall county smoking rate\n\nPropose a reasonable spatially explicit LGM to assess the relationship between lung cancer risk and smoking.\n\n\nSolution\n\n\nYou can see that a few counties—especially one in the southwest—have much higher numbers (dark blue), while most counties have relatively low to moderate cases (lighter shades). Overall, lung cancer cases are unevenly distributed, with certain areas experiencing significantly higher concentrations than others.\nA possible LGM for disease risk modelling could be a BYM with the following structure:\n\n\nobservational model: \\(Y_i \\sim \\mathrm{Poisson}(\\theta_i \\times E_i)\\), where \\(Y_i\\) represent the number of cases, \\(\\theta_i\\) is the relative risk, and \\(E_i\\) the expected number of cases\nthe latent field where the latent components of our model’s linear predictor are linked to the RR as \\(\\log (\\theta_i) = \\alpha_0 +\\beta_1 \\text{smoking rate}_i +u_i +v_i\\) where \\(\\exp(\\alpha)\\) is the baseline lung cancer relative risk, \\(\\beta_1\\) is the smoking effect , \\(v_i\\) represents an unstructured spatial effects and \\(u_i\\) is an iCAR component representing the spatially structured variability.\nthe hyperparameters: the precision (or variance) parameters of the unstructured and structured spatial effects \\(\\tau_u,\\tau_v\\)\n\n\n\n\n\n\n2 Geostatistics\n\n\n\n\n\n\n Task 5\n\n\n\nSuppose we have a geostatistical process, \\(\\{Z(s); s \\in D\\},~ D \\subset\\mathbb{R}^2\\) which is stationary with mean, \\(\\mu_z\\) and covariance \\(\\mathrm{Cov}(h)\\). Define what is meant by:\n\nweakly stationary\nisotropic\n\nThen, write down an expression for the autocorrelation function \\(\\rho_z(h)\\) in terms of the covariance function.\n\n\nSolution\n\nStationarity implies mean is constant across spatial domain D. Mathematically, a geostatistical process \\(\\{Z(s); s \\in D\\},~ D \\subset\\mathbb{R}^2\\) is weakly stationary if:\n\n\\(E[Z(s)] = \\mu_z(s) = \\mu_z\\) for some finite constant \\(\\mu_z\\) which does not depend on \\(s\\).\n\\(\\mathrm{Cov}[Z(s),Z(s+h)] = C_z(s,s+h) = C_z(h)\\), a finite constant that can depend on h but not on s.\n\nAn isotropic geostatistical process is a stationary process (i.e., the mean is constant, and the covariance only depends on the spatial lag and not the values themselves), but where the covariance between points only depends on the distance between the points, and not the direction. Mathematically, a weakly stationary geostatistical process \\(\\{Z(s); s \\in D\\}\\) is isotropic if the covariance function \\(C_z(h)\\) can be further simplified to \\(C_z(h) = C_Z(||h||)\\); where \\(h=||h||\\) denotes the length of the lag vector \\(h\\).\nAssuming \\(Z(s)\\) is stationary at lag \\(h\\) we have\n\\[\n    \\begin{aligned}\n    \\rho_z(h)&= \\dfrac{\\mathrm{Cov}[Z(s),Z(s+h)]}{\\sqrt{\\mathrm{Cov}[Z(s)]\\mathrm{Cov}(Z(s+h)]}}\\\\\n    &= \\dfrac{C_z(h)}{\\sqrt{C_z(0)C_z(0)}}\\\\\n    &= \\dfrac{C_z(h)}{C_z(0)}\n    \\end{aligned}\n    \\]\n\n\n\nFigure 3 below shows summary plots for measurements of a geostatistical process, namely levels of Nitrogen measured at different locations across Chesapeake Bay, USA.\n\n\n\n\n\n\nFigure 3: Summary plots for nitrogen levels across Chesapeake bay. Top left plot colour scale red to blue (high to low values).\n\n\n\n\n\n\n\n\n\n Task 6\n\n\n\nSubjectively, comment on what each of these plots tells you about the data\n\n\nSolution\n\nHistogram does not look bell shaped & symmetric, there appears to be some evidence of bimodality. It is possible a transformation could be applied to stabilize the variance (e.g., a log transformation). It should be noted however that histograms are difficult to interpret and how they appear can change substantially when the number of bins used is changed.\n\n\n\n\n\n\n\n\n\n Task 7\n\n\n\nDescribe how you would check for spatial correlation in the data and model it if it were present.\n\n\nSolution\n\nThis a geostatistical data set. To assess spatial correlation we could:\n\nCompute semi-variance for nitrogen levels for all pairs of points and plot (variogram cloud)\nBin the variogram cloud to obtain empirical variogram\nConstruct null envelops under the assumption of no spatial autocorrelation and check for deviations from this assumption.\n\nA possible geostatistical model that accounts for spatial autocorrelation could take the form\n\\[\n\\begin{aligned}\n\\log(\\text{nitrogen})_i &\\sim \\mathrm{Normal}(\\mu_i,\\sigma^2_e)\\\\\n\\mu_i & =\\beta_0 + Z(s)\\\\\n\\end{aligned}\n\\] Here, the log concentrations are assumed to follow a Gaussian distribution with an observational error \\(\\sigma^2_e\\) and mean \\(\\mu_i\\) which is linked to the latent components \\(\\beta_0\\) ,representing the mean nitrogen concentration levels (on the log scale), and \\(Z(s)\\) - an mean-zero isotropic and stationary Gaussian Field with a variance-covariance matrix (e.g., a Matérn covariance) that accounts for the spatial correlation. \\(Z(s)\\) can be then approximated as a continuously indexed a Gaussian Markov Random field using the stochastic partial differential equation approach (SPDE).\n\n\n\n\n\n\n\n\n\n Task 8\n\n\n\nAn empirical semi-variogram with a MC envelope is shown below (Figure 4). Comment on the plot with regards to the presence of spatial correlation.\n\n\n\n\n\n\nFigure 4: Empirical variogram and MC simulation envelope\n\n\n\n\n\nSolution\n\nThe dashed lines represent a simulated Monte Carlo envelope corresponding to no spatial correlation. This envelope corresponds to the range of plausible semi-variograms that could be produced if the data contained no spatial correlation. Therefore, if the empirical semi-variogram that is calculated from the real data lies completely within the envelopes, then there is no evidence of spatial correlation. If they lie outside at some point then there is evidence of correlation.\n\n\n\n\n\n3 Point Processes\n\n\n\n\n\n\n Task 9\n\n\n\nExplain what a spatial point pattern is, define complete spatial randomness, and how it is assessed.\n\n\nSolution\n\nA Spatial point process is a set of locations, irregularly distributed within a designated region and presumed to have been generated by some form of stochastic (random) mechanism - A realisation from a spatial point process is termed a spatial point pattern – a countable collection of points \\(x_i\\).\nGiven any spatial region A, CSR asserts that\n\nConditional on the total number of events in A, the events are uniformly distributed over \\(|A|\\)\nThe random variable total number of events in \\(A\\) follows a Poisson distribution with mean \\(\\lambda|A|\\).\n\nIn (ii) above, \\(\\lambda\\) is termed the intensity, or the expected number of events per unit of area.\nA process satisfying (i) and (ii) is called a Spatial Poisson process (with intensity \\(\\lambda\\)).\n\\(K(t)=(1/\\lambda)\\times E(\\text{number of events within a distance t of an arbitrary event})\\)\n\nFor the case of a Poisson process, \\(K_{CRS}(t) = \\pi t^2\\).\nFor the case of clustered patterns, we would expect for short distances \\(t\\) that \\(K(t) &gt; \\pi \\times t^2\\). For regular patterns, we would expect that for short distances \\(t\\) that \\(K(t)&lt; \\pi \\times t^2\\).\n\n\n\n\n\n\n\n\n\n\n Task 10\n\n\n\nDiscuss two limitations of using Ripleys’ \\(K\\) for assessing CSR in spatial point patterns.\n\n\nSolution\n\nRipley’s \\(K\\) function grows quadratically with distance (\\(t\\)) because it is based on area (\\(\\pi t^2\\)) This makes interpretation difficult since the expected value under Complete Spatial Randomness (CSR) also follows this quadratic relationship.\nSensitivity to Edge Effects; Points near the boundaries of the study area have fewer neighboring points within distance (t), leading to underestimation of \\(K(t)\\).\n\n\n\n\n\n\n\n\n\n Task 11\n\n\n\nFigure 5 (a, left) shows the \\(K\\) function for the locations of 500 trees of a particular species within an area of tropical rainforest while Figure 5 (b, right) shows the \\(K\\) function for the Lansing wood tree species. For each of these plots comment on the spatial pattern of the data that generated the K functions with respect to complete spatial randomness.\n\n\n\n\n\n\nFigure 5: (a) \\(K\\) function for location of a particular species of tree in a tropical rainforest and (b) the \\(K\\) function for the distribution of trees in Lansing Wood.\n\n\n\n\n\nSolution\n\n\nA: The observed \\(K\\) function is above the theoretical \\(K\\) function (under CSR) and outwith the grey confidence region. Hence we can say that the location of trees in the tropical rainforest follow a clustered spatial pattern.\nB: The observed \\(K\\) function is difficult to distinguish from the \\(K\\) function under CSR and falls entirely within the grey confidence region. Hence we can say that the locations of trees in Lansing wood follow a homogeneous spatial Poisson process."
  },
  {
    "objectID": "lab_3.html",
    "href": "lab_3.html",
    "title": "Lab session 3",
    "section": "",
    "text": "Aim of this practical session:\nIn this practical session we are going\nDownload Lab 3 R script\nThe data for this session is available throughout different packages, if you cannot access some of these libraries you can the download the data using the link below and then load it into R using the load() function:\nload(\"lab_3_DataFiles.RData\")\nDownload Lab 3 Data Sets\nFirst lets load the R libraries we will use during this session:\n# Libraries for Data manipulation\nlibrary(tidyr)\nlibrary(dplyr)\nlibrary(sf)\nlibrary(terra)\n\n# Libraries for producing maps\nlibrary(ggplot2)\nlibrary(viridis)\nlibrary(tidyterra)\nlibrary(mapview)\nlibrary(patchwork)\n\n# Libraries for the Analysis\nlibrary(INLA)\nlibrary(inlabru)\nlibrary(spdep) \nlibrary(gstat)\nlibrary(variosig)"
  },
  {
    "objectID": "lab_3.html#the-data-lip-cancer-rates-in-scotland",
    "href": "lab_3.html#the-data-lip-cancer-rates-in-scotland",
    "title": "Lab session 3",
    "section": "The Data: Lip cancer rates in Scotland",
    "text": "The Data: Lip cancer rates in Scotland\nIn this example we model the number of lip cancer rates in Scotland in the years 1975–1980 at the county level in order to evaluate the presence of an association between sun exposure and lip cancer.\n\n\n\n\n\nThe data is available on the SpatialEpi R package\n\nlibrary(SpatialEpi)\ndata(scotland_sf)\n\nThe scotland_sf data is a Simple Features (sf) object containing the spatial polygon information for the set of 56 counties. The sf package allows us to work with vector data which is used to represent points, lines, and polygons. It can also be used to read vector data stored as a shapefiles.\nThe dataset contains the following variables of interest:\n\n\n\n\n\n\n\nVariable\nMeaning\n\n\n\n\ncounty.names\nScotland County name\n\n\ncases\nNumber of Lip Cancer cases per county\n\n\nexpected\nExpected number of lip cancer cases\n\n\nAFF\nProportion of the population who work in agricultural fishing and farming\n\n\ngeometry\nGeometric representation of counties in Scotland"
  },
  {
    "objectID": "lab_3.html#standardized-mortality-ratios-and-spatial-correlation",
    "href": "lab_3.html#standardized-mortality-ratios-and-spatial-correlation",
    "title": "Lab session 3",
    "section": "1.1 Standardized Mortality Ratios and spatial correlation",
    "text": "1.1 Standardized Mortality Ratios and spatial correlation\nIn epidemiology, disease risk is usually estimated using Standardized Mortality Ratios (SMR). The SMR for a given spatial areal unit \\(i\\) is defined as the ratio between the observed ( \\(Y_i\\) ) and expected ( \\(E_i\\) ) number of cases:\n\\[\nSMR_i = \\dfrac{Y_i}{E_i}\n\\]\nA value \\(SMR &gt; 1\\) indicates that there are more observed cases than expected which corresponds to a high risk area. On the other hand, if \\(SMR&lt;1\\) then there are fewer observed cases than expected, suggesting a low risk area.\nWe can manipulate sf objects the same way we manipulate standard data frame objects via the dplyr package. Lets use the pipeline command %&gt;% and the mutate function to calculate the yearly SMR values for each county:\n\n# Compute the SMR and add a region index (for later modelling)\nscotland_sf &lt;- scotland_sf %&gt;% mutate(\n  SMR = cases/expected,\n  region_id = 1:nrow(scotland_sf))\n\nNow we use ggplot to visualize our data by adding a geom_sf layer and coloring it according to our variable of interest (i.e., SMR) choosing an appropriate color palette using the scale_fill_viridis from the viridis package:\n\n# Visualize the regions colored by the SMR\nggplot()+geom_sf(data=scotland_sf,aes(fill=SMR))+scale_fill_viridis(direction = -1,option = \"magma\")\n\n\n\n\n\n\n\n\nWe can see that higher lip cancer risks occurs in the north of Scotland.\n\n\n\n\n\n\n Task\n\n\n\nProduce a map that shows the spatial distribution of the proportion of the population who work in agricultural fishing and farming (AFF) for each county.\n\n\n\nClick here to see the solution\n\nggplot()+geom_sf(data=scotland_sf,aes(fill=AFF))+scale_fill_viridis(direction = -1,option = \"magma\")\n\n\n\n\n\n\n\n\n\n\n\n\n1.1.1 Neighbourhood structure\nA key aspect of any spatial analysis is that observations closer together in space are likely to have more in common than those further apart. This can lead us towards approaches similar to those used in time series, where we consider the spatial closeness of our regions in terms of a neighbourhood structure.\nThe function poly2nb() of the spdep package can be used to construct a list of neighbors based on areas with contiguous boundaries (e.g., using Queen contiguity).\n\nW.nb &lt;- poly2nb(scotland_sf,queen = TRUE)\n\nWarning in poly2nb(scotland_sf, queen = TRUE): some observations have no neighbours;\nif this seems unexpected, try increasing the snap argument.\n\n\nWarning in poly2nb(scotland_sf, queen = TRUE): neighbour object has 4 sub-graphs;\nif this sub-graph count seems unexpected, try increasing the snap argument.\n\nW.nb\n\nNeighbour list object:\nNumber of regions: 56 \nNumber of nonzero links: 234 \nPercentage nonzero weights: 7.461735 \nAverage number of links: 4.178571 \n3 regions with no links:\n3, 53, 55\n4 disjoint connected subgraphs\n\n\nThe warning tell us that the neighbourhood is comprised of 4 interconnected regions. By looking at the neighbourhood graph below, we can see that these are the three separate island in the north.\n\nplot(st_geometry(scotland_sf), border = \"lightgray\")\nplot.nb(W.nb, st_geometry(scotland_sf), add = TRUE)\n\n\n\n\n\n\n\n\n\n\n1.1.2 Moran’s I\nOnce we have identified a set of neighbours using our chosen method, we can use this to account for correlation.\nWe can use Moran’s \\(I\\) as a measure of global spatial autocorrelation based on a neighbourhood matrix:\n\\[ w_{ij} = \\begin{cases} 1 & \\text{if areas } (B_i, B_j) \\text{ are neighbours.}\\\\ 0 & \\text{otherwise.} \\end{cases} \\]\nIn R we can use the nb2listw function to create \\(w\\) while specifying zero.policy = TRUE as we have some regions with no neighbors.\n\n# neighbors list \nnbw &lt;- nb2listw(W.nb, style = \"W\",zero.policy = TRUE)\n\nMoran’s \\(I\\) ranges between -1 and 1, and can be interpreted in a similar way to a standard correlation coefficient.\n\n\\(I=1\\) implies that we have perfect spatial correlation.\n\\(I=0\\) implies that we have complete spatial randomness.\n\\(I=-1\\) implies that we have perfect dispersion (negative correlation).\n\nOur observed \\(I\\) is a point estimate, and we may also wish to assess whether it is significantly different from zero. We can test for a statistically significant spatial correlation using a permutation test, with hypotheses:\n\\[\n\\begin{aligned}\nH_0&: \\text{ negative or no spatial association } (I \\leq 0)\\\\\nH_1&: \\text{ positive spatial association } (I &gt; 0)\n\\end{aligned}\n\\]\nWe can use moran.test() to test this hypothesis by setting alternative = \"greater\". To do so, we need to supply list containing the neighbors via the nb2listw() function from the spdep package. Lets assess now the spatial autocorrelation of the SMR in 2011:\n\n# Global Moran's I\ngmoran &lt;- moran.test(scotland_sf$SMR, nbw,\n                     alternative = \"greater\")\ngmoran\n\n\n    Moran I test under randomisation\n\ndata:  scotland_sf$SMR  \nweights: nbw  \nn reduced by no-neighbour observations  \n\nMoran I statistic standard deviate = 5.6068, p-value = 1.03e-08\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.497024704      -0.019230769       0.008478093 \n\n\n\n\n\n\n\n\n Question\n\n\n\nWhat do we conclude from the Moran’s I test?\n\n\nAnswer\n\nSince have set the alternative hypothesis to be $ I &gt; 0$ and have a p-value \\(&lt;0.05\\), we then reject the null hypothesis and conclude there is evidence for positive spatial autocorrelation."
  },
  {
    "objectID": "lab_3.html#fitting-a-bym-model",
    "href": "lab_3.html#fitting-a-bym-model",
    "title": "Lab session 3",
    "section": "1.2 Fitting a BYM model",
    "text": "1.2 Fitting a BYM model\nAs with the other types of spatial modelling, our goal is to observe and explain spatial variation in our data. Generally, we are aiming to produce a smoothed map which summarises the spatial patterns we observe in our data.\n\n1.2.1 The Model structure\nNow will fit a BYM model to the data where we consider a Poisson model for the observed cases. Remember that we can formulate this as a Latent Gaussian Model (LGM) on threes stages:\n\n\nStage 1: We assume the responses are Poisson distributed: \\[\n\\begin{aligned}y_i|\\eta_i & \\sim \\text{Poisson}(E_i\\lambda_i)\\\\\\text{log}(\\lambda_i) = \\eta_i & = \\beta_0 + \\beta_1 \\mathrm{AFF} + u_i + z_i\\end{aligned}\n\\]\n\n\n\n\nStage 2: \\(\\eta_i\\) is linked to the disease relative risks (RR) via the log link function and is a linear function of four components: \\(\\beta_0\\) the overall intercept, \\(\\beta_1\\) the AFF effect,\\(\\mathbf{u} = (u_1, \\dots, u_n)\\) a spatially structured model (or ICAR) with precision matrix \\(\\tau_u\\mathbf{Q}\\) and an unstructured iid random effect \\(\\mathbf{z} = (z_1, \\dots, z_n)\\) with precision \\(\\tau_z\\)\n\n\n\n\nStage 3: \\(\\{\\tau_{z},\\tau_u\\}\\): Precision parameters for the random effects\n\n\nThe latent field is then given by \\(\\mathbf{x}= (\\beta_0, \\beta_1, u_1, u_2,\\ldots, u_n,z_1,...,z_n)\\), the hyperparameters are \\(\\boldsymbol{\\theta} = (\\tau_u,\\tau_z)\\).\nWe can fit this model with inlabru by following the next workflow and defining:\n\nThe model components (cmp)\nThe formula for the linear predictor\nThe observational model (via the bru_obs function)\nfit the model using the bru function\n\n\n\nModel components\nOur model has 4 compontents, (i) a global intercept, (ii) the covariate AFF effect (which is a column in our data set), (iii) a unstructured random effect \\(\\mathbf{z} \\sim N(0,\\tau_z^{-1})\\)) and (iv) a structured random effect \\(\\mathbf{u}\\) that follows an ICAR model structure of the form:\n\\[\nu_i|\\mathbf{u}_{-i},\\tau_u, \\sim N\\left(\\frac{1}{d_i}\\sum_{j\\sim i}u_j,\\frac{1}{d_i\\tau_u}\\right)\n\\]\nWhere\n\n\\(\\mathbf{u}_{-i} = (u_i,\\ldots,u_{i-1},u_{i+1},\\ldots,u_n)^T\\)\n\\(\\tau_u\\) is the precision parameter (inverse variance).\n\\(d_i\\) is the number of neighbours\n\\(Q\\) denotes the precision matrix defined as\n\n\\[\nQ_{i,j} = \\begin{cases}\nd_i, & i = j \\\\\n-1, & i \\sim j \\\\\n0, &\\text{otherwise}\n\\end{cases}\n\\]\nThus, we can define the precision matrix Q according the proximity matrix using the nb2mat function (this is similar to nb2listw we used before but it will give us a matrix rather than a list):\n\n\nCode\nR &lt;- nb2mat(W.nb, style = \"B\", zero.policy = TRUE)\ndiag = apply(R,1,sum)\nQ = -R\ndiag(Q) = diag\n\n\nRecall The precision matrix \\(Q\\) depends on the neighboring structure and \\(\\tau_u\\) (which will be estimated). Now, to make the precision parameters of models with different intrinsic Gaussian random field comparable we add a sum-to-zero constrain \\(\\sum_i^n u_i = 0\\) (see scale.model = TRUE in the code below).\n\n\nCode\ncmp = ~ Intercept(1) + beta_1(AFF, model = \"linear\") + \n   z_i(region_id , model = \"iid\") +\n  u_i(region_id, model = \"besag\", graph = Q,scale.model = TRUE) \n\n\nHere, Intercept, beta_1, u_i and z_i are the names of the model components (note these are just labels we could have chosen any other names for them)\n\n\nThe linear predictor and the observational model\nNow we need to specify:\n\nThe linear predictor formula for the model components (here we declare the labels that we used in the previous step)\n\n\n\nCode\nformula = cases ~ Intercept + beta_1 + u_i + z_i\n\n\n\nThe observational model/likelihood for our observations:\n\n\n\nCode\nlik = bru_obs(formula = formula,\n\n              family = \"poisson\",\n\n              E = expected,\n\n              data = scotland_sf)\n\n\nNotice how we add the expected number of cases as our offset through the E argument of bru_obs and specify the distribution of our data through the family argument.\n\n\nFit the Model and Extract the Results\nWe finally fit the model using the bru function and extract some posterior summaries using the summary function:\n\n\nCode\nfit = bru(cmp, lik)\nsummary(fit)\n\n\ninlabru version: 2.13.0.9016 \nINLA version: 25.08.21-1 \nComponents: \nLatent components:\nIntercept: main = linear(1)\nbeta_1: main = linear(AFF)\nz_i: main = iid(region_id)\nu_i: main = besag(region_id)\nObservation models: \n  Family: 'poisson'\n    Tag: &lt;No tag&gt;\n    Data class: 'sf', 'data.frame'\n    Response class: 'numeric'\n    Predictor: cases ~ Intercept + beta_1 + u_i + z_i\n    Additive/Linear: TRUE/TRUE\n    Used components: effects[Intercept, beta_1, z_i, u_i], latent[] \nTime used:\n    Pre = 0.582, Running = 0.331, Post = 0.145, Total = 1.06 \nFixed effects:\n            mean    sd 0.025quant 0.5quant 0.975quant   mode kld\nIntercept -0.306 0.119     -0.538   -0.307     -0.069 -0.307   0\nbeta_1     4.317 1.272      1.758    4.337      6.761  4.337   0\n\nRandom effects:\n  Name    Model\n    z_i IID model\n   u_i Besags ICAR model\n\nModel hyperparameters:\n                      mean       sd 0.025quant 0.5quant 0.975quant    mode\nPrecision for z_i 22037.15 24116.14    1474.71 14479.55   86070.45 4023.70\nPrecision for u_i     4.14     1.43       2.02     3.91       7.60    3.48\n\nMarginal log-Likelihood:  -193.98 \n is computed \nPosterior summaries for the linear predictor and the fitted values are computed\n(Posterior marginals needs also 'control.compute=list(return.marginals.predictor=TRUE)')\n\n\nLooking at \\(\\exp(\\hat{\\beta_1} \\times 0.10)\\) we can see that a 10 percentage-point increase in the proportion working in agriculture/fishing/farming is associated with about a 2 increase in disease risk, after accounting for the spatial structures. In general, areas with a higher proportion of people working in agriculture, fishing, and farming tend to have substantially higher disease risk, even after accounting for spatial correlation using the ICAR effect.\n\n\n\n\n\n\n Question\n\n\n\n\nWhat is the estimated value for \\(\\beta_0\\)? \nLook at the estimated values of the hyperparameters , which of the two spatial components (structured or unstructured) explains more of the variability in the counts? unstructuredstructured"
  },
  {
    "objectID": "lab_3.html#plot-predictions",
    "href": "lab_3.html#plot-predictions",
    "title": "Lab session 3",
    "section": "1.3 Plot predictions",
    "text": "1.3 Plot predictions\nWe can then create a map of the RR using the predict function and supply the formula of our relative risks:\n\\[\n\\lambda_i = \\exp \\{\\beta_0 +\\beta_1 \\mathrm{AFF} + u_i +z_i\\}\n\\]"
  },
  {
    "objectID": "lab_3.html#the-data",
    "href": "lab_3.html#the-data",
    "title": "Lab session 3",
    "section": "2.1 The Data",
    "text": "2.1 The Data\nIn this example, we will explore data on the Pacific Cod (Gadus macrocephalus) from a trawl survey in Queen Charlotte Sound.\n\n\n\n\n\nThe pcod dataset is available from the sdmTMB package and contains the presence/absence records of the Pacific Cod during each surveys along with the biomass density of Pacific cod in the area swept (kg/Km\\(^2\\)) from 2003 to 2017. In this example we only consider year 2003.\nLet’s create an initial sf spatial object using the standard geographic coordinate system (EPSG:4326). This correctly defines the point locations based on latitude and longitude.\n\nlibrary(sdmTMB)\npcod = sdmTMB::pcod \n\npcod = pcod %&gt;% filter(year==2003)\npcod_sf =   st_as_sf(pcod, coords = c(\"lon\",\"lat\"), crs = 4326)\n\nNow we can transform to the standard UTM Zone 9N projection (EPSG:32609) and change the spatial units to km to better reflect the scale of our ecological study and to make resulting distance/area values more intuitive to interpret:\n\npcod_sf = st_transform(pcod_sf,\n                       crs = \"+proj=utm +zone=9 +datum=WGS84 +no_defs +type=crs +units=km\" )\n\nLet’s map the presence/absence of the Pacific Cod in 2003 using the mapview function:\n\npcod_sf %&gt;% \n  mutate(present = as.factor(present)) %&gt;%\nmapview(zcol = \"present\",\n        layer.name = \"Occupancy status of Pacific Cod in 2003\")\n\n\n\n\n\n\n\n\n\n\n\n Task\n\n\n\nUse ggplot and the sf library to map the biomass density of the pacific cod\n\n\nhint\n\nYou can plot ansf object by adding a geom_sf layer to a ggplot object.\n\n\n\n\nClick here to see the solution\n\nggplot()+ \n  geom_sf(data=pcod_sf,aes(color=density))+ \n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\n\n\n\n2.1.1 Raster Data\nThe qcs_grid data contain the depth values stored as \\(2\\times 2\\) km grid for Queen Charlotte Sound. Environmental data are typically stored in raster format, which represents spatially continuous phenomena by dividing a region into a grid of equally-sized cells, each storing a value for the variable of interest. In R, the terra package is a modern and powerful tool for efficiently working with raster data. The function rast(), can be used both to read raster files from standard formats (e.g., .tif or .tiff) and to create a new raster object from a data frame. For instance, the following code creates a raster from the qcs_grid grid data for Queen Charlotte Sound.\n\nqcs_grid = sdmTMB::qcs_grid\ndepth_r &lt;- rast(qcs_grid, type = \"xyz\")\n\nThe raster object contains three layers corresponding to the (i) depth values, (ii) the scaled depth values and (iii) the squared depth values.\nNotice that there are no CRS associated with the raster. Thus, we can assign appropriate CRS using the crs function. Additionally, we also want the raster CRS to match the CRS in the survey data (recall that we have previously reprojected our data to utm coordinates). We can assign an appropriate CRS that matches the CRS of the sf object as follows:\n\ncrs(depth_r) &lt;- crs(pcod_sf)\n\nWe can use the tidyterra package to plot raster data using ggplot by adding a geom_spatraster function and then select an appropriate fill and color palettes:\n\nggplot()+ \n  geom_spatraster(data=depth_r$depth)+\n  geom_sf(data=pcod_sf,aes(color=factor(present))) +\n    scale_color_manual(name=\"Occupancy status for the Pacific Cod\",\n                     values = c(\"black\",\"orange\"),\n                     labels= c(\"Absence\",\"Presence\"))+\n  scale_fill_viridis(name = \"Depth\",\n                   na.value = \"transparent\" )\n\n\n\n\n\n\n\n\nWe can check the spatial extension of our study area using the ext function:\n\next(depth_r)[1:2] %&gt;% diff() # difference in x coord\n\nxmax \n 242 \n\next(depth_r)[3:4] %&gt;% diff()  # difference in y coord\n\nymax \n 204 \n\n\nNote that this is in Km since we have transformed our original data.\n\n\n2.1.2 Autocorrelation and Variograms\nSpatial statistics quantifies the fundamental principle that nearby things are closely related. This spatial dependence means that observation are not independent, as assumed by most classical statistical models, but are instead correlated relative to their proximity. While this correlation can be a valuable source of information, it must be explicitly accounted for to avoid wrong inference and incorrect conclusions.\nThe first step is to assess whether there is any evidence of spatial dependency in our data. Spatial dependence in georeferenced data can be explored by a function known as a variogram \\(2\\gamma(\\cdot)\\) (or semivariogram \\(\\gamma(\\cdot)\\)). The variogram is similar in many ways to the autocorrelation function used in time series modelling. In simple terms, it is a function which measures the difference in the spatial process between a pair of locations a fixed distance apart.\nWe can calculate the binned- empirical variogram for the data using variogram function from the gstat library. This plot shows the semi-variances for each pair of points. Lets compute a variogram for the biomass log-density of the Pacific Cod in 2003:\n\nvario_binned &lt;- gstat::variogram(log(density) ~ depth_scaled  + depth_scaled2,\n                                 data = pcod_sf %&gt;% filter(density&gt;0),\n                                 cloud = FALSE,\n                                 cutoff = 300)\nplot(vario_binned)\n\n\n\n\n\n\n\n\nAssessing spatial dependence\nWe can construct null envelope based on permutations of the data values across the locations, i.e. envelopes built under the assumption of no spatial correlation. By overlapping these envelopes with the empirical variograms we can determine whether there is some spatial dependence in our data ,e.g. if our observed variograms falls outside of the envelopes constructed under spatial randomness.\nWe can construct permutation envelopes on the gstat empirical variogram using the envelope function from the variosig R package. Then we can visualize the results using the envplot function:\n\nvarioEnv &lt;- envelope(vario_binned,\n                     data = pcod_sf %&gt;% filter(density&gt;0),\n                     locations = st_coordinates(pcod_sf %&gt;% filter(density&gt;0)),\n                     formula = log(density) ~ depth_scaled  + depth_scaled2,\n                     nsim = 499)\nenvplot(varioEnv)\n\n\n\n\n\n\n\n\n[1] \"There are 2 out of 15 variogram estimates outside the 95% envelope.\"\n\n\nIt seems that the depth and squared depth covariates account for large part of the spatial variability in the log density as only 2 point fall outside the null envelopes. However, we have discarded all locations were no fish were caught so we might prefer to fit a model that explicitly accounts for this."
  },
  {
    "objectID": "lab_3.html#hurdle-geostatistical-model",
    "href": "lab_3.html#hurdle-geostatistical-model",
    "title": "Lab session 3",
    "section": "2.2 Hurdle Geostatistical Model",
    "text": "2.2 Hurdle Geostatistical Model\nHere we present a multilikelihood approach to jointly model the the log-biomass density while accounting for the presence of zeros in our data. A two-part model can be constructed to accommodate zero-inflated continuous data by combining separate likelihoods: one for the occurrence (e.g., Bernoulli) and one for the conditional positive amount (e.g., log-normal). The primary advantage of this framework is the ability to model the probability of an event and its magnitude independently.\n\n2.2.1 The Model structure\n\nStage 1 Model for the response(s)\\[\n\\begin{aligned}\ny_i|\\eta^{(1)}_i&\\sim \\text{Binomial}(1,\\pi_i)\\\\\n\\log(z_i)|\\eta^{(2)}_i&\\sim \\text{Normal}(\\mu_i,\\tau_e^{-1})\\\\\n\\end{aligned}\n\\]\n\nWe then define a likelihood for each outcome.\n\n\\(y_i =\\begin{cases} 1 &\\text{if fishes have been caught at location } \\mathbf{s}_i  \\\\ 0 &\\text{otherwise}\\end{cases}\\)\n\\(z_i =\\begin{cases} NA &\\text{if no fish were  caught at location } \\mathbf{s}_i  \\\\ \\text{biomass density at location } \\mathbf{s}_i &\\text{otherwise}\\end{cases}\\)\n\n\n\nThis structure is equivalent to a Hurdle-log-Normal model, where the overall expected value of log biomass is given by the product \\(\\pi_{i} * \\mu_{i}\\), with \\(\\mu_{i}\\) representing the conditional expectation from the log-normal component.\nNext we define the components of our linear predictor. Notice how we are defining this model in a LGM framework:\n\nStage 2 Latent field model \\[\n\\begin{aligned}\n\\eta^{(1)}_i &= \\text{logit}(\\pi_i) = X'\\beta + \\xi_i\\\\\n\\eta^{(2)}_i &= \\mu_i = X'\\alpha + \\omega_i\n\\end{aligned}\n\\]\n\n\\(\\{\\alpha,\\beta\\}\\) = Intercepts + covariate effects.\n\\(\\{\\xi,\\omega\\}\\) = are the Gaussian fields with Matérn covariance (separate for each outcome).\n\n\nFor the occurrence of fish, the linear predictor gets mapped to the logit of the probability of the Bernoulli model while the linear predictor for the biomass density is mapped to the mean of a log normal distribution.\n\nStage 3 Hyperparameters\n\nThe hyperparameter for the model are:\n\nobservational error (nugget) \\(\\tau_e\\)\nMatérn field(s) parameters \\(\\{\\rho^{(1)},\\rho^{(2)},\\tau_{d}^{(1)},\\tau_{d}^{(2)}\\}\\)\n\n\n\n2.2.2 The workflow\nWhen fitting a geostatistical model we need to fulfill the following tasks:\n\nBuild the mesh\nDefine the SPDE representation of the spatial GF. This includes defining the priors for the range and sd of the spatial GF\nDefine the components of the linear predictor. This includes the spatial GF and all eventual covariates\nDefine the observation model using the bru_obs() function\nRun the model using the bru() function\n\n\n\n1. Building the mesh\nThe first task, when dealing with geostatistical models is to build the mesh that covers the area of interest. For this purpose we use the function fm_mesh_2d.\nOne way to build the mesh is to start from the locations where we have observations, these are contained in the dataset pcod_sf.\n\nmesh = fm_mesh_2d(loc = pcod_sf,           # Build the mesh\n                  max.edge = c(10,20),     # The largest allowed triangle edge length.\n                  offset = c(5,50))       # The automatic extension distance\nggplot() + gg(mesh) + geom_sf(data= pcod_sf, aes(color = factor(present)), size = 0.1) + xlab(\"\") + ylab(\"\")\n\n\n\n\n\n\n\n\nAs you can see from the plot above, some of the locations are very close to each other, this causes some very small triangles. This can be avoided using the option cutoff = which collapses the locations that are closer than a cutoff (those points are collapsed in the mesh construction but, of course, not when it come to estimaation.)\n\nmesh = fm_mesh_2d(loc = pcod_sf,           # Build the mesh\n                  cutoff = 2,\n                  max.edge = c(10,20),     # The largest allowed triangle edge length.\n                  offset = c(5,50))       # The automatic extension distance\nggplot() + gg(mesh) + geom_sf(data= pcod_sf, aes(color = factor(present)), size = 0.1) + xlab(\"\") + ylab(\"\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Task\n\n\n\nLook at the documentation for the fm_mesh_2d function typing\n\n?fm_mesh_2d\n\nplay around with the different options and create different meshes.\nThe rule of thumb is that your mesh should be:\n\nfine enough to well represent the spatial variability of your process, but not too fine in order to avoid computation burden\nthe triangles should be regular, avoid long and thin triangles.\nThe mesh should contain a buffer around your area of interest (this is what is defined in the offset option) in order to avoid boundary artefact in the estimated variance.\n\n\n\n\n\n2. Define the SPDE representation of the spatial GF\nTo define the SPDE representation of the spatial GF we use the function inla.spde2.pcmatern. This takes as input the mesh we have defined and the PC-priors definition for \\(\\rho\\) and \\(\\sigma\\) (the range and the marginal standard deviation of the field).\nPC priors Gaussian Random field are defined in (Fuglstad et al. 2018). From a practical perspective for the range \\(\\rho\\) you need to define two paramters \\(\\rho_0\\) and \\(p_{\\rho}\\) such that you believe it is reasonable that\n\\[\nP(\\rho&lt;\\rho_0)=p_{\\rho}\n\\]\nwhile for the marginal variance \\(\\sigma\\) you need to define two parameters \\(\\sigma_0\\) and \\(p_{\\sigma}\\) such that you believe it is reasonable that\n\\[\nP(\\sigma&lt;\\sigma_0)=p_{\\sigma}\n\\]\nHere are some alternatives for defining priors for our model\n\nspde_model1 =  inla.spde2.pcmatern(mesh,\n                                  prior.sigma = c(.1, 0.5),\n                                  prior.range = c(30, 0.5))\nspde_model2 =  inla.spde2.pcmatern(mesh,\n                                  prior.sigma = c(10, 0.5),\n                                  prior.range = c(1000, 0.5))\nspde_model3 =  inla.spde2.pcmatern(mesh,\n                                  prior.sigma = c(1, 0.5),\n                                  prior.range = c(100, 0.5))\n\n\n\n\n\n\n\n Question\n\n\n\nConsider the pcod_sf, the spatial extension and type of the data…is some of the previous choices more reasonable than other? spde_model1spde_model2spde_model3\nNOTE Remember that a prior should be reasonable..but the model should not totally depend on it.\n\n\n\n\n3. Define the components of the linear predictor\nWe have now defined a mesh and a SPDE representation of the spatial GF. We now need to define the model components:\n\ncmp &lt;- ~\n   Intercept_biomass(1) +\n    depth_biomass(depth_scaled, model = \"linear\") +\n    depth2_biomass(depth_scaled2, model = \"linear\") +\n    space_biomass(geometry, model = spde_model3) +\n    Intercept_caught(1) +\n    depth_caught(depth_scaled, model = \"linear\") +\n    depth2_caught(depth_scaled2, model = \"linear\") +\n    space_caught(geometry, model = spde_model3)\n\nNOTE since the dataframe we use (pcod_sf) is an sf object the input in the space() component is the geometry of the dataset.\n\n\n4. Define the observation model\nSince we have two likelihoods we need to define two observational models\n\nfml_1 = density ~  Intercept_biomass + depth_biomass + depth2_biomass + space_biomass\nfml_2 = present ~ Intercept_caught + depth_caught + depth2_caught + space_caught\n  \nbiomass_obs &lt;- bru_obs(formula = fml_1,\n      family = \"lognormal\",\n      data = pcod_sf  %&gt;% filter(density&gt;0)) # restrict to those locations where fish were caught\n\npresence_obs &lt;- bru_obs(formula = fml_2 ,\n  family = \"binomial\",\n  data = pcod_sf,\n)\n\n\n\n5. Run the model & Extract results\n\n\n\n\n\n\n\nNow we fit the model and produce some summaries:\n\nfit_hurdle &lt;- bru(\n  cmp,\n  biomass_obs,\n  presence_obs\n)\n\nsummary(fit_hurdle)\n\ninlabru version: 2.13.0.9016 \nINLA version: 25.08.21-1 \nComponents: \nLatent components:\nIntercept_biomass: main = linear(1)\ndepth_biomass: main = linear(depth_scaled)\ndepth2_biomass: main = linear(depth_scaled2)\nspace_biomass: main = spde(geometry)\nIntercept_caught: main = linear(1)\ndepth_caught: main = linear(depth_scaled)\ndepth2_caught: main = linear(depth_scaled2)\nspace_caught: main = spde(geometry)\nObservation models: \n  Family: 'lognormal'\n    Tag: &lt;No tag&gt;\n    Data class: 'sf', 'tbl_df', 'tbl', 'data.frame'\n    Response class: 'numeric'\n    Predictor: \n        density ~ Intercept_biomass + depth_biomass + depth2_biomass + \n            space_biomass\n    Additive/Linear: TRUE/TRUE\n    Used components: effects[Intercept_biomass, depth_biomass, depth2_biomass, space_biomass], latent[] \n  Family: 'binomial'\n    Tag: &lt;No tag&gt;\n    Data class: 'sf', 'tbl_df', 'tbl', 'data.frame'\n    Response class: 'numeric'\n    Predictor: present ~ Intercept_caught + depth_caught + depth2_caught + space_caught\n    Additive/Linear: TRUE/TRUE\n    Used components: effects[Intercept_caught, depth_caught, depth2_caught, space_caught], latent[] \nTime used:\n    Pre = 1.26, Running = 7.55, Post = 0.896, Total = 9.7 \nFixed effects:\n                    mean    sd 0.025quant 0.5quant 0.975quant   mode   kld\nIntercept_biomass  3.498 0.332      2.885    3.482      4.227  3.435 0.000\ndepth_biomass     -0.509 0.294     -1.098   -0.505      0.058 -0.505 0.000\ndepth2_biomass    -0.339 0.228     -0.789   -0.339      0.106 -0.339 0.000\nIntercept_caught   1.761 2.370     -3.217    1.607      7.269  1.265 0.003\ndepth_caught      -2.564 0.492     -3.626   -2.533     -1.693 -2.460 0.000\ndepth2_caught     -1.494 0.305     -2.143   -1.478     -0.944 -1.433 0.000\n\nRandom effects:\n  Name    Model\n    space_biomass SPDE2 model\n   space_caught SPDE2 model\n\nModel hyperparameters:\n                                           mean     sd 0.025quant 0.5quant\nPrecision for the lognormal observations   1.15  0.550      0.465    1.028\nRange for space_biomass                   36.65 25.895      6.893   30.136\nStdev for space_biomass                    1.00  0.278      0.571    0.962\nRange for space_caught                   158.38 91.047     56.969  135.566\nStdev for space_caught                     2.20  0.661      1.192    2.105\n                                         0.975quant    mode\nPrecision for the lognormal observations       2.57   0.827\nRange for space_biomass                      103.70  18.520\nStdev for space_biomass                        1.66   0.888\nRange for space_caught                       398.93 101.339\nStdev for space_caught                         3.77   1.924\n\nMarginal log-Likelihood:  -657.88 \n is computed \nPosterior summaries for the linear predictor and the fitted values are computed\n(Posterior marginals needs also 'control.compute=list(return.marginals.predictor=TRUE)')\n\n\n\n\n2.2.3 Model Predictions:\nWe now want to extract the estimated posterior mean and sd of spatial GF. To do this we first need to define a grid of points where we want to predict:\n\npxl1 = data.frame(crds(depth_r), \n                  as.data.frame(depth_r)) %&gt;% \n       filter(!is.na(depth)) %&gt;%\nst_as_sf(coords = c(\"x\",\"y\"),crs=st_crs(pcod_sf)) \n\nthen compute the prediction for both the spatial GFs and both linear predictors (note that we use the plogis function to map \\(\\eta_i^{(1)}\\) to \\(\\pi_i^{(i)}\\)). To compute the expected log-biomass density we need:\n\n\\(\\pi_i^{(i)}\\) = Catching probability\n\\(\\mathbb{E}[Z(s)|Y(s)] = \\exp\\left(\\mu(s) + \\dfrac{1}{2\\tau_{e}}\\right)\\) = conditional mean\n\\(\\mathbb{E}(Z(s)) =\\pi(s)\\times \\mathbb{E}[Z(s)|Y(s)]\\) = unconditional mean\n\n\npred &lt;- predict( fit_hurdle , pxl1,\n  ~ {\n    pi &lt;- plogis(Intercept_caught + depth_caught + depth2_caught + space_caught)  # catching probability\n    mu_log  &lt;-  Intercept_biomass + depth_biomass + depth2_biomass + space_biomass  \n    sd &lt;- sqrt(1/Precision_for_the_lognormal_observations)\n    conditional_mean &lt;- exp(mu_log + 0.5 * sd^2)  \n    dens &lt;- pi * conditional_mean # biomass density\n    list(\n      pi = pi,\n      conditional_mean = conditional_mean,\n      dens = dens)\n  },n.samples = 2500)\n\nFinally, we can plot the maps\n\nggplot() + gg(pred$pi, geom = \"tile\",aes(fill = mean)) +  scale_fill_viridis() + ggtitle(\"Posterior mean for catch probability\")\n\n\n\n\n\n\n\nggplot() + gg(pred$dens, geom = \"tile\",aes(fill = mean))+  scale_fill_viridis(option = \"G\")+ ggtitle(\"Posterior mean of biomass density \")\n\n\n\n\n\n\n\nggplot() + gg(pred$dens, geom = \"tile\",aes(fill = sd)) + scale_fill_viridis(option = \"B\") + ggtitle(\"Posterior sd biomass density\")\n\n\n\n\n\n\n\n\nNote The posterior sd is lowest at the observation points. Note how the posterior sd is inflated around the border, this is the “border effect” due to the SPDE representation."
  },
  {
    "objectID": "lab_3.html#gorilla-nesting-sites",
    "href": "lab_3.html#gorilla-nesting-sites",
    "title": "Lab session 3",
    "section": "3.1 Gorilla Nesting Sites",
    "text": "3.1 Gorilla Nesting Sites\nIn this example we will fit a point process model to analyse the locations of nesting sites of gorillas, and associated covariates, in a National Park in Cameroon.\n\n\n\n\n\nThe data set is available from the inlabru R package. The data come from a study of gorillas in the Kagwene Gorilla Sanctuary, Cameroon, by the Wildlife Conservation Society Takamanda-Mone Landscape Project (WCS-TMLP). The dataset contains the spatial locations of 647 nesting sites of gorilla groups observed in the sanctuary over time. Locations are given as UTM (Zone 32N) coordinates in metres. The observation window is the boundary of the sanctuary, represented as a polygon.\nFirst lets load the data:\n\ndata(gorillas_sf, package = \"inlabru\")\n\nHere we will extract the information about:\n\nThe spatial location of the nests (nests)\nThe mesh for building our spatial model (mesh)\nThe observational window (boundary)\nThe Digital elevation of terrain, in metres (elev).\n\n\nnests &lt;- gorillas_sf$nests\nmesh &lt;- gorillas_sf$mesh\nboundary &lt;- gorillas_sf$boundary\ngcov &lt;- gorillas_sf_gcov()\nelev &lt;- gcov$elevation\nelev &lt;- elev - mean(terra::values(elev), na.rm = TRUE) #scale the  covariate\n\nWe can visualize the Data as follows:\n\nggplot() +\n  gg(elev) +\n  gg(boundary, alpha = 0.2) +\n  gg(nests, color = \"white\", cex = 0.5)+\n  scale_fill_viridis(option = \"F\")\n\n\n\n\n\n\n\n\nWe can see that the elevation covariate has been extended beyond the region of interest. Recall that this is need for computational purposes as we need the covariate to be available at the quadrature locations for approximating our point process model."
  },
  {
    "objectID": "lab_3.html#the-model",
    "href": "lab_3.html#the-model",
    "title": "Lab session 3",
    "section": "3.2 The Model",
    "text": "3.2 The Model\nWe are now going to use the scaled-elevation as a covariate in addition to a GF to explain the variability of the intensity \\(\\lambda(s)\\) over the domain of interest.\nOur model is \\[\n\\log\\lambda(s) = \\beta_0 + \\beta_1x(s) + \\xi(s)\n\\] where \\(x(s)\\) is the altitude at location \\(s\\) and \\(\\xi(s)\\) is a GF\nThe likelihood becomes:\n\\[\n\\begin{aligned}\np(\\mathbf{y} | \\lambda)  & \\propto \\exp \\left( -\\int_\\Omega \\lambda(\\mathbf{s}) \\mathrm{d}\\mathbf{s} \\right) \\prod_{i=1}^n \\lambda(\\mathbf{s}_i) \\\\\n& = \\exp \\left( -\\int_\\Omega \\exp(\\beta_0 + \\beta_1x(s)) \\mathrm{d}\\mathbf{s} \\right) \\prod_{i=1}^n \\lambda(\\mathbf{s}_i) \\\\\n\\end{aligned}\n\\]\nwhere \\(|\\Omega|\\) is the area of the domain of interest.\nWe need to approximate the integral using a numerical integration scheme as:\n\\[\n\\approx\\exp\\left(-\\sum_{k=1}^{N_k}w_k\\lambda(s_k)\\right)\\prod_{i=1}^n \\lambda(\\mathbf{s}_i)\n\\]\nWhere \\(N_k\\) is the number of integration points \\(s_1,\\dots,s_{N_k}\\) and \\(w_1,\\dots,w_{N_k}\\) are the integration weights. In addition we also need to approximate the GF - we can use the SPDE approach as before to achieve this.\nThe first step, as any time we use the SPDE approach is to define the mesh and the priors for the marginal variance and range:\n\nmesh &lt;- gorillas_sf$mesh\n\nggplot() + gg(mesh) + geom_sf(data = nests)\n\n\n\n\n\n\n\nspde_model =  inla.spde2.pcmatern(mesh,\n  prior.sigma = c(0.1, 0.01),\n  prior.range = c(0.1, 0.01)\n)\n\nWe can then define the integration weights. Here we use the same points to define the SPDE approximation and to approximate the integral in the likelihood (note that it does not have to be like this, BUT integration weight and SPDE weights have to be consistent with each other)\n\nips = fm_int(mesh, samplers = boundary)\n\nggplot() + geom_sf(data = ips, aes(color = weight)) +\n  gg(mesh) +\n   scale_color_viridis()"
  },
  {
    "objectID": "lab_3.html#fitting-the-model",
    "href": "lab_3.html#fitting-the-model",
    "title": "Lab session 3",
    "section": "3.3 Fitting the Model",
    "text": "3.3 Fitting the Model\nThe model has three components: intercept, linear effect of altitude and the spatial GRF\n\n\nCode\ncmp = ~ Intercept(1) + space(geometry, model = spde_model) + elev(elev, model = \"linear\")\n\nformula = geometry ~ Intercept + space + elev\n\n\nlik = bru_obs(\"cp\",\n              formula = formula,\n              data = nests,\n              ips = ips)\n\nfit_gorillas = bru(cmp, lik)\n\nsummary(fit_gorillas)\n\n\ninlabru version: 2.13.0.9016 \nINLA version: 25.08.21-1 \nComponents: \nLatent components:\nIntercept: main = linear(1)\nspace: main = spde(geometry)\nelev: main = linear(elev)\nObservation models: \n  Family: 'cp'\n    Tag: &lt;No tag&gt;\n    Data class: 'sf', 'tbl_df', 'tbl', 'data.frame'\n    Response class: 'numeric'\n    Predictor: geometry ~ Intercept + space + elev\n    Additive/Linear: TRUE/TRUE\n    Used components: effects[Intercept, space, elev], latent[] \nTime used:\n    Pre = 0.974, Running = 8.72, Post = 0.47, Total = 10.2 \nFixed effects:\n           mean    sd 0.025quant 0.5quant 0.975quant  mode kld\nIntercept 1.127 0.478      0.154    1.137      2.038 1.137   0\nelev      0.004 0.001      0.002    0.004      0.006 0.004   0\n\nRandom effects:\n  Name    Model\n    space SPDE2 model\n\nModel hyperparameters:\n                mean    sd 0.025quant 0.5quant 0.975quant  mode\nRange for space 1.76 0.217      1.376     1.75       2.23 1.713\nStdev for space 1.00 0.085      0.848     1.00       1.18 0.995\n\nMarginal log-Likelihood:  -1254.95 \n is computed \nPosterior summaries for the linear predictor and the fitted values are computed\n(Posterior marginals needs also 'control.compute=list(return.marginals.predictor=TRUE)')"
  },
  {
    "objectID": "lab_3.html#model-predictions-1",
    "href": "lab_3.html#model-predictions-1",
    "title": "Lab session 3",
    "section": "3.4 Model Predictions",
    "text": "3.4 Model Predictions\nThe predict function of inlabru can take as argument an sf objects. So, we can use the inlabru function fm_pixels to generate an sf object with points only within the boundary, using its mask argument, as shown below:\n\npred.df &lt;- fm_pixels(mesh, mask = boundary)\n\nThen, we simply supply the intensity and log intensity :\n\ne.pred &lt;- predict(\n  fit_gorillas,\n  pred.df,\n  ~ list(\n    int = exp(space + elev + Intercept),\n    int.log = space + elev + Intercept\n  )\n)\n\np1 &lt;- ggplot() +\n  gg(e.pred$int, aes(fill = log(sd)), geom = \"tile\") +\n  gg(boundary, alpha = 0) +\n  gg(nests, shape = \"+\") \np2 &lt;- ggplot() +\n  gg(e.pred$int.log, aes(fill = exp(mean + sd^2 / 2)), geom = \"tile\") +\n  gg(boundary, alpha = 0) +\n  gg(nests, shape = \"+\")\n(p1 | p2)\n\n\n\n\n\n\n\n\n\n3.4.1 Estimate abundance\nFinally, we want to use the fitted model to estimate the total number of nesting sites in the whole region. To do this, we first have to define the expected number of sites as:\n\\[\nE(N_{\\Omega}) = \\int_{\\Omega}\\exp(\\lambda(s))\\ ds\n\\]\nThen simulate a realization of \\(N_{\\Omega}\\) to include also the likelihood variability in our estimate. To do so, we pass both the fitted model and the integration weights to the predict function. Then, we specify a reasonable range for where posterior poisson density is defined:\n\nNest.e &lt;- predict(\n  fit_gorillas,\n  fm_int(mesh, boundary),\n  ~ data.frame(\n    N = 400:900,\n    density = dpois(400:900,\n      lambda = sum(weight * exp(space + elev + Intercept))\n    )\n  ),\n  n.samples = 2000\n)\n\nWe can plot the posterior density of the abundance as follows:\n\nggplot(data = Nest.e) +\n  geom_line(aes(x = N, y = mean, colour = \"Posterior\")) +\n    geom_vline(xintercept = nrow(nests),\n             colour = \"red\") +\n  xlab(expression(Lambda))"
  },
  {
    "objectID": "reading_list.html",
    "href": "reading_list.html",
    "title": "Reading list",
    "section": "",
    "text": "Blangiardo, M., & Cameletti, M. (2015). Spatial and spatio-temporal Bayesian models with R-INLA. John Wiley & Sons.\nGómez-Rubio, V. (2020). Bayesian inference with INLA. Chapman and Hall/CRC.\nKrainski, E., Gómez-Rubio, V., Bakka, H., Lenzi, A., Castro-Camilo, D., Simpson, D., … & Rue, H. (2018). Advanced spatial modeling with stochastic partial differential equations using R and INLA. Chapman and Hall/CRC.\nLindgren, F., Lindström, J., & Rue, H. (2010). An explicit link between gaussian fields and gaussian markov random fields; the spde approach.\nLindgren, F., Bolin, D., & Rue, H. (2022). The SPDE approach for Gaussian and non-Gaussian fields: 10 years and still running. Spatial Statistics, 50, 100599.\nMiller, D. L., Glennie, R., & Seaton, A. E. (2020). Understanding the stochastic partial differential equation approach to smoothing. Journal of Agricultural, Biological and Environmental Statistics, 25(1), 1-16.\nMoraga, P. (2023). Spatial statistics for data science: theory and practice with R. Chapman and Hall/CRC.\nRue, H., Riebler, A., Sørbye, S. H., Illian, J. B., Simpson, D. P., & Lindgren, F. K. (2017). Bayesian computing with INLA: a review. Annual Review of Statistics and Its Application, 4, 395-421.\nSimpson, D., Illian, J. B., Lindgren, F., Sørbye, S. H., & Rue, H. (2016). Going off grid: Computationally efficient inference for log-Gaussian Cox processes. Biometrika, 103(1), 49-70.\nSimpson, D., Rue, H., Riebler, A., Martins, T. G., & Sørbye, S. H. (2017). Penalising model component complexity: A principled, practical approach to constructing priors.\nVan Niekerk, J., Krainski, E., Rustand, D., & Rue, H. (2023). A new avenue for Bayesian inference with INLA. Computational Statistics & Data Analysis, 181, 107692."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Module 3",
    "section": "",
    "text": "This module provides an overview of widely-used contemporary methods for analysing spatial data in ecology and environmental sciences. We begin by developing spatial modelling techniques for disease risk assessment as a critical interface between environmental hazards and public health. From there, we move to modern computational tools for spatial prediction and inference that integrate hierarchical structures, latent processes, and scalable inference frameworks suited to complex spatial data.\nThe module balances methodological principles with hands-on application, focusing on fitting interpretable spatial models that account for spatial dependence and produce robust predictions for conservation planning, environmental management, and ecosystem assessment."
  },
  {
    "objectID": "index.html#lectures",
    "href": "index.html#lectures",
    "title": "Module 3",
    "section": "2.1 Lectures",
    "text": "2.1 Lectures\nThere will be two - 1 hr lectures per week at 42 Bute Gardens:916\n\n\n\n\n\n\nNote\n\n\n\nLectures will be recorded if the room’s technology allows them to be."
  },
  {
    "objectID": "index.html#tutorials",
    "href": "index.html#tutorials",
    "title": "Module 3",
    "section": "2.2 Tutorials",
    "text": "2.2 Tutorials\nIn addition, there will be four tutorials for this course. There are two tutorial groups - please check on MyCampus which one you are in.\n\nTutorial Group 1 - Monday 10amTutorial Group 2- Wednesday 12 noon\n\n\nTutorial groups:\n\nSTATS 4009 - TU01 (23738)\nSTATS 5031 - TU01 (24174)\n\nVenue:\nAdam Smith: 281\nTutorial dates:\n\n26-Jan-2026\n09-Feb-2026\n23-Feb-2026\n09-Mar-2026\n\n\n\nTutorial groups:\n\nSTATS 4009 - TU02 (23739)\nSTATS 5031 - TU02 (24175)\n\nVenue:\nJoseph Black Building:C407 Agricultm\nTutorial dates:\n\n28 -Jan-2026\n11-Feb-2026\n25-Feb-2026\n11-Mar-2026\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nYou are expected to have attempted the exercise sheets before the tutorial - they will be available in advance."
  },
  {
    "objectID": "index.html#labs",
    "href": "index.html#labs",
    "title": "Module 3",
    "section": "2.3 Labs",
    "text": "2.3 Labs\nThere will be three labs taking place in Boyd Orr Building:418 Lab from 15:00-17:00pm on the following dates (clicking on the date will direct you to the lab material):\n\nLab session 1 - Jan 30th\nLab session 2 - Feb 27th\nLab session 3 - March 13th"
  },
  {
    "objectID": "Bayesian_computing.html",
    "href": "Bayesian_computing.html",
    "title": "Introduction to the Integrated Laplace Approximation",
    "section": "",
    "text": "–&gt;"
  },
  {
    "objectID": "Bayesian_computing.html#definition-gaussian-markov-random-field-gmrf",
    "href": "Bayesian_computing.html#definition-gaussian-markov-random-field-gmrf",
    "title": "Introduction to the Integrated Laplace Approximation",
    "section": "Definition: Gaussian Markov Random Field (GMRF)",
    "text": "Definition: Gaussian Markov Random Field (GMRF)\nA random variable \\(\\mathbf{u}\\) is said to be a Gaussian Markov random field (GMRF) with respect to a graph \\(G\\), with vertices \\(\\{1,2,\\dots,n\\}\\) and edges \\(E\\), mean vector \\(\\boldsymbol{\\mu}\\), and precision matrix \\(\\mathbf{Q}\\), if its probability distribution is\n\\[\n\\pi(\\mathbf{u})\n=\n|\\mathbf{Q}|^{1/2}(2\\pi)^{-n/2}\n\\exp\\left\\{\n-\\frac{1}{2}\n(\\mathbf{u}-\\boldsymbol{\\mu})^{\\top}\n\\mathbf{Q}\n(\\mathbf{u}-\\boldsymbol{\\mu})\n\\right\\},\n\\]\nand the precision matrix satisfies the Markov property\n\\[\nQ_{ij} \\neq 0\n\\;\\Longleftrightarrow\\;\n\\{i,j\\} \\in E.\n\\]\nThat is, conditional independence between \\(u_i\\) and \\(u_j\\) corresponds to zeros in the precision matrix."
  },
  {
    "objectID": "Bayesian_computing.html#approximating-the-posterior-of-hyperparameters",
    "href": "Bayesian_computing.html#approximating-the-posterior-of-hyperparameters",
    "title": "Introduction to the Integrated Laplace Approximation",
    "section": "Approximating the posterior of hyperparameters",
    "text": "Approximating the posterior of hyperparameters\nTo build an approximation to \\(p(\\theta \\mid \\mathbf{y})\\) we can use definition of conditional probability where, conditional on the observations \\(\\mathbf{y}\\)\n\\[\n\\begin{aligned}\np(\\mathbf{u}~,\\theta\\mid \\mathbf{y}) &= p(\\mathbf{u}\\mid\\mathbf{y},\\theta)~p(\\theta\\mid \\mathbf{y}) \\\\\n\\Rightarrow p(\\theta \\mid \\mathbf{y}) &= \\dfrac{\\color{tomato}{p(\\mathbf{u}~,\\theta \\mid \\mathbf{y})}}{p(\\mathbf{u} \\mid \\mathbf{y},\\theta)}\n\\end{aligned}\n\\]\nApplying Bayes rules we get that \\(\\color{tomato}{p(\\mathbf{u}~,\\theta \\mid \\mathbf{y})} = \\frac{p(y \\mid \\mathbf{u}, \\theta)\\,p(\\mathbf{u}, \\theta)} {p(y)}\\)\n\\[\n\\begin{aligned}\np(\\mathbf{u}~,\\theta\\mid \\mathbf{y}) &=\n\\color{tomato}{\\frac{p(y \\mid \\mathbf{u}, \\theta)\\,\n      p(\\mathbf{u}, \\theta)}\n     {p(y)}} \\frac{1}{p(\\mathbf{u} \\mid \\theta, \\mathbf{y})} ~~\\color{grey}{\\text{Factorization of the joint prior} ~~p(\\mathbf{u},\\theta)=p(\\mathbf{u}\\mid \\theta)p(\\theta)}\\\\\n&=\n\\frac{p(y \\mid \\mathbf{u}, \\theta)\\,\n      p(\\mathbf{u} \\mid \\theta)\\,\n      p(\\theta)}\n     {p(y)} \\frac{1}{p(\\mathbf{u} \\mid \\theta, \\mathbf{y})} \\\\\n&\\propto \\dfrac{p(\\mathbf{y}\\mid \\mathbf{u},\\theta)p(\\mathbf{u}\\mid \\theta)p(\\theta)}{p(\\mathbf{u}\\mid \\theta, \\mathbf{y})}\n\\end{aligned}\n\\]\nWhere\n\n\\(p(\\mathbf{y}\\mid \\mathbf{u},\\theta)\\) is the likelihood function which we know\n\\(p(\\mathbf{u}\\mid \\theta) = N(0,Q^{-1})\\) is the Gaussian prior for the latent field which we also know\n\\(p(\\theta)\\) is a non-Gaussian prior for the hyperameters (also known)\n\\(p(\\mathbf{u}\\mid \\theta, \\mathbf{y})\\) the non-Gaussian density for the full condition of the latent effects … which we DON’t know\n\nIn practice we approximate \\(p(\\mathbf{u}\\mid \\theta, \\mathbf{y})\\) with a Gaussian distribution \\(p_G(\\mathbf{u}\\mid \\theta, \\mathbf{y})\\). How? we use the Laplace method.\n\n\n\n\n\n\nLaplace approximation\n\n\n\nThe Laplace approximation is an old technique for the approximation of integrals. Suppose the following integral that we want to approximate as \\(n \\to \\infty\\):\n\\[\nI_n = \\int_x \\exp(n f(x))\\,dx,\n\\]\nWe represent \\(f(x)\\) by means of a Taylor series expansion evaluated at a point \\(x_0\\)in which \\(f(x)\\) has its maximum so that\n\\[\n\\begin{aligned}\nf(x) &\\approx f(x_0) + \\cancelto{0}{f'(x_0)(x-x_0)} + \\frac{1}{2}f''(x_0)(x-x_0)^2 \\\\\n\\Rightarrow I_n &\\approx \\exp\\left\\{n f(x_0)\\right\\} \\underbrace{\\int_x\\exp\\left\\{\\frac{n}{2} (x-x_0)^2f''(x_0) \\right\\}dx}_{\\text{Gaussian Kernell}}\\\\\n\\therefore \\tilde{I}_n &= \\exp(n f(x_0))\n\\sqrt{\\frac{2\\pi}{-n f''(x_0)}}\n\\end{aligned}\n\\] Considering \\(nf(x)\\) as the sum of log-likelihoods and \\(x\\) as the unknown parameter, then we can apply Laplace method to approximate it by evaluating the Taylor series expansion at \\(x_0\\).\n\n\nThe full conditional we want to approximate is given by\n\\[\n\\begin{aligned}\np(\\mathbf{u}\\mid \\theta, \\mathbf{y}) &\\propto \\pi(\\mathbf{u} \\mid \\theta)\\,\\pi(\\mathbf{y} \\mid \\mathbf{u}, \\theta) \\\\\n&\\propto\n\\exp\\!\\left(\n-\\frac{1}{2}\\mathbf{u}^\\top \\mathbf{Q} \\mathbf{u}\n+\n\\sum_i \\log \\pi(y_i \\mid \\eta_i, \\theta)\n\\right).\n\\end{aligned}\n\\]\nWe can take the second-order Taylor series expansion for the log density \\(\\log p(y_i\\mid \\eta_i,\\theta)\\).\nRecall that by letting $ c=-f’’(x_0),;b=cx_0$ the second-order Taylor series expansion for \\(f(x)\\) evaluated at its maximum \\(x_0\\) is:\n\\[\n\\begin{aligned}\nf(x)&\\approx f(x_0) +\\tfrac12f''(x_0)(x-x_0)^2 \\\\\n&\\approx f(x_0)+\\tfrac12f''(x_0) (x^2 -2x_0x +x_0^2)   \\\\\n&\\approx\\underbrace{\\bigl[f(x_0)+\\tfrac12f''(x_0)x_0^2\\bigr]}_{\\text{constant } a}+ \\underbrace{\\bigl[-f''(x_0)x_0\\bigr]}_{b}x+\\tfrac12\\underbrace{f''(x_0)}_{-c}x^2 \\qquad\\color{grey}{\\text{setting }  c=-f''(x_0),\\;b=cx_0}\\\\\n&\\approx a+bx-\\tfrac12cx^2.\n\\end{aligned}\n\\]\nThus the second order Taylor expansion for the log density yields to\n\\[\n\\begin{aligned}\np(\\mathbf{u}\\mid \\theta, \\mathbf{y}) &\\propto \\exp\\!\\left\\{\n-\\frac{1}{2}\\mathbf{u}^\\top \\mathbf{Q} \\mathbf{u}\n+\n\\sum_i ( a_i + b_i \\eta_i - \\frac{1}{2}c_i \\eta_i^2)\n\\right\\}.\\\\\n&\\approx \\exp\\left\\{ \\frac{1}{2}\\mathbf{u}^\\intercal \\tilde{Q}\\mathbf{u} + \\tilde{\\mathbf{u}}\\right\\}\n\\end{aligned}\n\\]\nIn short, we approximate \\(p(\\mathbf{u}\\theta,\\mathbf{y})\\) with a Gaussian \\(N(\\tilde{Q}\\tilde{\\mathbf{b}},\\tilde{Q})\\) with \\(\\tilde{Q}= Q + \\begin{bmatrix}\\text{diag}(\\mathbf{c}) &0\\\\0&0 \\end{bmatrix}\\) and \\(\\tilde{\\mathbf{b}} = [\\mathbf{b}~ 0]^\\intercal\\)\n\n\\(\\mathbf{b}\\): Contains the first derivatives of the log-likelihood w.r.t. each \\(\\eta_i\\), evaluated at the expansion point (i.e. the mode).\n\\(\\mathbf{c}\\): Contains the negative second derivatives (observed Fisher information) of the log-likelihood w.r.t. each \\(\\eta_i\\).\n\nThus, the approximation of the joint posterior of the hyperparameters is given by:\n\\[\n\\begin{aligned}\np(\\theta\\mid \\mathbf{y}) &\\propto \\dfrac{p(\\mathbf{y}\\mid \\mathbf{u},\\theta)p(\\mathbf{u}\\mid \\theta)p(\\theta)}{p(\\mathbf{u}\\mid \\theta, \\mathbf{y})} \\\\\n&\\approx\n\\left.\n\\frac{\np(\\mathbf{y} \\mid \\mathbf{u},\\theta)\\,\np(\\mathbf{u} \\mid \\theta)\\,\np(\\theta)\n}{\np_G(\\mathbf{u} \\mid \\mathbf{y},\\theta)\n}\n\\right|_{u=u^*(\\theta)}.\n\\end{aligned}\n\\]\nwhere \\(p_G(\\mathbf{u} \\mid \\mathbf{y},\\theta))\\) is the Gaussian approximation – based the Laplace method - of \\(p(\\mathbf{u}\\mid \\theta, \\mathbf{y})\\) and \\(u^*(\\theta)\\) is the mode for a given \\(\\theta\\). This Gaussian approximation will be exact as \\(n \\to \\infty\\).\nDifferent optimization techniques like Newton-type methods can be used to find the mode \\(\\tilde{p}(\\theta\\mid \\mathbf{y})\\) and then exploring \\(\\tilde{p}(\\theta\\mid \\mathbf{y})\\) to find grid point for numerical integration. Figure 1 illustrates the numerical integration approach to compute \\(p(\\theta\\mid \\mathbf{y})\\) for a regular grid series of points \\(\\theta_k, k = 1,\\ldots,K\\)\n\n\n\n\n\n\nFigure 1: Illustration of numerical approximation of the hyperparameters posterior density\n\n\n\nNow we just need to approximate \\(p(u_i\\mid \\theta\\mathbf{y})\\), but unfortunately we cannot use the marginals from the preivous Gaussian approximation \\(p_G(\\mathbf{u} \\mid \\mathbf{y},\\theta)\\) directly as there can be errors in the location and/or errors due to the lack of skewness."
  },
  {
    "objectID": "Bayesian_computing.html#approximating-the-posterior-for-the-latent-field",
    "href": "Bayesian_computing.html#approximating-the-posterior-for-the-latent-field",
    "title": "Introduction to the Integrated Laplace Approximation",
    "section": "Approximating the posterior for the latent field",
    "text": "Approximating the posterior for the latent field\nThere are two ways of approximating the marginals for the latent fields. The first one is by simply using the Gaussian approximation and computing the posterior conditional distributions \\(p(u_i\\mid \\theta,\\mathbf{y})\\) directly as marginals of \\(p_G(\\mathbf{u} \\mid \\mathbf{y},\\theta)\\) i.e.,\n\\[\n\\tilde{p}(u_i\\mid \\theta,\\mathbf{y})=N(u_i;\\mu_i(\\theta),\\sigma_i^2(\\theta))\n\\]\nwith mean \\(\\mu_i(\\theta)\\) and variance \\(\\sigma^2_i(\\theta)\\). However, while computationally cost effective, this approximation is generally not very good specially for non-Gaussian likelihoods due to errors in the location and /or lack of skewness.\nThus, a different approach is to write the vector of hyperparameters \\(\\mathbf{u} = \\{u_i,\\mathbf{u}_{-i}\\}\\) and use the Laplace approximation again for:\n\\[\n\\begin{aligned}\np(u_i \\mid \\boldsymbol{\\psi}, \\mathbf{y}) &= \\frac{p(\\{u_i,\\mathbf{u}_{-i}\\} \\mid \\theta, \\mathbf{y})}{p(\\mathbf{u}_{-i}\\mid u_i, \\theta, \\mathbf{y})} \\\\\n&= \\frac{p(\\mathbf{u}, \\theta \\mid \\mathbf{y})}{p(\\theta \\mid \\mathbf{y})} \\cdot \\frac{1}{p(\\mathbf{u}_{-i} \\mid u_i, \\theta, \\mathbf{y})}\\\\\n&\\propto \\frac{p(\\mathbf{u}, \\theta \\mid \\mathbf{y})}{p(\\mathbf{u}_{-i} \\mid u_i, \\theta, \\mathbf{y})} \\\\\n&\\left.\\approx \\frac{p(\\mathbf{u}, \\theta \\mid \\mathbf{y})}{\\widetilde{p}(\\mathbf{u}_{-i} \\mid u_i, \\theta, \\mathbf{y})} \\right|_{\\mathbf{u}_{-i}=\\mathbf{u}^*_{-i}(u_i,\\theta)}=:\\tilde{p}(u_i\\mid\\theta,\\mathbf{y}).\n\\end{aligned}\n\\]\nwhere \\(\\widetilde{p}(\\mathbf{u}_{-i} \\mid u_i, \\theta, \\mathbf{y})\\) is the Laplace Gaussian approximation to \\(p(\\mathbf{u}_{-i} \\mid u_i, \\theta, \\mathbf{y})\\) and \\(\\mathbf{u}_{-i}^*(u_i, \\theta)\\) is its mode.\nThis nested extra Laplace approximation step (an hence the Integrated Nested Laplace Approximation name) is sufficiently accurate but computationally more expensive that the simpler Gaussian strategy. Thus, Variational Bayes techniques have been used to improve the mean of the GMRF approximation instead (see (Niekerk and Rue 2021)). This correction is computationally efficient and achieves accuracy for the mean similar to a full integrated nested Laplace approximation.\nFrom this improved GMRF approximation we can then compute the marginals \\(p_G^*(u_i \\mid \\mathbf{y},\\theta)\\) to obtain:\n\\[\n\\tilde{p}(u_i\\mid\\mathbf{y}) = \\sum_k p_G^*(u_i \\mid \\mathbf{y},\\theta) \\tilde{p}(\\theta\\mid\\mathbf{y})w_k\n\\]"
  },
  {
    "objectID": "notes/tutorial_sheet_4.html",
    "href": "notes/tutorial_sheet_4.html",
    "title": "Tutorial Sheet 4",
    "section": "",
    "text": "The figure below shows coloured dissolved organic matter (CDOM) across a lake along with the corresponding Moran's I plot (Figure 1). \n\n\n\n\n\n\nFigure 1: Lake CDOM plotted over space (left) and the corresponding Moran's I plot (right) based on Queen's distance\n\n\n\nThe estimated Global Moran's I is 0.02 with a variance of 0.02.\n\n\n\n\n\n\nInterpreting Moran’s I plot\n\n\n\nA Moran's I scatter plot visualizes spatial autocorrelation by plotting the values of observations in different areas against their spatially lagged values, which represent the weighted average of neighboring values. A positive correlation in the plot suggests spatial clustering (similar values are located near each other), while a negative correlation indicates spatial dispersion (neighboring values tend to be dissimilar). The slope of the regression line through the points provides an estimate of Moran's I, a statistic that quantifies the degree of spatial autocorrelation.\n\n\n\n\nDefine what is meant by the term neighbourhood matrix and discuss two approaches for defining it, giving a drawback of each\n\n\n\nComment on the spatial variability in CDOM for this lake, with specific reference to the Moran's I plot\n\n\n\nIn your own words what does the Modifiable Areal Unit Problem (MAUP) refers to?\n\n\n\nFigure 2 shows the number of lung cancer cases in Pennsylvania per county, USA, in 2002.\n\n\n\n\n\n\n\n\nFigure 2: Number of Lung Cancer cases in Pennsylavnia counties in 2002\n\n\n\n\n\n\nInterpret the map , what does this tells you about the number of lung cancer cases?\nIn addition to the number of cases, the data set contains county-level information of the following variables:\n\n\nExpected number of cases \\(E_i\\) computed as \\(E_i = \\sum_{j=1}^{m} r_j \\times n_j\\) ( \\(r_j\\) is rate of disease and \\(n_j\\) the population in stratum \\(j\\))\nOverall county smoking rate\n\nPropose a reasonable spatially explicit model to assess the relationship between lung cancer risk and smoking."
  },
  {
    "objectID": "notes/tutorial_sheet_4.html#task-1",
    "href": "notes/tutorial_sheet_4.html#task-1",
    "title": "Tutorial Sheet 4",
    "section": "",
    "text": "Define what is meant by the term neighbourhood matrix and discuss two approaches for defining it, giving a drawback of each"
  },
  {
    "objectID": "notes/tutorial_sheet_4.html#task-2",
    "href": "notes/tutorial_sheet_4.html#task-2",
    "title": "Tutorial Sheet 4",
    "section": "",
    "text": "Comment on the spatial variability in CDOM for this lake, with specific reference to the Moran's I plot"
  },
  {
    "objectID": "notes/tutorial_sheet_4.html#task-3",
    "href": "notes/tutorial_sheet_4.html#task-3",
    "title": "Tutorial Sheet 4",
    "section": "",
    "text": "In your own words what does the Modifiable Areal Unit Problem (MAUP) refers to?"
  },
  {
    "objectID": "notes/tutorial_sheet_4.html#task-4",
    "href": "notes/tutorial_sheet_4.html#task-4",
    "title": "Tutorial Sheet 4",
    "section": "",
    "text": "Figure 2 shows the number of lung cancer cases in Pennsylvania per county, USA, in 2002.\n\n\n\n\n\n\n\n\nFigure 2: Number of Lung Cancer cases in Pennsylavnia counties in 2002\n\n\n\n\n\n\nInterpret the map , what does this tells you about the number of lung cancer cases?\nIn addition to the number of cases, the data set contains county-level information of the following variables:\n\n\nExpected number of cases \\(E_i\\) computed as \\(E_i = \\sum_{j=1}^{m} r_j \\times n_j\\) ( \\(r_j\\) is rate of disease and \\(n_j\\) the population in stratum \\(j\\))\nOverall county smoking rate\n\nPropose a reasonable spatially explicit model to assess the relationship between lung cancer risk and smoking."
  },
  {
    "objectID": "notes/tutorial_sheet_4.html#task-5",
    "href": "notes/tutorial_sheet_4.html#task-5",
    "title": "Tutorial Sheet 4",
    "section": "Task 5",
    "text": "Task 5\nSuppose we have a geostatistical process, \\(\\{Z(s); s \\in D\\},~ D \\subset\\mathbb{R}^2\\) which is stationary with mean, \\(\\mu_z\\) and covariance \\(\\mathrm{Cov}(h)\\). Define what is meant by:\n\nweakly stationary\nisotropic\n\nThen, write  down  an  expression  for  the  autocorrelation  function \\(\\rho_z(h)\\)  in  terms  of  the covariance function."
  },
  {
    "objectID": "notes/tutorial_sheet_4.html#task-6",
    "href": "notes/tutorial_sheet_4.html#task-6",
    "title": "Tutorial Sheet 4",
    "section": "Task 6",
    "text": "Task 6\nFigure 3 below shows summary plots for measurements of a geostatistical process, namely levels of Nitrogen measured at different locations across Chesapeake Bay, USA. Subjectively, comment on what each of these plots tells you about the data\n\n\n\n\n\n\nFigure 3: Summary plots for nitrogen levels across Chesapeake bay. Top left plot colour scale red to blue (high to low values)."
  },
  {
    "objectID": "notes/tutorial_sheet_4.html#task-7",
    "href": "notes/tutorial_sheet_4.html#task-7",
    "title": "Tutorial Sheet 4",
    "section": "Task 7",
    "text": "Task 7\nDescribe how you would check for spatial correlation in the data and model it if it were present."
  },
  {
    "objectID": "notes/tutorial_sheet_4.html#task-8",
    "href": "notes/tutorial_sheet_4.html#task-8",
    "title": "Tutorial Sheet 4",
    "section": "Task 8",
    "text": "Task 8\nAn empirical semi-variogram with a MC envelope is shown below (Figure 4). Comment on the plot with regards to the presence of spatial correlation.\n\n\n\n\n\n\nFigure 4: Empirical variogram and MC simulation envelope"
  },
  {
    "objectID": "notes/tutorial_sheet_4.html#task-9",
    "href": "notes/tutorial_sheet_4.html#task-9",
    "title": "Tutorial Sheet 4",
    "section": "Task 9",
    "text": "Task 9\nExplain what a spatial point pattern is, define complete spatial randomness, and how it is assessed."
  },
  {
    "objectID": "notes/tutorial_sheet_4.html#task-10",
    "href": "notes/tutorial_sheet_4.html#task-10",
    "title": "Tutorial Sheet 4",
    "section": "Task 10",
    "text": "Task 10\nDiscuss two limitations of using Ripleys’ \\(K\\) for assessing CSR in spatial point patterns."
  },
  {
    "objectID": "notes/tutorial_sheet_4.html#task-11",
    "href": "notes/tutorial_sheet_4.html#task-11",
    "title": "Tutorial Sheet 4",
    "section": "Task 11",
    "text": "Task 11\nFigure 5 (a, left) shows the \\(K\\) function for the locations of 500 trees of a particular species within an area of tropical rainforest while Figure 5 (b, right) shows the \\(K\\) function for the Lansing wood tree species. For each of these plots comment on the spatial pattern of the data that generated the K functions with respect to complete spatial randomness.\n\n\n\n\n\n\nFigure 5: (a) \\(K\\) function for location of a particular species of tree in a tropical rainforest and (b) the \\(K\\) function for the distribution of trees in Lansing Wood."
  },
  {
    "objectID": "notes/notes_7.html",
    "href": "notes/notes_7.html",
    "title": "Modelling Areal Data",
    "section": "",
    "text": "Space is inherent to all ecological processes, influencing dynamics such as migration, dispersal, and species interactions. A primary goal in ecology is to understand how these processes shape species distributions and dynamics across space.\n\nSpatial data are data which have any form of geographical information attached to them. The emergence of modelling frameworks for spatial ecology has been facilitated by the rise of new technologies and data collection systems like aerial photographs, GPS tracking, satellite imagery, and biologging devices.\n\n\n\n\n\nGPS tags for survey and monitoring waterbirds movement\n\nMost of environmental variables and ecological processes of interest will vary over space to some degree and we will typically use this spatial information to help us understand the relationship between our data points. Furthermore, incorporating space into statistical modelling approaches has driven the formulation of novel ecological questions and the development of new analytical methods to address them.\nThe overall goal of any piece of spatial analysis is to understand the spatial patterns in our data. This could involve:\n\nEstimating differences in mean, variance or some other summary statistic over space.\nPredicting the value at some unobserved location.\nIdentifying hotspot with high (or low) value compared to the rest of the region.\n\naims also vary with different types of spatial data…\n\nSpatial patterns are omnipresent in both environmental and ecological data. In general, our environmental or ecological processes of interest can be described by three main categories of spatial data :\nDiscrete space:\n\nData on a spatial grid (areal data)\n\nContinuous space:\n\nGeostatistical (geo-referenced) data\nSpatial point processe data\n\nThe components of the models we will cover are used to reflect spatial dependence structures in discrete and continuous space.\nIn point processes we measure the locations where events occur (e.g. trees in a forest, earthquakes) and the coordinates of such occurrences are our data.\n\n\nOccurrence records of four ungulate species in the Tibet (taken from (Liang et al. 2021)).\n\nIn geostatistical data, measurements are taken at a set of fixed locations. (e.g. air monitoring stations to quantify pollution levels, or a quadrants strategically placed to monitor wildlife).\n\n\nScotland river temperature monitoring network\n\nFinally, in areal data our measurements are summarised across a set of discrete, non-overlapping spatial units (e.g., delimited conservation regions, postcode areas, council regions).\n\n\nMap of bird conservation regions (BCRs) showing the proportion of bird species within each region showing a declining trend (Stanton et al. 2016)\n\n\nScales describe the spatial and temporal dimensions at which an ecological process occurs. Many ecological patterns and processes occur at different spatial and temporal scales and thus, understanding and quantifying these scales is essential to provide an accurate intrepretation of the ecological processes we aimed to study.\nThe spatial scale is often described by the grain (a.k.a. spatial resolution) and the extent. The former refersto the finest spatial unit of measurement for a given process, whereas the later refer to the total length or area of the study. The ratio between these two measurements is knows as Scope.\nThere is typically a trade-off between the grain and the extent, mainly due to practicality. For example, it can be costly to work at large extents while collecting data at fine grain sizes. But also, some of the processes occurring at these finer scales become simply noise when we look at systems at larger extents.\nIn spatial ecology, the scale at which an ecological process occur can have a major impact on the interpretation of our analysis. For example, Figure 1 shows the locations of isopods burrows in the northern Negev Desert, Israel. If we look at the complete data set we can see a strong aggregation pattern. However, when focusing on two subsets with smaller extents, the observed patterns shift, ranging from random to aggregated.\n\n\n\n\n\nFigure 1: Isopod burrow data with n=2015 individual burrows in a study area with an extent of 75 600 m\\(^2\\). Inset maps show two subsets with reduced extent of 6.4 m\\(^2\\) each. Source: (Dungan et al. 2002)\n\n\nThe spatial scale of analysis is crucial in ecological and environmental studies, as it can significantly influence the patterns we observe and the conclusions we draw. For example, changing the grain of the study (while holding the extent constant) can introduce bias and uncertainty in the observed ecological patterns. This is because aggregated data can have different properties than the sample data from which they were derived, i.e. the assumption that a relationships exists one level of aggregation does not necessarily hold at another level of aggregation, particularly in situations where data are aggregated into irregular sampling units (modifiable areal unit problem -MAUP). This is illustrated in figure Figure 2, where two variables (v1 and v2) recorded on uniform point level show little correlation. As we aggregate the values on a larger grain size, we get a slight increase in the slope and \\(R^2\\) values. Now if we aggregate the same point data using the non-homogeneous aggregation scheme now v1 and v2 become highly correlated.\n\n\n\n\n\nFigure 2: Plots of variables v1 and v2 for each individual in the survey and summarized using a uniform and non-uniform aggregation scheme\n\n\nThus, if the data we had at hand came from the non-uniform aggregation scheme, it would be wrong to assume that a correlation also exists on an individual-level data. In ecology this is knows as ecological fallacy, which arises when incorrect inferences about individual sample units are made based on aggregated data.\nFor example, aggregating species abundance (counts) data at a national scale might obscure local hotspots of biodiversity observed at finer scales. Results depend on how the areas are divided, even if the total number of zones remains constant. Different arrangements of the same spatial units can lead to different conclusions. When individual-level data is grouped, the variability within units is lost, which can mask finer-scale patterns or exaggerate certain trends. As consequence, conservation strategies should consider the spatial scale for effective conservation decisions.\n\nSpatial modelling: why is it necessary?\nTobler’s first law of geography states that:\n\nEverything is related to everything else, but near things are more related than distant things”\n\nSpatial statistics quantify spatial variance and correlation based on distance, aligning with Tobler’s law—nearby measurements tend to be more correlated, while this relationship weakens with distance. Spatial dependence can reveal ecological processes like species interactions and responses to environmental gradients. However, it also poses challenges for statistical analysis, as it violates the common assumption of independent data in traditional methods.\nAnalyzing spatial dependence can offer insights into the biological processes shaping observed patterns, such as social behavior, resource distribution, or dispersal. While measuring spatial dependence alone may not provide definitive answers, it helps generate hypotheses and refine predictions. Additionally, spatial dependence can influence assessments of conservation threats and strategies.\nSpatial dependence can arise for many reasons. It may result from endogenous processes inherent to the system (e.g., localized dispersal leading to organism clustering or social and grouping behaviors) or be the result of exogenous factors such as spatially dependent environmental gradients used by the organism of interest. Additionally, spatial dependence can stem from model mis-specifications, including the omission of key covariates or incorrect functional assumptions, such as failing to account for nonlinear relationships between predictors and responses.\nIn ecology, spatial data often come from point samples, where spatial relationships are captured through x–y coordinates (point processes) or aerial (lattice) data, such as counties or watersheds, where spatial dependence is modeled using a neighborhood matrix (spatial weights matrix). Ecological models commonly incorporate these spatial structures to account for spatial dependence in the process of interest.\n\n\nExamples of aerial data used in spatial modelling. Aereal data can come from (a) polygon based information or can be generated (b) from point data using Voronoi tessellation. We can describe spatial dependence through the links among locations (right panel) with a spatial neighborhood (weights) matrix\n\nEcological models can be classified based on the spatial structure they consider. For example, spatial filtering methods account for spatial patterns by incorporating functions of x–y coordinates. In this approach, space is treated as a predictor variable within a regression framework—typically a flexible model such as polynomial regression or a generalized additive model (GAM), where smooth terms for x–y coordinates help capture large-scale spatial dependence within a region. Another common approach is the use of conditional autoregressive models, which are applied to areal (or lattice) data. In these models, spatial dependence is captured through a spatial neighborhood weights matrix. Before exploring these models further, let’s briefly define what areal data are."
  },
  {
    "objectID": "notes/notes_7.html#types-of-spatial-data",
    "href": "notes/notes_7.html#types-of-spatial-data",
    "title": "Modelling Areal Data",
    "section": "",
    "text": "Spatial patterns are omnipresent in both environmental and ecological data. In general, our environmental or ecological processes of interest can be described by three main categories of spatial data :\nDiscrete space:\n\nData on a spatial grid (areal data)\n\nContinuous space:\n\nGeostatistical (geo-referenced) data\nSpatial point processe data\n\nThe components of the models we will cover are used to reflect spatial dependence structures in discrete and continuous space.\nIn point processes we measure the locations where events occur (e.g. trees in a forest, earthquakes) and the coordinates of such occurrences are our data.\n\n\nOccurrence records of four ungulate species in the Tibet (taken from (Liang et al. 2021)).\n\nIn geostatistical data, measurements are taken at a set of fixed locations. (e.g. air monitoring stations to quantify pollution levels, or a quadrants strategically placed to monitor wildlife).\n\n\nScotland river temperature monitoring network\n\nFinally, in areal data our measurements are summarised across a set of discrete, non-overlapping spatial units (e.g., delimited conservation regions, postcode areas, council regions).\n\n\nMap of bird conservation regions (BCRs) showing the proportion of bird species within each region showing a declining trend (Stanton et al. 2016)"
  },
  {
    "objectID": "notes/notes_7.html#sec-scales",
    "href": "notes/notes_7.html#sec-scales",
    "title": "Modelling Areal Data",
    "section": "",
    "text": "Scales describe the spatial and temporal dimensions at which an ecological process occurs. Many ecological patterns and processes occur at different spatial and temporal scales and thus, understanding and quantifying these scales is essential to provide an accurate intrepretation of the ecological processes we aimed to study.\nThe spatial scale is often described by the grain (a.k.a. spatial resolution) and the extent. The former refersto the finest spatial unit of measurement for a given process, whereas the later refer to the total length or area of the study. The ratio between these two measurements is knows as Scope.\nThere is typically a trade-off between the grain and the extent, mainly due to practicality. For example, it can be costly to work at large extents while collecting data at fine grain sizes. But also, some of the processes occurring at these finer scales become simply noise when we look at systems at larger extents.\nIn spatial ecology, the scale at which an ecological process occur can have a major impact on the interpretation of our analysis. For example, Figure 1 shows the locations of isopods burrows in the northern Negev Desert, Israel. If we look at the complete data set we can see a strong aggregation pattern. However, when focusing on two subsets with smaller extents, the observed patterns shift, ranging from random to aggregated.\n\n\n\n\n\nFigure 1: Isopod burrow data with n=2015 individual burrows in a study area with an extent of 75 600 m\\(^2\\). Inset maps show two subsets with reduced extent of 6.4 m\\(^2\\) each. Source: (Dungan et al. 2002)\n\n\nThe spatial scale of analysis is crucial in ecological and environmental studies, as it can significantly influence the patterns we observe and the conclusions we draw. For example, changing the grain of the study (while holding the extent constant) can introduce bias and uncertainty in the observed ecological patterns. This is because aggregated data can have different properties than the sample data from which they were derived, i.e. the assumption that a relationships exists one level of aggregation does not necessarily hold at another level of aggregation, particularly in situations where data are aggregated into irregular sampling units (modifiable areal unit problem -MAUP). This is illustrated in figure Figure 2, where two variables (v1 and v2) recorded on uniform point level show little correlation. As we aggregate the values on a larger grain size, we get a slight increase in the slope and \\(R^2\\) values. Now if we aggregate the same point data using the non-homogeneous aggregation scheme now v1 and v2 become highly correlated.\n\n\n\n\n\nFigure 2: Plots of variables v1 and v2 for each individual in the survey and summarized using a uniform and non-uniform aggregation scheme\n\n\nThus, if the data we had at hand came from the non-uniform aggregation scheme, it would be wrong to assume that a correlation also exists on an individual-level data. In ecology this is knows as ecological fallacy, which arises when incorrect inferences about individual sample units are made based on aggregated data.\nFor example, aggregating species abundance (counts) data at a national scale might obscure local hotspots of biodiversity observed at finer scales. Results depend on how the areas are divided, even if the total number of zones remains constant. Different arrangements of the same spatial units can lead to different conclusions. When individual-level data is grouped, the variability within units is lost, which can mask finer-scale patterns or exaggerate certain trends. As consequence, conservation strategies should consider the spatial scale for effective conservation decisions."
  },
  {
    "objectID": "notes/notes_7.html#spatial-dependence",
    "href": "notes/notes_7.html#spatial-dependence",
    "title": "Modelling Areal Data",
    "section": "",
    "text": "Spatial modelling: why is it necessary?\nTobler’s first law of geography states that:\n\nEverything is related to everything else, but near things are more related than distant things”\n\nSpatial statistics quantify spatial variance and correlation based on distance, aligning with Tobler’s law—nearby measurements tend to be more correlated, while this relationship weakens with distance. Spatial dependence can reveal ecological processes like species interactions and responses to environmental gradients. However, it also poses challenges for statistical analysis, as it violates the common assumption of independent data in traditional methods.\nAnalyzing spatial dependence can offer insights into the biological processes shaping observed patterns, such as social behavior, resource distribution, or dispersal. While measuring spatial dependence alone may not provide definitive answers, it helps generate hypotheses and refine predictions. Additionally, spatial dependence can influence assessments of conservation threats and strategies.\nSpatial dependence can arise for many reasons. It may result from endogenous processes inherent to the system (e.g., localized dispersal leading to organism clustering or social and grouping behaviors) or be the result of exogenous factors such as spatially dependent environmental gradients used by the organism of interest. Additionally, spatial dependence can stem from model mis-specifications, including the omission of key covariates or incorrect functional assumptions, such as failing to account for nonlinear relationships between predictors and responses.\nIn ecology, spatial data often come from point samples, where spatial relationships are captured through x–y coordinates (point processes) or aerial (lattice) data, such as counties or watersheds, where spatial dependence is modeled using a neighborhood matrix (spatial weights matrix). Ecological models commonly incorporate these spatial structures to account for spatial dependence in the process of interest.\n\n\nExamples of aerial data used in spatial modelling. Aereal data can come from (a) polygon based information or can be generated (b) from point data using Voronoi tessellation. We can describe spatial dependence through the links among locations (right panel) with a spatial neighborhood (weights) matrix\n\nEcological models can be classified based on the spatial structure they consider. For example, spatial filtering methods account for spatial patterns by incorporating functions of x–y coordinates. In this approach, space is treated as a predictor variable within a regression framework—typically a flexible model such as polynomial regression or a generalized additive model (GAM), where smooth terms for x–y coordinates help capture large-scale spatial dependence within a region. Another common approach is the use of conditional autoregressive models, which are applied to areal (or lattice) data. In these models, spatial dependence is captured through a spatial neighborhood weights matrix. Before exploring these models further, let’s briefly define what areal data are."
  },
  {
    "objectID": "notes/notes_7.html#overview-of-areal-data",
    "href": "notes/notes_7.html#overview-of-areal-data",
    "title": "Modelling Areal Data",
    "section": "\n2.1 Overview of Areal Data",
    "text": "2.1 Overview of Areal Data\nAreal data are data which come from well defined geographical units such as postcode areas, health board or pixels on a satellite image. Many public health studies use data aggregated over groups rather than data on individuals - often this is for privacy reasons, but it may also be for convenience.\nAs with the other types of spatial modelling, our goal is to observe and explain spatial variation in our data. Again, we are trying to understand and account for spatial dependence. Generally, we are aiming to produce a smoothed map which summarises the spatial patterns we observe in our data. This often leads to identifying spatial extremes and/or boundaries (step changes) in the spatial surface.\nAreal data are commonly used in public health settings. The image shows pollution in counties of England\n\n\n\n\nAreal data is also very common in ecology where samples are taken on a lattice or become the byproduct of some sort aggregated spatial point process. For example, the map below shows the species richness (no. of different species) occur within a 10 km grid in Scotland.\n\n\nSpecies richness in Scotland in 2020 by the Butterfly conservation UKBMS"
  },
  {
    "objectID": "notes/notes_7.html#definition-of-an-areal-process",
    "href": "notes/notes_7.html#definition-of-an-areal-process",
    "title": "Modelling Areal Data",
    "section": "\n2.2 Definition of an Areal Process",
    "text": "2.2 Definition of an Areal Process\nAn areal process (or lattice process) is a stochastic process defined on a set of regions that form a partition of our region of interest \\(D\\). Let \\(B_1, \\ldots B_m\\) be our set of \\(m\\) distinct regions such that:\n\\[\\bigcup\\limits_{i=1}^m \\hspace{1mm}B_i = D.\\]\nHere we require that our regions are non-overlapping, with\n\\[B_i \\cap B_j = \\emptyset.\\] Then our areal process is simply the stochastic process \\[\\{Z(B_i); i=1,\\ldots,m\\}.\\]"
  },
  {
    "objectID": "notes/notes_7.html#sparse-matrices",
    "href": "notes/notes_7.html#sparse-matrices",
    "title": "Modelling Areal Data",
    "section": "\n3.1 Sparse Matrices",
    "text": "3.1 Sparse Matrices\nThey key lies in sparse matrices. Assume that our random effect is modelled as follows:\n\\[\n\\mathcal{u}\\sim\\mathcal{N}(0,\\Sigma)\n\\]\nWe have tow options:\n\nForce the covariance matrix \\(\\Sigma\\) to be sparse\nForce the precision matrix \\(\\Sigma^{-1}\\) to be sparse\n\nBy forcing the covariance matrix \\(\\Sigma\\) to be sparse we have that:\n\n\\(\\Sigma_{ij} = \\text{Cov}(u_i, u_j)\\) : Covariance between \\(u_i\\) and \\(u_j\\)\n\\(\\Sigma_{ij} = 0\\) \\(\\longrightarrow\\) \\(u_i\\) and \\(u_j\\) are independent\nA sparse covariance matrix implies that many elements of \\(\\mathbf{u}\\) are mutually independent…..is this desirable?\n\nAlternatively, if we force the precision matrix \\(\\mathbf{Q} = \\Sigma^{-1}\\) to be sparse we might ask ourselves\n\nWhat does \\(Q_{ij}\\) represents?\nWhat does a sparse precision matrix implies?\n\nLets look at a simpler example.\n\n3.1.1 Example: The AR(1) process\nSuupose the following AR(1) process:\n\\[\n\\begin{aligned}\n\\mathbf{i=1}&:  x_1 \\sim N\\left(0, \\frac{1}{1-\\phi^2}\\right)\\\\\n\\mathbf{i=2,\\dots,T}&:  x_i  = \\phi\\  x_{i-1} +\\epsilon_i,\\ \\epsilon_i\\sim\\mathcal{N}(0,1)\n\\end{aligned}\n\\]\nThis AR(1) is very common to model dependence in time. The joint distribution of \\(\\mathbf{x}=x_1,x_2,\\dots\\) is Gaussian and the Covariance Matrix is given by\n\\[\n\\Sigma = \\frac{1}{1-\\phi^2}  \\begin{bmatrix}\n1& \\phi & \\phi^2  & \\dots& \\phi^N \\\\\n\\phi & 1& \\phi  & \\dots& \\phi^{N-1} \\\\\n\\phi^2 & \\phi & 1 & \\dots& \\phi^{N-2} \\\\\n\\dots& \\dots& \\dots& \\dots& \\dots& \\\\\n\\phi^{N} & \\phi^{N-1}& \\phi^{N-2}  & \\dots& 1\\\\\n\\end{bmatrix}\n\\]\nNote that this is a dense matrix where all elements of the \\(\\mathbf{x}\\) vector are dependent. We can visualize the covariance function for different time lags displacements \\(h\\) to see this:\n\n\n\n\n\n\n\n\nNow lets have a look at the Precision Matrix\n\\[\n\\mathbf{Q} = \\Sigma^{-1} =  \\begin{bmatrix}\n1& -\\phi & 0  & 0 &\\dots& 0 \\\\\n-\\phi & 1 + \\phi^2& -\\phi  & 0 & \\dots& 0 \\\\\n0 & -\\phi & 1-\\phi^2 &-\\phi &  \\dots& 0 \\\\\n0 & 0 & -\\phi &1-\\phi^2 & \\dots & \\dots \\\\\n\\dots& \\dots& \\dots& \\dots& \\dots& \\dots& \\\\\n0 &0 & 0 & \\dots  & -\\phi& 1\\\\\n\\end{bmatrix}\n\\]\nNote that:\n\nThis is a tridiagonal matrix, it is sparse\nThe tridiagonal form of \\(\\mathbf{Q}\\) can be exploited for quick calculations.\n\nWhat is the key property of this example that causes \\(\\mathbf{Q}\\) to be sparse?\nThe key lies in the full conditionals\n\\[\nx_t|\\mathbf{x}_{-t}\\sim\\mathcal{N}\\left(\\frac{\\phi}{1-\\phi^2}(x_{t-1}+x_{t+1}), \\frac{1}{1+\\phi^2}\\right)\n\\]\n\n\n\n\nHere:\n\nThe circles represent the values of \\(x\\) at individual time points\nThere is a line between them if they are conditionally dependent (e.g., \\(x_t\\) is conditionally independent of \\(x_{t-2}\\) given \\(x_{t-1}\\))\nEach timepoint is only conditionally dependent on the two closest neighbours\nThe nonzero pattern in the precision matrix is given by the neighborhood structure of the process\n\n3.1.2 Markov models in space\nWe can extend the previous idea into space. For example, on the following first order conditional autoregressive model or a CAR(1) model every node is conditionally dependent on its four nearest neighbours\n\n\n\n\nModels based on neighbourhood have a name in statistics: they are Markovian models. Markovian models are specified entirely through “neighbourhood structures”\nRecall our first Model:\n\\[\n\\begin{aligned}\ny_i &\\sim \\mathrm{Poisson}(e^{\\eta_i}) \\\\\n\\eta_i &= \\beta_0 + \\mathbf{x}'\\beta + u_i ~~~ u_i \\sim N(0,Q^{-1})\n\\end{aligned}\n\\]\nThis mean \\(u_i\\) is independent of all the other parameters \\(\\mathbf{u}_{-i}\\), given the set of its neighbors. I.e., for any pair of elements (\\(i,j\\)) in \\(\\mathbf{u}\\), \\(u_i\\perp u_j|\\mathbf{u}_{-ij}\\Longleftrightarrow Q{ij} =0\\). In other words, \\(Q{ij} \\neq 0\\) only if \\(j\\in \\{i,\\mathcal{N}(i)\\}\\)."
  },
  {
    "objectID": "notes/notes_7.html#gaussian-markov-random-field-gmrf",
    "href": "notes/notes_7.html#gaussian-markov-random-field-gmrf",
    "title": "Modelling Areal Data",
    "section": "\n3.2 Gaussian Markov random field (GMRF)",
    "text": "3.2 Gaussian Markov random field (GMRF)\nAn informal definition of a Gauss Markov random field (GMRF) is\n\na Gaussian distribution where the non-zero elements of the precision matrix are defined by a neighbourhood matrix (or graph structure)\neach region conditionally has a Gaussian distribution with\nmean equal to the average of the neighbours and\nprecision proportional to the number of neighbours\n\nA more formal definition goes as follows:\nLet \\(\\mathbf{u}\\) be a GMRF wrt \\(\\mathcal{G} = (\\mathcal{V},\\mathcal{E})\\)).\n\\(\\mathcal{V}\\) Vertices: \\(1,2,\\ldots,n\\). \\(\\mathcal{E}\\) Edges \\(\\{i,j\\}\\)\n\nNo edge between \\(i\\) and \\(j\\) if \\(u_i \\perp u_j \\mid \\mathbf{u}_{ij}.\\)\nAn edge between \\(i\\) and \\(j\\) if \\(u_i \\not\\perp u_j \\mid \\mathbf{u}_{ij}.\\)\n\nKey point: A graph defines the sparsity structure of Q\n\n\n\n\n\n\nDefinition\n\n\n\nA random variable \\(\\mathbf{u}\\) is said to be a GMRF wrt to the graph \\(\\mathcal{G}\\) with vertices \\(\\{1, 2,\\dots , n\\}\\) and edges \\(\\mathcal{E}\\) , with mean \\(\\mu\\) and precision matrix \\(\\mathbf{Q}\\) if its probability distribution is given by\n\\[\n\\mathbf{u} \\sim N(\\mu,Q^{-1})\n\\]\nand\n\\(Q_{ij} \\neq 0\\Longleftrightarrow \\{i,j\\}\\in\\mathcal{E} ~~\\forall~~i\\neq j\\)"
  },
  {
    "objectID": "notes/notes_7.html#icar-model",
    "href": "notes/notes_7.html#icar-model",
    "title": "Modelling Areal Data",
    "section": "\n3.3 ICAR Model",
    "text": "3.3 ICAR Model\nAn example of a GMRF is the the Besag model a.k.a. Intrinsic Conditional Autoregressive (ICAR) model. The conditional distribution for \\(u_i\\) is\n\\[\nu_i|\\mathbf{u}_{-i},\\tau_u, \\sim N\\left(\\frac{1}{d_i}\\sum_{j\\sim i}u_j,\\frac{1}{d_i\\tau_u}\\right)\n\\]\n\n\\(\\mathbf{u}_{-i} = (u_i,\\ldots,u_{i-1},u_{i+1},\\ldots,u_n)^T\\)\n\\(\\tau_u\\) is the precision parameter (inverse variance).\n\\(d_i\\) is the number of neighbours\nThe mean of \\(u_i\\) is equivalent to the the mean of the effects over all neighbours, and the precision is proportional to the number of neighbours (e.g., if an area has many neighbors then its variance will be smaller)\n\nThen, the joint distribution is given by:\n\\[\n\\mathbf{u}|\\tau_u \\sim N\\left(0,\\frac{1}{\\tau_u}Q^{-1}\\right),\n\\]\nWhere \\(Q\\) denotes the precision matrix defined as\n\\[\nQ_{i,j} = \\begin{cases}\nd_i, & i = j \\\\\n-1, & i \\sim j \\\\\n0, &\\text{otherwise}\n\\end{cases}\n\\tag{1}\\]\nThis structure matrix directly defines the neighbourhood structure and is sparse.\n\n\n\n\n\n\nExample\n\n\n\nLets look a Larynx cancer relative risk map and how by connecting all the neighbouring areas we get a graph structure graph\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLets focus on one small part of the graph and apply an ICAR model where each region conditionally has a Gaussian distribution with mean equal to the average of the neighbours and a precision proportional to the number of neighbour\n\\[\nx_9\\mid\\mathbf{x}_{-9}\\sim N\\left(\\frac{1}{6}(x_7+x_{11}+x_{12}+x_{13}+x_{14}+x_{15},\\frac{1}{6\\tau}\\right)\n\\]\n\n\n\n\nThe sub graph leads to a precision matrix with 21.6% non-zero elements while the full graph leads to a precision matrix with 0.1% non-zero elements.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe question now is how do we do choose the neighborhood structure?"
  },
  {
    "objectID": "notes/notes_7.html#neighbourhood-structures",
    "href": "notes/notes_7.html#neighbourhood-structures",
    "title": "Modelling Areal Data",
    "section": "\n3.4 Neighbourhood structures",
    "text": "3.4 Neighbourhood structures\nEach of our regions \\(B_i\\) has a set of other nearby which can be considered neighbours. We might expect that areas have more in common with their neighbours. Therefore we can construct dependence structures based on the principle that neighbours are correlated and non-neighbours are uncorrelated. However, we need to come up with a sensible way of defining what a neighbour is in this context.\nThere are many different ways to define a region’s neighbours. The most common ones fall into three main categories - those based on borders, and those based on distance. The methods based on common borders assume that regions which share a border on a map are neighbours (Figure 3). Methods based on distance assume that regions which are within a certain distance of each other are neighbours (Figure 4).\nThe common border approach is simple and easy to implement. However, it treats all borders the same, regardless of length, which can be unrealistic. Also means areas very close together are not neighbours if there is even a small gap between them.\n\n\n\n\n\nFigure 3: Illustration of border-based neighbours.\n\n\nA distance-based approach may initially seem more sensible as a concept, but there are a number of challenges. What distance do you choose? How do you decide that? There is no easy answer. Where do you measure from? The value will be different depending on whether you use nearest border or central point.\n\n\n\n\n\nFigure 4: Illustration of distance-based neighbours.\n\n\nOnce we have identified a set of neighbours using our chosen method, we can use this to account for correlation. We construct a neighbourhood matrix (or proximity matrix), which defines how each of our \\(m\\) regions relate to each other. Let \\(W\\) denote an \\(m \\times m\\) matrix where the \\((i,j)\\)th entry, \\(w_{ij}\\) denotes the proximity between regions \\(B_i\\) and \\(B_j\\). The values of this matrix can be discrete (which regions are neighbours) or continuous (how far apart are the regions).\nBy far the most common approach is to use a binary neighbourhood matrix, \\(W\\), denoted by\n\\[\nw_{ij} = \\begin{cases}\n1 & \\text{if areas } (B_i, B_j) \\text{ are neighbours.}\\\\\n0 & \\text{otherwise.}\n\\end{cases}\n\\]\nBinary matrices are used for their simplicity. Fitting spatial models often requires us to invert \\(W\\), and this is less computationally intensive for sparse matrices.\n\n\n\n\nNow that we have defined a measure of spatial proximity for areal data, we can use this to assess spatial dependence. Essentially, we can now ask the question of how similar a region is to its neighbours. We can consider global correlation, measured across the entire region, and also local correlation which allows for regional variation. In this course, we will focus on modelling global autocorrelation using Moran’s I."
  },
  {
    "objectID": "notes/notes_7.html#morans-i",
    "href": "notes/notes_7.html#morans-i",
    "title": "Modelling Areal Data",
    "section": "\n3.5 Moran’s I",
    "text": "3.5 Moran’s I\nMoran’s \\(I\\) is a measure of global spatial autocorrelation, and can be considered an extension of the Pearson correlation coefficient. For a set of data \\(Z_1, \\ldots, Z_m\\) measured on regions \\(B_1, \\ldots B_m\\), with neighbourhood matrix \\(W\\), we can compute Moran’s I as:\n\\[\nI = \\frac{m}{\\sum_{i=1}^m \\sum_{j=1}^m w_{ij}}\\frac{\\sum_{i=1}^m \\sum_{j=1}^m w_{ij} (Z_i - \\bar{Z})(Z_j - \\bar{Z})}{\\sum_{i=1}^m (Z_i - \\bar{Z})^2}\n\\]\nThis is basically a function of differences in values between neighbouring areas.\nMoran’s \\(I\\) ranges between -1 and 1, and can be interpreted in a similar way to a standard correlation coefficient.\n\n\\(I=1\\) implies that we have perfect spatial correlation.\n\\(I=0\\) implies that we have complete spatial randomness.\n\\(I=-1\\) implies that we have perfect dispersion (negative correlation).\n\nOur observed \\(I\\) is a point estimate, and we may also wish to assess whether it is significantly different from zero. We can test for a statistically significant spatial correlation using a permutation test, with hypotheses:\n\\[\n\\begin{aligned}\nH_0&: \\text{ no spatial association } (I=0)\\\\\nH_1&: \\text{ some spatial association } (I \\neq 0)\n\\end{aligned}\n\\]\nWe carry out \\(k\\) random permutations of our data (reassign each data value to a random location) and compute a Moran’s \\(I\\) for each permutation (\\(I_{perm} = I_1, \\ldots, I_k\\)). We reject the null hypothesis if our observed Moran’s I for our real data (\\(I_{obs}\\)) could not have plausibly come from the distribution of \\(I_{perm}\\).\nThe plots below show perfect dispersion, complete spatial randomness and high spatial correlation respectively.\n\n\n\n\n\n\n\n\n\n\n\nFigure 5: \\(\\hat{I} =1;\\) Std.Dev 0.095; pval \\(&lt;\\) 0.05\n\n\n\n\n\n\n\n\n\nFigure 6: \\(\\hat{I} =0.001;\\) Std.Dev 0.095; pval \\(=\\) 0.865\n\n\n\n\n\n\n\n\n\nFigure 7: \\(\\hat{I} =865;\\) Std.Dev 0.095; pval \\(&lt;\\) 0.05\n\n\n\n\n\n\nIllustration of Moran’s I using simulated data."
  },
  {
    "objectID": "notes/notes_7.html#bayesian-inference",
    "href": "notes/notes_7.html#bayesian-inference",
    "title": "Modelling Areal Data",
    "section": "\n4.1 Bayesian Inference",
    "text": "4.1 Bayesian Inference\nIn the Bayesian framework all unknown quantities in the model are treated as random variables, and the aim is to estimate the joint posterior distribution of the unknown parameters \\(\\theta\\) given the observed data \\(\\mathbf{y}\\).\n\nWe obtain this distribution through Bayes’ theorem:\n\n\\[\n\\pi(\\theta \\mid \\mathbf{y}) = \\frac{\\pi(\\mathbf{y} \\mid \\theta)\\pi(\\theta)}{\\pi(\\mathbf{y})}\n\\]\n\n\nComponents:\n\n\\(\\pi(\\mathbf{y} \\mid \\theta)\\) is the likelihood of \\(\\mathbf{y}\\) given parameters \\(\\theta\\)\n\\(\\pi(\\theta)\\) is the prior distribution of the parameters\n\n\\(\\pi(\\mathbf{y})\\) is the marginal likelihood (normalizing constant):\n\\[\n    \\pi(\\mathbf{y}) = \\int_\\Theta \\pi(\\mathbf{y} \\mid \\theta) \\pi(\\theta) d\\theta\n\\]\n\nmarginalizing \\(\\pi(\\mathbf{y})\\) means integrating out all the uncertainty on \\(\\theta\\)\n\n\n\nWhen the posterior distribution lacks a closed-form solution, computational methods must be used to approximate it. To efficiently estimate spatially structured spatial models, particularly those incorporating Conditional Autoregressive (CAR) priors, the Integrated Nested Laplace Approximation (INLA) framework (Van Niekerk et al., 2023) provides a powerful alternative to traditional Markov chain Monte Carlo (MCMC) methods."
  },
  {
    "objectID": "notes/notes_7.html#inla",
    "href": "notes/notes_7.html#inla",
    "title": "Modelling Areal Data",
    "section": "\n4.2 INLA",
    "text": "4.2 INLA\nWe can use INLA to approximate the joint posterior of our parameters of interest. INLA is a fast and accurate method to do Bayesian inference with latent Gaussian models and can be used with Bayesian hierarchical models where we model in different stages or levels:\n\nStage 1: What is the distribution of the responses?\nStage 2: What is the distribution of the underlying latent components?\nStage 3: What are our prior beliefs about the hyperparameters?\n\nStage 1: Data Generating Process\nOn Stage 1, we look at how is our data (\\(\\mathbf{y}\\)) generated from the underlying components \\(\\mathbf{x}\\) and hyperparameters \\(\\theta\\) in the model\nResponse types:\n\nGaussian (temperature, rainfall, weight)\nCount data (disease cases, species counts)\nPoint patterns (tree locations)\nBinary data (yes/no responses)\nSurvival data (time to event)\n\nIt is also important how data are collected!\nAll of this information is placed into our likelihood \\(\\pi(\\mathbf{y}|\\theta)\\). We assume that given the underlying components (\\(\\mathbf{x}\\)) and hyperparameter (\\(\\theta\\)), the data are independent on each other\n\\[\n\\pi(y|x,\\theta) = \\prod_{i\\in\\mathcal{I}}\\pi(y_i|\\mathbf{x}_{\\mathcal{I}},\\theta)\n\\]\nThis implies that all the dependence structure in the data is explained on Stage 2.\nStage 2: Dependence Structure\nThe underlying unobserved components \\(\\mathbf{x}\\) are called latent components and can be:\n\nFixed effects of covariates\nUnstructured random effects (individual effects, group effects)\nStructured random effects (AR(1), regional effects, \\(\\ldots\\) )\n\nThese are linked to the responses in the likelihood through linear predictors.\nAn important feature of INLA is that it considers the The latent field \\(\\mathbf{x}\\) ito be a GMRF\n\\[\n     \\pi(\\mathbf{x}|\\theta)\\propto\\exp\\left\\{-\\frac{1}{2}\\mathbf{x}^T\\mathbf{Q}\\mathbf{x}\\right\\}\n\\]\nThsu, the precision matrix \\(\\mathbf{Q}\\) is sparse. We can also factorize the likelihood as: \\(\\pi(\\mathbf{y}|\\mathbf{x},\\theta) = \\prod_i\\pi(y_i|\\eta_i,\\theta)\\). This means that our Data are conditional independent given \\(\\mathbf{u}\\) and \\(\\theta\\). Also, that each data point depends on only 1 element of the latent field: the predictor \\(\\eta_i\\), where \\(\\eta\\) is a linear combination of other elements of \\(\\mathbf{x}\\): \\(\\eta = \\mathbf{A}^T\\mathbf{x}\\).\nStage 3: The hyperparameters\nLastly, we assume that the vector of hyperparameters \\(\\theta\\) is low dimensional. See how both, the likelihood and the latent model, typically have hyperparameters that control their behavior. The hyperparameters \\(\\theta\\) can include:\nExamples likelihood:\n\nVariance of observation noise\nDispersion parameter in the negative binomial model -\nProbability of a zero (zero-inflated models)\n\nExamples latent model:\n\nVariance of unstructured effects\nCorrelation of multivariate effects\nRange and variance of spatial effects\nAutocorrelation parameter \\(\\rho\\)"
  },
  {
    "objectID": "notes/notes_7.html#lgms-and-the-inla-strategy-in-short",
    "href": "notes/notes_7.html#lgms-and-the-inla-strategy-in-short",
    "title": "Modelling Areal Data",
    "section": "\n4.3 LGMs and the INLA strategy in short",
    "text": "4.3 LGMs and the INLA strategy in short\nThese three stages constitute the basis of latent Gaussian models (LGMs).\n\n\nA LGM consists of three elements:\n\na likelihood model\na latent Gaussian field (i.e., the latent components of our model)\na vector of non-Gaussian hyperparameters\n\n\nThe characteristic property is that the latent part of the hierarchical model is Gaussian, \\(\\mathbf{x}|\\theta \\sim N(0,Q^{-1})\\), where the expected value is \\(0\\) and the precision matrix is \\(Q\\).\n\nAssumption\nWe are mainly interested in posterior marginals \\(p(x_i|\\mathbf{y})\\) and \\(p(\\theta_j|\\mathbf{y})\\)\nStrategy\n\nWe use numerical integration in a smart way\nWe approximate what we do not know analytically exploiting the Gaussian structure of \\(\\mathbf{x}|y\\)."
  },
  {
    "objectID": "notes/notes_7.html#standardized-mortality-ratios-and-spatial-correlation",
    "href": "notes/notes_7.html#standardized-mortality-ratios-and-spatial-correlation",
    "title": "Modelling Areal Data",
    "section": "\n5.1 Standardized Mortality Ratios and spatial correlation",
    "text": "5.1 Standardized Mortality Ratios and spatial correlation\nIn epidemiology, disease risk is assessed using Standardized Mortality Ratios (SMR):\n\\[ SMR_i = \\dfrac{Y_i}{E_i} \\]\n\nA value \\(SMR &gt; 1\\) indicates a high risk area.\nA value \\(SMR&lt;1\\) suggests a low risk area.\nSIRs may be misleading in counties with small populations.\nmodel-based approaches enable to incorporate covariates and borrow information from neighboring counties to improve local estimates,\n\nThe data is available on the SpatialEpi R package and can be loaded and visualized as follows:\n\nCode# Library where the data is stored\nlibrary(SpatialEpi)\n\n# Libraries for Data manipulation\nlibrary(tidyr)\nlibrary(dplyr)\n\n# Libraries for producing maps\nlibrary(ggplot2)\nlibrary(sf)\nlibrary(mapview)\n\n# Load the Data\ndata(scotland_sf)\n\n# Compute the SMR and add a region index (for later modelling)\nscotland_sf &lt;- scotland_sf %&gt;% mutate(\n  SMR = cases/expected,\n  region_id = 1:nrow(scotland_sf))\n\n# Visualize the regions colored by the SMR\nggplot()+geom_sf(data=scotland_sf,aes(fill=SMR))+scale_fill_scico(direction = -1)\n\n\n\n\n\n\n\n\n\n\nDo we need to model spatial dependence?\nRecall that, Moran’s \\(I\\) ranges between -1 and 1, and can be interpreted in a similar way to a standard correlation coefficient.\n\n\\(I=1\\) implies that we have perfect spatial correlation.\n\\(I=0\\) implies that we have complete spatial randomness.\n\\(I=-1\\) implies that we have perfect dispersion (negative correlation).\n\nOur observed \\(I\\) is a point estimate, and we may also wish to assess whether it is significantly different from zero. We can test for a statistically significant spatial correlation using a permutation test, with hypotheses:\n\\[\n\\begin{aligned}\nH_0&: \\text{ negative or no spatial association } (I \\leq 0)\\\\\nH_1&: \\text{ positive spatial association } (I &gt; 0)\n\\end{aligned}\n\\]\nWe can use moran.test() to test this hypothesis by setting alternative = \"greater\". To do so, we need to supply list containing the neighbors via the nb2listw() function from the spdep package. Lets assess now the spatial autocorrelation of the SMR in 2011:\n\nCode# list of neighbors based on areas with contiguous boundaries\nW.nb &lt;- poly2nb(scotland_sf,queen = TRUE)\n\n# Creat proximity matrix\nR &lt;- nb2listw(W.nb, style = \"B\", zero.policy = TRUE)\n\n# Global Moran's I\ngmoran &lt;- moran.test(scotland_sf$SMR, R,\n\n                     alternative = \"greater\")\n\ngmoran\n\n\n    Moran I test under randomisation\n\ndata:  scotland_sf$SMR  \nweights: R  \nn reduced by no-neighbour observations  \n\nMoran I statistic standard deviate = 5.3678, p-value = 3.984e-08\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.437119239      -0.019230769       0.007227652 \n\n\nSince have set the alternative hypothesis to be \\(I &gt; 0\\) and have a p-value \\(&lt;0.05\\), we then reject the null hypothesis and conclude there is evidence for positive spatial autocorrelation."
  },
  {
    "objectID": "notes/notes_7.html#bym-model",
    "href": "notes/notes_7.html#bym-model",
    "title": "Modelling Areal Data",
    "section": "\n5.2 BYM model",
    "text": "5.2 BYM model\nThe ICAR model introduced earlier (#sec-besag), accounts only for spatially structured variability and does not include a limiting case where no spatial structure is present. Thus, we typically add an unstructured random effect \\(z_i\\mid \\tau_z \\sim N(0,\\tau_{z}^{-1})\\)\n\nThe resulting model \\(v_i = u_i + z_i\\) is known as the Besag-York-Mollié model (BYM)\n\nThe structured spatial effect is controlled by \\(\\tau_u\\) which control the degree of smoothing:\n\nHigher \\(\\tau_u\\) values lead to stronger smoothing (less spatial variability).\nLower \\(\\tau_u\\) values allow for greater local variation.\n\n\n\nThis is an example of a LGM expressed as follows:\n\n\n\nStage 1: We assume the responses are Poisson distributed: \\[\n\\begin{aligned}y_i|\\eta_i & \\sim \\text{Poisson}(E_i\\lambda_i)\\\\\\text{log}(\\lambda_i) = \\eta_i & = \\beta_0 + \\beta_1 \\mathrm{AFF} + u_i + z_i\\end{aligned}\n\\]\n\n\n\n\n\n\nStage 2: \\(\\eta_i\\) is a linear function of four components: an intercept, proportion of population engaged in agriculture, fishing, or forestry (AFF) effect , a spatially structured effect \\(u\\) and an unstructured iid random effect \\(z\\): \\[\\eta_i = \\beta_0 + \\beta_1\\mathrm{AFF}_i + u_i + z_i\\]\n\n\n\n\n\n\nStage 3: \\(\\{\\tau_{z},\\tau_u\\}\\): Precision parameters for the random effects\n\n\nThe latent field is \\(\\mathbf{x}= (\\beta_0, \\beta_1, u_1, u_2,\\ldots, u_n,z_1,...)\\), the hyperparameters are \\(\\boldsymbol{\\theta} = (\\tau_u,\\tau_z)\\), and must be given a prior.\nThe Model structure\n\\[\n\\begin{aligned}\ny_i|\\eta_t & \\sim \\text{Poisson}(E_i\\lambda_i)\\\\\n\\text{log}(\\lambda_i) = \\eta_i & = \\beta_0 + \\beta_1 \\mathrm{AFF}_i + u_i + v_i\n\\end{aligned}\n\\]\n\n\\(\\beta_0\\) is the intercept that represents the overall risk\n\\(\\beta_1\\) is the coefficient of the AFF covariate\n\\(u_i\\) is a spatial structured component\n\\(v_i\\) is a spatial unstructured component\n\nThe neighbourhood structure\nHere we generate the adjacency matrix to represent the structure of the neighbours using the poly2nb function from the spdep R package.\n\nCodelibrary(spdep)\n\nW.nb &lt;- poly2nb(scotland_sf,queen = TRUE)\n\nplot(st_geometry(scotland_sf), border = \"lightgray\")\n\nplot.nb(W.nb, st_geometry(scotland_sf), add = TRUE)\n\n\n\n\n\n\nThen we define the proximity matrix using the nb2mat function and define the precision matrix Q according to Equation 1:\nRecall The precision matrix \\(Q\\) depends on the neighboring structure and \\(\\tau_u\\) (which will be estimated). Now, to make the precision parameters of models with different intrinsic Gaussian random field comparable we add a sum-to-zero constrain \\(\\sum_i^n u_i = 0\\) (see scale.model = TRUE in the code below).\nWe can fit this model with inlabru, which is a wrapper around INLA (we will cover this more in detail on the labs). We just need to define\n\nThe model components (cmp)\nThe formula for the linear predictor\nThe observational model (via the bru_obs function)\nfit the model using the bru function\n\n\nCodecmp = ~ Intercept(1) + beta_1(AFF, model = \"linear\") +\n\n  u_i(region_id, model = \"besag\", graph = Q,scale.model = TRUE) +\n\n  z_i(region_id , model = \"iid\")\n\nformula = cases ~ Intercept + beta_1 + u_i + z_i\n\nlik = bru_obs(formula = formula,\n\n              family = \"poisson\",\n\n              E = expected,\n\n              data = scotland_sf)\n\nfit = bru(cmp, lik)\n\n\nThe posterior summaries for fixed effect and hyperparameters can be obtain by typing fit$summary.fixed and fit$summary.hyperpar. The following table summarises the results obtained from the model:\n\n\n\n\n\n\n\nINLA Model Results\n\n\nPosterior summaries of fixed effects and hyperparameters\n\n\nParameter\nMean\n2.5% Quantile\n97.5% Quantile\n\n\n\n\nIntercept\n−0.306\n−0.538\n−0.069\n\n\nbeta_1\n4.317\n1.758\n6.761\n\n\nPrecision for u_i\n4.139\n2.023\n7.597\n\n\nPrecision for z_i\n22,037.802\n1,474.292\n86,083.027\n\n\n\n\n\n\nThe model revealed a strong positive association between sun exposure and lip cancer risk. The relative risks \\(\\lambda_i\\) can be obtained as follows:\n\\[\n\\begin{aligned}\n\\lambda_i &= \\exp\\left(\\eta_i\\right) \\\\\n&= \\exp\\Bigl(\\beta_0 + \\beta_1 \\times \\mathrm{AFF}_i + u_i + z_i\\Bigr)\n\\end{aligned}\n\\]\nWe can then compare a map of the RR of each area against the SMR we computed previously:\n\n\n\n\n\n\n\n\nWe see RR values are shrunk towards 1 compared to the SMR values because each area’s spatial effect is pulled toward its neighbors’ average."
  },
  {
    "objectID": "notes/notes_7.html#bym2-model-pc-priors",
    "href": "notes/notes_7.html#bym2-model-pc-priors",
    "title": "Modelling Areal Data",
    "section": "\n5.3 BYM2 Model & PC priors",
    "text": "5.3 BYM2 Model & PC priors\nIn the original BYM, the spatially structured component must be scaled so that \\(\\tau_u\\) produces consistent smoothness across different neighborhood structures.\nSimpson et al. (2017) proposed a new parametrization of the BYM model that improves parameter interpretability. The BYM2 model proposed by Simpson et al. (2017) uses a scaled spatially structured effect \\(u^*\\) and unstructured random effect \\(v^*\\):\n\\[\n\\mathbf{b} = \\dfrac{1}{\\sqrt{\\tau_b}} \\left(\\sqrt{1-\\phi}v^*+\\sqrt{\\phi}u^*\\right).\n\\]\nHere, the precision \\(\\tau_b&gt;0\\) controls the marginal variance contribution of the weighted sum \\(u^*\\) and \\(v^*\\). The mixing parameter \\(0 \\leq \\phi \\leq 1\\) measures the proportion of the marginal variance explained by the structured effect \\(u^*\\). Thus, if \\(\\phi =1\\) the model captures only spatially structured variability, whereas when \\(\\phi = 0\\) , it accounts solely for unstructured spatial noise.\nThis parametrization of the BYM2 model also allows the specification of Penalized Complexity (PC) priors, which provide an intuitive way to control the amount of spatial smoothing and avoid overfitting. The key idea behind PC priors is to shrink the model towards a simpler baseline (e.g., a non-spatial model) unless the data provide strong evidence for a more complex structure. To define the prior for the marginal precision \\(\\tau_b\\) and the mixing parameter \\(\\phi\\), we use the probability statements \\(P(1/\\sqrt{\\tau_b} &gt; U) = \\alpha\\) and \\(P(\\phi &lt; U) = \\alpha\\) respectively.\nIf we consider a marginal standard deviation of approximately 0.5 is a reasonable upper bound, we can use the rule of thumb described by Simpson et al. (2017) and set \\(U = 0.5/0.31\\) and \\(\\alpha = 0.01\\), this is equivalent to the probability statement \\(P((1/\\sqrt{\\tau_b}) &gt; 0.5/0.31)) = 0.01\\). Then, the PC prior for the mixing parameter can be specified as \\(P (\\phi &lt; 0.5) = 2/3\\). The later statement assumes that the unstructured random effect accounts for more of the variability than the spatially structured effect."
  },
  {
    "objectID": "notes/notes_9.html",
    "href": "notes/notes_9.html",
    "title": "Modelling Geostatistical Data",
    "section": "",
    "text": "In ecology many of the ecological processes of interest can be view as an aggregation of a spatial point process. For example, Figure 1 shows how species occupancy (presence or absence of a species in a given area), and abundance (the number of individual of a species that occur in an area) are related quantities to a spatial point process.\n\n\n\n\n\nFigure 1: Illustration of the unidirectional information exchange structure of different spatial processes in ecology.\n\n\nMany contemporary data sources (e.g., Citizen science, GPS tracking, camera traps, bio-logging devices, etc) collect georeferenced information about the location where species occur. This point-based information provides valuable insights into species distributions and ecosystem dynamics.\nA key focus of spatial point pattern analysis is to quantify spatial arrangements, whether aggregated, uniform, or random. Recognizing these patterns is fundamental to interpreting ecological interactions, such as territoriality, competition, and social behavior. Moreover, spatial point patterns can reveal mechanisms of species coexistence and the factors influencing biodiversity.\nFrom a conservation perspective, the clustering of habitat patches—conceptualized as points across a landscape—can have profound effects on extinction risk and population persistence. Understanding the spatial scale of these patterns helps ecologists assess species invasion dynamics and long-term population viability.\nIn the following sections, we introduce spatial point pattern analysis and its application to ecological and conservation-related questions, emphasizing how species dispersion patterns shape and inform biodiversity management.\nA spatial point process is a set of locations…presumed to have been generated by some form of stochastic (random) mechanism”.\nIn other words, the point process is a random variable operating in continuous space, and we observe realisations of this variable as point patterns across space (and/or time). Spatial point pattern analysis focuses on examining patterns of points to establish whether there are regularities in the process they represent.\nConsider a fixed geographical region \\(A\\). The set of locations at which events occur are denoted \\(\\mathbf{s} = s_1,\\ldots,s_n\\). We let \\(N(A)\\) be the random variable which represents the number of events in region \\(A\\).\nOur primary interest is in measuring where events occur, so the locations are our data. We typically assume that a spatial point pattern is generated by an unique point process over the whole study area. This means that the delimitation of the study area will affect the observed point patters.\nThe observed distribution of points can be described based on the intensity of points within a delimited region. Thereof, we want to know whether there is any particular spatial pattern associated with the process.\nThere are three broad types of spatial structures which can be explored, each representing a different type of spatial dependence.\n\nComplete spatial randomness (CSR) - events occur at random, and independently of each other.\nClustered process - events occur close to existing events.\nRegular process - events occur away from existing events.\n\n\n\nExamples of spatial point patterns\n\nUnfortunately, it still isn’t straightforward to determine by eye which of these categories a set of points falls into. So we need a more systematic approach.\n\nWe can define the (first order) intensity of a point process as the expected number of events per unit area. This can also be thought of as a measure of the density of our points. In some cases, the intensity will be constant over space (homogeneous), while in other cases it can vary by location (inhomogeneous or heterogenous). If our intensity is homogeneous, we can define it as\n\\[\n\\lambda(s) = \\dfrac{N(A)}{|A|} =  \\lambda\n\\] We can use the concept of intensity to help us define complete spatial randomness (CSR).\n\n\n\n\n\n\nDef.\n\n\n\nFor any spatial region A, CSR requires that:\n\nUniformity and Independent scattering : Given the number of events \\(N(A) = n\\) in a region, the \\(n\\) events are independently and uniformly distributed over space (i.e., each event has an equal probability of occurring anywhere in the study area).\nPoisson distribution of point counts: The number of points in any set \\(A_i\\) follows a Poisson distribution with mean \\(\\lambda|A_i|\\), that is \\[N(A_i) \\sim \\text{Poisson}(\\lambda|A_i|).\\]\nIf these conditions are satisfied, we can describe our process as a homogeneous Poisson process.\n\n\n\nThe likelihood of a point pattern \\(\\mathbf{y} = \\left[ \\mathbf{s}_1, \\ldots, \\mathbf{s}_n \\right]^\\intercal\\) distributed as a HPP with intensity \\(\\lambda\\) and observation window \\(\\Omega\\) is\n\\[\n    p(\\mathbf{y} | \\lambda) \\propto \\lambda^n e^{ \\left( - |\\Omega| \\lambda \\right)} ,\n  \\]\n\n\n\\(|\\Omega|\\) is the size of the observation window.\n\n-   $\\lambda$ is the expected number of points per unit area.\n\n-   $|\\Omega|\\lambda$ the total expected number of points in the observation window.\nA key property of a Poisson process is that the number of points within any subset \\(A_i\\) of region \\(A\\) is Poisson distributed with constant rate \\(|A_i|\\lambda\\).\nWhile CSR rarely occurs in nature, it is the simplest null model that we use can to a determine whether an homogeneous Poisson process is appropriate for our data or not. We do so by contrasting the observed point pattern with a point pattern generated from the CRS model. One of the most common approaches for identifying point patterns is the Ripley’s K function.\n\nRipley’s \\(K\\) calculates the degree of spatial aggregation of points within a circle (buffer) of radius \\(r\\) and contrasts the observed pattern to that expected under CSR. Ripley’s K is defined as:\n\\[\nK(r) = \\dfrac{E[N(s_0,r)]}{\\lambda},\n\\]\nHere, \\(N(s_0,r)\\) denotes the number of events that occur within distance \\(r\\) of an event \\(s_0\\); clearly, as \\(r\\) increases, so too will \\(K(r)\\).\n\n\nBuffer with radius \\(r\\) centred at the event \\(s_0\\).\n\nRipley’s \\(K\\) can be estimated as:\n\\[\n\\hat{K}(r) = \\frac{1}{n}\\sum_{i=1}^n\\sum_{i\\neq j}I(d_{ij}&lt;r) \\times \\lambda^{-1}\n\\]\nThis first part of the equation correspond to the expected number of events that occur within a buffer of radius \\(r\\) , i.e. for each point \\(s_i \\in i,\\ldots,n\\) we count the number of points other than \\(i\\) that fall within the buffer of radius \\(r\\) and then we sum up the number of neighbors for all points and weight it by the total number of points \\(n\\) in the whole region \\(A\\). The second part of the equation correspond to the density of events estimated as \\(\\lambda = n/|A|\\).\nThe idea is to compare \\(\\hat{K}(r)\\) against the expected \\(K(r)\\) under CSR. If we assume an homogeneous Poisson process and given the area is that of a circle, we would expect that under CSR the expected \\(K(r)\\) is:\n\\[\nK_{CRS}(r) = \\dfrac{\\lambda \\pi r^2}{\\lambda}=\\pi\\times r^2\n\\]\nThat is, under CSR we would expect that the \\(K\\) function is equal to the area of the circle with radius \\(r\\). Then we compare \\(\\hat{K}(r)\\) and \\(K_{CSR}(r)\\):\n\nIf \\(\\hat{K}(r) &gt; K_{CSR}(r)\\) it means that more points are found within a radius \\(r\\) than what would be expected under complete randomness, suggesting a clustering pattern. \\(\\hat{K}(r)\\) will be relatively large for small values of \\(r\\), since events are likely to be surrounded by further members of the same cluster. E.g., tree seedlings often cluster near parent trees due to seed dispersal limitations.\nIf \\(\\hat{K}(r) &lt; K_{CSR}(r)\\), it indicates that the pattern is more regular since we observe fewer neighboring points within a distance \\(r\\) than expected under CSR. For regular patterns, \\(K(r)\\) will be relatively small for small values of \\(r\\), since there is likely to be more empty space around events. E.g., territorial animals (e.g., nesting birds) often exhibit regular spacing due to competition for space.\n\nWhen working with real data, some natural variation is to be expected even when CSR holds. We therefore need an approach which accounts for this when assessing for CSR to determine whether or not the observed pattern is non-random.\nWe can estimate \\(\\hat{K}(r)\\) across a set of distances \\(r\\) for our set of observed events. Then, \\(\\hat{K}(r)\\) can be compared to the theoretical function for CSR, \\(K(r) = \\pi \\times r^2\\). If the two functions are similar, then CSR is reasonable. See the example below,\n\n\nSimulated regular point process data and associate Ripley’s K function. Red line indicates theoretical \\(K_{CSR}(r)\\) and back line \\(\\hat{K}(r)\\).\n\nIn this example we can notice that for most distances \\(\\hat{K}(r) &lt; K_{CSR}(r)\\) . However we cannot be completely sure if the observed point pattern is best described by a CSR process or if the pattern is more regular. Thus, we might wish to be more precise in our comparison of the two lines.\nWe can calculate what is known as a simulation envelope which is produced by simulating multiple sets of data under CSR. These are not theoretically the same as confidence intervals, but can be described as significance bands. If our observed line falls outside of the envelope, this implies that CSR is not reasonable. Constructing a simulation envelope in our example produces:\n\n\nSimulated regular point process data and associate Ripley’s \\(K\\) function. Red line dotted indicates theoretical \\(K_{CSR}(r)\\) with grey areas denoting the simulation envelope and black solid line \\(\\hat{K}(r)\\).\n\nIn this example the black line lies outside the envelope between \\(r = 0\\) and \\(r = 0.15\\) therefore we still conclude that the data are regularly spaced. Figure 2 shows examples of simulated cluster and regular patterns with their associated Ripley’s \\(K\\) function.\n\n\n\n\n\nFigure 2: Simulated random (A) and clusterred (B) point processes data and associated Ripley’s \\(K\\) function. Red line dotted indicates theoretical \\(K_{CSR}(r)\\) with grey areas denoting the simulation envelope and black solid line \\(\\hat{K}(r)\\).\n\n\nWhile Ripley’s \\(K\\) function is widely used in environmental and ecological studies it has some caveats. For example, \\(K(s)\\) is a cumulative function, where all points less than \\(r\\) are also used. So, if there is a strong clustering patter at 5m but no pattern at larger distances, then Ripley’s \\(K\\) could still indicate a strong clustering at larger scales due to the data \\(&lt;\\) 5m still being used.\nAnother issues with this method is its sensitivity to edge effects. This occurs because points near the boundaries of the study area have fewer neighboring points within distance \\(r\\), leading to underestimation of $K(r)$ (when considering a radius \\(r\\) from a point near a boundary, the number of observed points is likely lower than the true number of points if points could occur outside of study area).\nThe \\(K(r)\\) function can be adjusted for edge effects by including some weights \\(w\\). There are several weights to specify these weights. For example, if the \\(i\\)th buffer lies completely within the study area then it receives a weight of \\(w= 1\\) but if it lies outside it receives a weight inverse of the fraction of circumference lying inside the area, (e.g., if half of a point’s search area lies within the study area then the point will receive a weight \\(w=2\\), meaning it contributes twice as much to compensate for the missing area).\nLastly, the above analyses assume that the point process is stationary (homogeneous over space) and isotropic (point process does not change with direction). These assumption rarely hold true in real-data, thus, inhomogeneous point process (IPP) models are often used for inference prediction and mapping spatial patterns."
  },
  {
    "objectID": "notes/notes_9.html#summaries-of-point-processes",
    "href": "notes/notes_9.html#summaries-of-point-processes",
    "title": "Modelling Geostatistical Data",
    "section": "",
    "text": "We can define the (first order) intensity of a point process as the expected number of events per unit area. This can also be thought of as a measure of the density of our points. In some cases, the intensity will be constant over space (homogeneous), while in other cases it can vary by location (inhomogeneous or heterogenous). If our intensity is homogeneous, we can define it as\n\\[\n\\lambda(s) = \\dfrac{N(A)}{|A|} =  \\lambda\n\\] We can use the concept of intensity to help us define complete spatial randomness (CSR).\n\n\n\n\n\n\nDef.\n\n\n\nFor any spatial region A, CSR requires that:\n\nUniformity and Independent scattering : Given the number of events \\(N(A) = n\\) in a region, the \\(n\\) events are independently and uniformly distributed over space (i.e., each event has an equal probability of occurring anywhere in the study area).\nPoisson distribution of point counts: The number of points in any set \\(A_i\\) follows a Poisson distribution with mean \\(\\lambda|A_i|\\), that is \\[N(A_i) \\sim \\text{Poisson}(\\lambda|A_i|).\\]\nIf these conditions are satisfied, we can describe our process as a homogeneous Poisson process.\n\n\n\nThe likelihood of a point pattern \\(\\mathbf{y} = \\left[ \\mathbf{s}_1, \\ldots, \\mathbf{s}_n \\right]^\\intercal\\) distributed as a HPP with intensity \\(\\lambda\\) and observation window \\(\\Omega\\) is\n\\[\n    p(\\mathbf{y} | \\lambda) \\propto \\lambda^n e^{ \\left( - |\\Omega| \\lambda \\right)} ,\n  \\]\n\n\n\\(|\\Omega|\\) is the size of the observation window.\n\n-   $\\lambda$ is the expected number of points per unit area.\n\n-   $|\\Omega|\\lambda$ the total expected number of points in the observation window.\nA key property of a Poisson process is that the number of points within any subset \\(A_i\\) of region \\(A\\) is Poisson distributed with constant rate \\(|A_i|\\lambda\\).\nWhile CSR rarely occurs in nature, it is the simplest null model that we use can to a determine whether an homogeneous Poisson process is appropriate for our data or not. We do so by contrasting the observed point pattern with a point pattern generated from the CRS model. One of the most common approaches for identifying point patterns is the Ripley’s K function."
  },
  {
    "objectID": "notes/notes_9.html#ripleys-k-function",
    "href": "notes/notes_9.html#ripleys-k-function",
    "title": "Modelling Geostatistical Data",
    "section": "",
    "text": "Ripley’s \\(K\\) calculates the degree of spatial aggregation of points within a circle (buffer) of radius \\(r\\) and contrasts the observed pattern to that expected under CSR. Ripley’s K is defined as:\n\\[\nK(r) = \\dfrac{E[N(s_0,r)]}{\\lambda},\n\\]\nHere, \\(N(s_0,r)\\) denotes the number of events that occur within distance \\(r\\) of an event \\(s_0\\); clearly, as \\(r\\) increases, so too will \\(K(r)\\).\n\n\nBuffer with radius \\(r\\) centred at the event \\(s_0\\).\n\nRipley’s \\(K\\) can be estimated as:\n\\[\n\\hat{K}(r) = \\frac{1}{n}\\sum_{i=1}^n\\sum_{i\\neq j}I(d_{ij}&lt;r) \\times \\lambda^{-1}\n\\]\nThis first part of the equation correspond to the expected number of events that occur within a buffer of radius \\(r\\) , i.e. for each point \\(s_i \\in i,\\ldots,n\\) we count the number of points other than \\(i\\) that fall within the buffer of radius \\(r\\) and then we sum up the number of neighbors for all points and weight it by the total number of points \\(n\\) in the whole region \\(A\\). The second part of the equation correspond to the density of events estimated as \\(\\lambda = n/|A|\\).\nThe idea is to compare \\(\\hat{K}(r)\\) against the expected \\(K(r)\\) under CSR. If we assume an homogeneous Poisson process and given the area is that of a circle, we would expect that under CSR the expected \\(K(r)\\) is:\n\\[\nK_{CRS}(r) = \\dfrac{\\lambda \\pi r^2}{\\lambda}=\\pi\\times r^2\n\\]\nThat is, under CSR we would expect that the \\(K\\) function is equal to the area of the circle with radius \\(r\\). Then we compare \\(\\hat{K}(r)\\) and \\(K_{CSR}(r)\\):\n\nIf \\(\\hat{K}(r) &gt; K_{CSR}(r)\\) it means that more points are found within a radius \\(r\\) than what would be expected under complete randomness, suggesting a clustering pattern. \\(\\hat{K}(r)\\) will be relatively large for small values of \\(r\\), since events are likely to be surrounded by further members of the same cluster. E.g., tree seedlings often cluster near parent trees due to seed dispersal limitations.\nIf \\(\\hat{K}(r) &lt; K_{CSR}(r)\\), it indicates that the pattern is more regular since we observe fewer neighboring points within a distance \\(r\\) than expected under CSR. For regular patterns, \\(K(r)\\) will be relatively small for small values of \\(r\\), since there is likely to be more empty space around events. E.g., territorial animals (e.g., nesting birds) often exhibit regular spacing due to competition for space.\n\nWhen working with real data, some natural variation is to be expected even when CSR holds. We therefore need an approach which accounts for this when assessing for CSR to determine whether or not the observed pattern is non-random.\nWe can estimate \\(\\hat{K}(r)\\) across a set of distances \\(r\\) for our set of observed events. Then, \\(\\hat{K}(r)\\) can be compared to the theoretical function for CSR, \\(K(r) = \\pi \\times r^2\\). If the two functions are similar, then CSR is reasonable. See the example below,\n\n\nSimulated regular point process data and associate Ripley’s K function. Red line indicates theoretical \\(K_{CSR}(r)\\) and back line \\(\\hat{K}(r)\\).\n\nIn this example we can notice that for most distances \\(\\hat{K}(r) &lt; K_{CSR}(r)\\) . However we cannot be completely sure if the observed point pattern is best described by a CSR process or if the pattern is more regular. Thus, we might wish to be more precise in our comparison of the two lines.\nWe can calculate what is known as a simulation envelope which is produced by simulating multiple sets of data under CSR. These are not theoretically the same as confidence intervals, but can be described as significance bands. If our observed line falls outside of the envelope, this implies that CSR is not reasonable. Constructing a simulation envelope in our example produces:\n\n\nSimulated regular point process data and associate Ripley’s \\(K\\) function. Red line dotted indicates theoretical \\(K_{CSR}(r)\\) with grey areas denoting the simulation envelope and black solid line \\(\\hat{K}(r)\\).\n\nIn this example the black line lies outside the envelope between \\(r = 0\\) and \\(r = 0.15\\) therefore we still conclude that the data are regularly spaced. Figure 2 shows examples of simulated cluster and regular patterns with their associated Ripley’s \\(K\\) function.\n\n\n\n\n\nFigure 2: Simulated random (A) and clusterred (B) point processes data and associated Ripley’s \\(K\\) function. Red line dotted indicates theoretical \\(K_{CSR}(r)\\) with grey areas denoting the simulation envelope and black solid line \\(\\hat{K}(r)\\).\n\n\nWhile Ripley’s \\(K\\) function is widely used in environmental and ecological studies it has some caveats. For example, \\(K(s)\\) is a cumulative function, where all points less than \\(r\\) are also used. So, if there is a strong clustering patter at 5m but no pattern at larger distances, then Ripley’s \\(K\\) could still indicate a strong clustering at larger scales due to the data \\(&lt;\\) 5m still being used.\nAnother issues with this method is its sensitivity to edge effects. This occurs because points near the boundaries of the study area have fewer neighboring points within distance \\(r\\), leading to underestimation of $K(r)$ (when considering a radius \\(r\\) from a point near a boundary, the number of observed points is likely lower than the true number of points if points could occur outside of study area).\nThe \\(K(r)\\) function can be adjusted for edge effects by including some weights \\(w\\). There are several weights to specify these weights. For example, if the \\(i\\)th buffer lies completely within the study area then it receives a weight of \\(w= 1\\) but if it lies outside it receives a weight inverse of the fraction of circumference lying inside the area, (e.g., if half of a point’s search area lies within the study area then the point will receive a weight \\(w=2\\), meaning it contributes twice as much to compensate for the missing area).\nLastly, the above analyses assume that the point process is stationary (homogeneous over space) and isotropic (point process does not change with direction). These assumption rarely hold true in real-data, thus, inhomogeneous point process (IPP) models are often used for inference prediction and mapping spatial patterns."
  },
  {
    "objectID": "notes/notes_9.html#fitting-an-ipp",
    "href": "notes/notes_9.html#fitting-an-ipp",
    "title": "Modelling Geostatistical Data",
    "section": "\n2.1 Fitting an IPP",
    "text": "2.1 Fitting an IPP\nThis integral is approximated as \\(\\int_\\Omega \\lambda(\\mathbf{s}) \\mathrm{d}\\mathbf{s} \\approx \\sum_{j=1}^J w_j \\lambda(\\mathbf{s}_j)\\).\n\n\\(w_j\\) are the integration weights\n\\(\\mathbf{s}_j\\) are the quadrature locations.\n\nThis serves two purposes:\n\nApproximating the integral\nre-writing the inhomogeneous Poisson process likelihood as a regular Poisson likelihood.\n\nThe idea behind this trick is to rewrite the approximate likelihood by introducing a dummy vector \\(\\mathbf{z}\\) and an integration weights vector \\(\\mathbf{w}\\) of length \\(J + n\\) such that\n\n\\(\\mathbf{z} = \\left[\\underbrace{0_1, \\ldots,0_J}_\\text{quadrature locations}, \\underbrace{1_1, \\ldots ,1_n}_{\\text{data points}} \\right]^\\intercal\\)\n\\(\\mathbf{w} = \\left[ \\underbrace{w_1, \\ldots, w_J}_\\text{quadrature locations}, \\underbrace{0_1, \\ldots, 0_n}_\\text{data points} \\right]^\\intercal\\)\n\nThen the approximate likelihood can be written as\n\\[\n\\begin{aligned}\np(\\mathbf{z} | \\lambda) &\\propto \\prod_{i=1}^{J + n} \\eta_i^{z_i} \\exp\\left(-w_i \\eta_i \\right) \\\\\n\\eta_i &= \\log\\lambda(\\mathbf{s}_i) = \\mathbf{x}(s)'\\beta\n\\end{aligned}\n\\]\nThis is similar to a product of Poisson distributions with means \\(\\eta_i\\), exposures \\(w_i\\) and observations \\(z_i\\)."
  },
  {
    "objectID": "notes/notes_9.html#HPP",
    "href": "notes/notes_9.html#HPP",
    "title": "Modelling Geostatistical Data",
    "section": "\n4.1 Fit a homogeneous Poisson Process",
    "text": "4.1 Fit a homogeneous Poisson Process\nAs a first exercise we are going to fit a homogeneous Poisson process (HPP) to the data. This is a model that assume constant intensity over the whole space so our linear predictor is then:\n\\[\n\\eta(s) = \\log\\lambda(s) = \\beta_0 , \\ \\mathbf{s}\\in\\Omega\n\\]\nso the likelihood can be written as:\n\\[\n\\begin{aligned}\np(\\mathbf{y} | \\lambda)  & \\propto \\exp \\left( -\\int_\\Omega \\lambda(\\mathbf{s}) \\mathrm{d}\\mathbf{s} \\right) \\prod_{i=1}^n \\lambda(\\mathbf{s}_i) \\\\\n& = \\exp\\left( -\\int_{\\Omega}\\exp(\\beta_0)ds\\right)\\prod_{i=1}^n \\lambda(\\mathbf{s}_i) \\\\\n\\end{aligned}\n\\]\nwhere \\(|\\Omega|\\) is the area of the domain of interest.\nWe need to approximate the integral using a numerical integration scheme as:\n\\[\n\\approx\\exp\\left(-\\sum_{k=1}^{N_k}w_k\\lambda(s_k)\\right)\\prod_{i=1}^n \\lambda(\\mathbf{s}_i)\n\\]\nWhere \\(N_k\\) is the number of integration points \\(s_1,\\dots,s_{N_k}\\) and \\(w_1,\\dots,w_{N_k}\\) are the integration weights.\nIn this case, since the intensity is constant, the integration scheme is really simple: it is enough to consider one random point inside the domain with weight equal to the area of the domain.\n\nCode# define integration scheme\n\nips &lt;- fm_int_object(\n  st_sample(region, 1), # some random location inside the domain\n  weight = st_area(region), # integration weight is the area of the domain\n  name = \"geometry\"\n)\n\n\ncmp = ~ 0 + beta_0(1)\n\nformula = geometry ~ beta_0\n\nlik = bru_obs(data = pp,\n              family = \"cp\",\n              formula = formula,\n              ips = ips)\nfit1 = bru(cmp, lik)\n\n\nWe can then:\n\nPlot the estimated posterior distribution of the intensity\nCompare the estimated expected number of fires on the whole domain with the observed ones.\n\n\nCode# 1) The estimated posterior distribution of the  intensity is\n\npost_int = inla.tmarginal(function(x) exp(x), fit1$marginals.fixed$beta_0)\npost_int %&gt;% ggplot() + geom_line(aes(x,y))\n\n\n\n\n\n\nCode# 2) To compute the expected number of points in the area we need to multiply the\n# estimated intensity by the area of the domain.\n# In the same plot we also show the number of observed fires as a vertical line.\n\npost_int = inla.tmarginal(function(x) st_area(region)* exp(x), fit1$marginals.fixed$beta_0)\npost_int %&gt;% ggplot() + geom_line(aes(x,y)) +\n  geom_vline(xintercept = dim(pp)[1])"
  },
  {
    "objectID": "notes/notes_9.html#NHPP",
    "href": "notes/notes_9.html#NHPP",
    "title": "Modelling Geostatistical Data",
    "section": "\n4.2 Fit an Inhomogeneous Poisson Process",
    "text": "4.2 Fit an Inhomogeneous Poisson Process\nThe model above has the clear disadvantages that assumes a constant intensity and from Figure 3 we clearly see that this is not the case.\nThe library spatstat contains also some covariates that can help explain the fires distribution. Figure (fit-altitude?) shows the location of fires together with the (scaled) altitude.\n\nCodeelev_raster = rast(clmfires.extra[[2]]$elevation)\nelev_raster = scale(elev_raster)\nggplot() +\n  geom_spatraster(data = elev_raster) +\n  geom_sf(data = pp) +\n  scale_fill_scico()\n\n\n\n\n\n\nFigure 4: Distribution of the observed forest fires and scaled altitude\n\n\n\n\nWe are now going to use the altitude as a covariate to explain the variability of the intensity \\(\\lambda(s)\\) over the domain of interest.\nOur model is\n\\[\n\\log\\lambda(s) = \\beta_0 + \\beta_1x(s)\n\\] where \\(x(s)\\) is the altitude at location \\(s\\).\nThe likelihood becomes:\n\\[\n\\begin{aligned}\np(\\mathbf{y} | \\lambda)  & \\propto \\exp \\left( -\\int_\\Omega \\lambda(\\mathbf{s}) \\mathrm{d}\\mathbf{s} \\right) \\prod_{i=1}^n \\lambda(\\mathbf{s}_i) \\\\\n& = \\exp \\left( -\\int_\\Omega \\exp(\\beta_0 + \\beta_1x(s)) \\mathrm{d}\\mathbf{s} \\right) \\prod_{i=1}^n \\lambda(\\mathbf{s}_i) \\\\\n\\end{aligned}\n\\]\nNow we need to choose an integration scheme to solve the integral.\nIn this case we will take a simple grid based approach where each quadrature location has an equal weight. Our grid consists of \\(N_k = 1000\\) points and the weights are all equal to \\(|\\Omega|/N_k\\).\n\nCoden.int = 1000\nips &lt;- st_sample(region, size = n.int, type = \"regular\") # May not be exactly n.int points\nips &lt;- fm_int_object(\n  ips,\n  weight = st_area(region) / length(ips),\n  name = \"geometry\"\n)\nggplot() + geom_sf(data = ips, aes(color = weight)) + geom_sf(data= region, alpha = 0)\n\n\n\n\n\n\nFigure 5: Integration scheme.\n\n\n\n\nOBS: The implicit assumption here is that the intensity is constant inside each grid box, and so is the covariate!!\nWe can now fit the model:\n\nCodecmp = ~ Intercept(1) + elev(elev_raster, model = \"linear\")\nformula = geometry ~ Intercept + elev\nlik = bru_obs(data = pp,\n              family = \"cp\",\n              formula = formula,\n              ips = ips)\nfit2 = bru(cmp, lik)\n\n\n\n\n\n\n\n\n Task\n\n\n\nWhat is the effect of the altitude on the (log) intensity of the process?\n\n\nTake hint\n\nYou can look at the summary for the fixed effects\n\n\n\n\nClick here to see the solution\n\nCodefit2$summary.fixed\n\n                mean         sd 0.025quant   0.5quant 0.975quant       mode kld\nIntercept -6.6238635 0.10356571 -6.8268485 -6.6238635 -6.4208784 -6.6238635   0\nelev       0.6970374 0.07246861  0.5550015  0.6970374  0.8390733  0.6970374   0\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\n⚠️ WARNING!!⚠️ When fitting a Point process, the integration scheme has to be fine enough to capture the spatial variability of the covariate!!\n\n\nNow we want to predict the log-intensity over the whole domain. Use the grid from the elevation raster to predict the intensity over the domain.\n\nCodeest_grid = st_as_sf(data.frame(crds(elev_raster)), coords = c(\"x\",\"y\"))\nest_grid  = est_grid[region,]\n\n\n\nCodepreds2 = predict(fit2, est_grid, ~ data.frame(log_scale = Intercept + elev,\n\n                                              lin_scale = exp(Intercept + elev)))\n# then visualize it like\npreds2$log_scale %&gt;%\n  ggplot() +\n  geom_sf(aes(color = mean)) +\n  scale_color_scico()\n\n\n\n\n\n\n\nFinally, we want to use the fitted model to estimate the total number of fires over the whole region. To do this we first have to fine the expected number of fires as:\n\\[\nE(N_{\\Omega}) = \\int_{\\Omega}\\exp(\\lambda(s))\\ ds\n\\]\nThen simulate possible realizations of \\(N_{\\Omega}\\) to include also the likelihood variability in our estimate:\n\nCodeN_fires = generate(fit2, ips,\n                      formula = ~ {\n                        lambda = sum(weight * exp(elev + Intercept))\n                        rpois(1, lambda)},\n                    n.samples = 2000)\n\nggplot(data = data.frame(N = as.vector(N_fires))) +\n  geom_histogram(aes(x = N),\n                 colour = \"blue\",\n                 alpha = 0.5,\n                 bins = 20) +\n  geom_vline(xintercept = nrow(pp),\n             colour = \"red\") +\n  theme_minimal() +\n  xlab(expression(Lambda))"
  },
  {
    "objectID": "notes/notes_9.html#LGCP",
    "href": "notes/notes_9.html#LGCP",
    "title": "Modelling Geostatistical Data",
    "section": "\n4.3 Fit a Log-Gaussian Cox Process",
    "text": "4.3 Fit a Log-Gaussian Cox Process\nFinally we want to fit a LGCP with log intensity:\n\\[\n\\log(s) = \\beta_0 + \\beta_1x + u(s)\n\\]\nwhere \\(\\beta_0\\) is the intercept, \\(\\beta_1\\) the effect of (standardized) altitude \\(x(s)\\) as before and \\(u(s)\\) is a Gaussian Random field defined through the SPDE approach.\n\n4.3.1 Define the mesh\nThe first step, as any time we use the SPDE approach is to define the mesh and the priors for the marginal variance and range:\n\nCodemesh = fm_mesh_2d(boundary = region,\n                  max.edge = c(8, 20),\n                  cutoff = 4, crs = NA)\n\nggplot() + gg(mesh) + geom_sf(data = pp)\n\n\n\n\n\n\nCodespde_model =  inla.spde2.pcmatern(mesh,\n                                  prior.sigma = c(1, 0.5),\n                                  prior.range = c(100, 0.5))\n\n\nWe can then define the integration weight. Here we use the same points to define the SPDE approximation and to approximate the integral in the likelihood. We will see later that this does not have to be like this, BUT integration weight and SPDE weights have to be consistent with each other!\n\nCodeips = fm_int(mesh, samplers = region)\n\nggplot() + geom_sf(data = ips, aes(color = weight)) +\n  gg(mesh) +\n   scale_color_scico()\n\n\n\n\n\n\n\n\n4.3.2 Run the model\n\nCodecmp = ~ Intercept(1) + space(geometry, model = spde_model) + elev(elev_raster, model = \"linear\")\n\nformula = geometry ~ Intercept + space + elev\n\nlik = bru_obs(\"cp\",\n              formula = formula,\n              data = pp,\n              ips = ips)\n\nfit3 = bru(cmp, lik)\n\n\nNote when running the model above you will get a warning:\n\n\nWarning in bru_log_warn(msg): Model input 'elev_raster' for 'elev' returned some NA values.\nAttempting to fill in spatially by nearest available value.\nTo avoid this basic covariate imputation, supply complete data.\n\n\nIt means that the bru() function cannot find the covariate values for some of the mesh nodes. This is a common situation. As the warning says, the bru() function automatically imputes the value of the covarite using the nearest nodes. This increases the running time of thebru() function, so one solution is to impute the values of the covariate over the whole mesh ‘before’ running the bru() function.\nHere, we notice that there are points for which elevation values are missing (see Figure 6 the red points that lies outside the raster extension ).\n\n\n\n\n\n\n\nFigure 6: Integration scheme for numerical approximation of the stochastic integral in La Mancha Region\n\n\n\n\nTo solve this, we can increase the raster extension so it covers all both data-points and quadrature locations as well. Then, we can use the bru_fill_missing() function to input the missing values with the nearest-available-value. We can achieve this using the following code:\n\nCode# Extend raster ext by 30 % of the original raster so it covers the whole mesh\nre &lt;- extend(elev_raster, ext(elev_raster)*1.3)\n# Convert to an sf spatial object\nre_df &lt;- re %&gt;% stars::st_as_stars() %&gt;%  st_as_sf(na.rm=F)\n# fill in missing values using the original raster\nre_df$lyr.1 &lt;- bru_fill_missing(elev_raster,re_df,re_df$lyr.1)\n# rasterize\nelev_rast_p &lt;- stars::st_rasterize(re_df) %&gt;% rast()\nggplot() + geom_spatraster(data = elev_rast_p)+\n  geom_sf(data=ips,alpha=0.25,col=1)\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe bru_fill_missing() function was added mainly to handle very local infilling on domain boundaries. For properly missing data, one should consider doing a proper model of the spatial field instead.\n\n\n\n4.3.3 Results\nWe can plot the estimated mean and standard deviation of the spatial GF and the log-intensity over the domain of interest\n\nCodepxl = fm_pixels(mesh, mask= region, dims = c(200,200))\npreds = predict(fit3, pxl, ~data.frame(spde = space,\n                                       log_int = Intercept + space + elev))\n\n#and plot as\nlibrary(scico)\nlibrary(patchwork)\n\nggplot(data= preds$spde) +\n  geom_sf(aes(color = mean)) +\n  scale_color_scico() +\n ggtitle(\"spde mean\") +\nggplot(data=preds$spde ) +\n  geom_sf(aes(color = sd)) +\n  scale_color_scico() +\n ggtitle(\"spde sd\") +\n\nggplot(data=preds$log_int) +\n  geom_sf(aes(color = mean)) +\n  scale_color_scico() +\n ggtitle(\"log-int mean\")\n\n\n\n\n\n\nCodeggplot(data=preds$log_int) +\n  geom_sf(aes(color = sd)) +\n  scale_color_scico() +\n ggtitle(\"log-int sd\") +\n  plot_layout(ncol=2)"
  },
  {
    "objectID": "notes/notes_9.html#density-surface-models",
    "href": "notes/notes_9.html#density-surface-models",
    "title": "Modelling Geostatistical Data",
    "section": "\n5.1 Density Surface Models",
    "text": "5.1 Density Surface Models\nCoupling distance sampling data with spatial modelling enables the production of maps showing spatially varying population density. This represents a significant advance over global abundance estimates, as it reveals how animals distribute themselves across the landscape in relation to environmental features. Historically, density surface modelling has been implemented through a two-stage process. The first stage involves estimating detectability to create an offset vector that accounts for imperfect detection. This offset is then incorporated into a second-stage model—typically a Generalized Linear Model (GLM) or Generalized Additive Model (GAM)—that analyzes count response data. Implementing this approach requires discretizing the continuous survey area into segments and aggregating the detected animals into counts within each spatial unit.\n\n\nSchematic of the two-stage density surface modelling approach\n\nThe discretization process necessary for this approach is illustrated below, showing how continuous survey transects are divided into segments for analysis:\n\n\nDiscretization of survey data for two-stage modelling\n\nDespite its widespread use, the two-stage approach suffers from a critical limitation: it fails to properly propagate uncertainty from the first-stage detection model to the second-stage spatial model. The detection function parameters are estimated with associated uncertainty, but this uncertainty is effectively fixed when creating the offset, leading to potentially overconfident predictions and underestimated standard errors in the final spatial maps.\nThe solution\nThe goal of contemporary methodological development has been to create a one-stage distance sampling model that simultaneously estimates detectability and the spatial distribution of animals. This unified approach is achieved through a point process framework, which naturally accommodates both the ecological process generating animal locations and the observation process determining which animals are detected.\n\n\nThe one-stage approach using a point process framework\n\nThis integrated framework ensures that all sources of uncertainty are accounted for in a single, coherent analysis, producing more reliable estimates and predictions."
  },
  {
    "objectID": "notes/notes_9.html#thinned-point-processes",
    "href": "notes/notes_9.html#thinned-point-processes",
    "title": "Modelling Geostatistical Data",
    "section": "\n5.2 Thinned Point Processes",
    "text": "5.2 Thinned Point Processes\nThe Log-Gaussian Cox Process (LGCP) provides a flexible and powerful framework for modelling spatial point patterns. It models the intensity of points—representing animal locations—by incorporating the effects of spatial covariates and including a mean-zero spatially structured random effect. This random effect accounts for unexplained heterogeneity in the distribution that is not captured by the measured covariates, making the LGCP particularly suitable for ecological data where many influential factors may be unobserved.\nEcological survey data never represents a complete census of all animals present. To account for the imperfect detection inherent in distance sampling, we specify a thinning probability function:\n\\[\ng(s) = \\mathbb{P}(\\text{a point at s is detected}|\\text{a point is at s})\n\\]\nThis function represents the probability that an animal at location \\(s\\) is detected by the observer, given that an animal is actually present at that location.\nA key and remarkably useful property of the Log-Gaussian Cox Process is that a realization of a point process with intensity \\(\\lambda(s)\\), when thinned by the probability function \\(g(s)\\), itself follows a LGCP. The intensity of this observed, thinned process is simply the product of the true intensity and the thinning probability:\n\\[\n\\underbrace{\\tilde{\\lambda}(s)}_{\\text{observed process}} = \\underbrace{\\lambda(s)}_{\\text{true process}} \\times \\underbrace{g(s)}_{\\text{thinning probability}}\n\\]\nThis mathematical formulation allow us to build unified models that simultaneously estimate both the true density of animals and the detection process. The thinning framework then separates the ecological process of interest from the observation process, while maintaining them within a single, coherent statistical model.\nStandard distance sampling approaches specify \\(g(s)\\) as a function that declines with increasing distance: - horizontal distance to the observer for point transects - perpendicular distance to the transect line for line transects\n\n\n\n\nIn this course we will focus on perpendicular distances only. The thinning probability function is specified as a parametric family of functions:\n\nHalf-normal: \\(g(\\mathbf{s}|\\sigma) = \\exp(-0.5 (d(\\mathbf{s})/\\sigma)^2)\\)\nHazard-rate :\\(g(\\mathbf{s}|\\sigma) = 1 - \\exp(-(d(\\mathbf{s})/\\sigma)^{-1})\\)\n\n\n\n\n\n\n\n\n\nTo make \\(g(s)\\) and \\(\\lambda(s)\\) identifiable, we assume intensity is constant with respect to distance from the observer. In practice this means we assume animals are uniformly distributed with respect to distance from the line"
  },
  {
    "objectID": "notes/notes_9.html#summary-of-distance-sampling-pp-approach",
    "href": "notes/notes_9.html#summary-of-distance-sampling-pp-approach",
    "title": "Modelling Geostatistical Data",
    "section": "\n5.3 Summary of Distance sampling PP approach",
    "text": "5.3 Summary of Distance sampling PP approach\n\nThe true point pattern \\(Y = \\mathbf{s}_1,\\ldots,\\mathbf{s}_n\\) are a realization of a Point process with intensity \\(\\lambda(s)\\)\n\n\n\n\n\n\n\nWe design a sampling survey to collect the data along transects\n\n\n\n\n\n3. detected points are generated from the thinned PP with intensity \\(\\color{red}{\\tilde{\\lambda}(s)}= \\lambda(s)\\color{red}{g(d(s))}\\) \\[ \\therefore \\log \\color{red}{\\tilde{\\lambda}(s)} = \\overbrace{\\log \\lambda (s)}^{\\mathbf{x}'\\beta + \\xi(s)} + \\overbrace{\\log \\color{red}{g(d(s))}}^{-0.5~d(\\mathbf{s})^2\\sigma^{-2}}\\] \n\nThe encounter rate, i.e. the number of observed animals within a distance \\(W\\) follows \\(m \\sim \\text{Poisson} \\left(\\int_0^W \\tilde{\\lambda}(d)\\text{d}d\\right)\\)\nThe pdf of detected distances is \\(\\pi(d_1,\\ldots,d_m|m) \\propto \\prod_{i=1}^m\\dfrac{\\tilde{\\lambda}(d_i)}{\\int_0^W \\tilde{\\lambda}(d)\\text{d}d}\\) where \\(\\color{red}{\\tilde{\\lambda}(d_i)} = \\lambda \\color{red}{g(d_i)}\\) if we assume constant intensity with respect the distance to the observer.\n\nIf the strips width ( \\(2W\\) ) is narrow compared to study region (\\(\\Omega\\)) we can treat them as lines. This mean we need to define the Poisson process likelihood along the kronecker spaces (line \\(\\times\\) distance). Accounting for imperfect detection the thinned Poisson process model on (space, distance) along the transects becomes:\n\\[\n\\begin{aligned}\n\\log \\tilde{\\lambda}(s,\\text{distance}) &= \\overbrace{\\mathbf{x}'\\beta + \\xi(s)}^{\\log \\lambda(s)} + \\log \\mathbb{P}(\\text{detection at }s|\\text{distance},\\sigma) + \\log(2)\\\\\n\\mathbb{P}(\\text{detection}) &=1-\\exp\\left(-\\frac{\\sigma}{\\text{distance}}\\right)\n\\end{aligned}\n\\]\n\nHere \\(\\log 2\\) accounts for the two-sided detection.\nTypically \\(\\mathbb{P}(distance)\\) is a non-linear function, that is where inlabru can help via a Fixed point iteration scheme (further details available in this vignette)\nwe define \\(\\log (\\sigma)\\) as a latent Gaussian variable and iteratively linearise it."
  },
  {
    "objectID": "notes/notes_9.html#results-1",
    "href": "notes/notes_9.html#results-1",
    "title": "Modelling Geostatistical Data",
    "section": "\n6.1 Results",
    "text": "6.1 Results\nposterior summaries\n\n\n\n\n\n\n\nmean\n0.025quant\n0.975quant\n\n\n\nIntercept\n−8.41\n−9.47\n−7.62\n\n\nsigma\n−0.05\n−0.46\n0.36\n\n\nRange for space\n131.74\n41.79\n320.28\n\n\nStdev for space\n1.17\n0.72\n1.78\n\n\n\n\n\n\nWe can also to plot the posterior density of the Matérn field parameters\n\nCodespde.posterior(fit, \"space\", what = \"range\") %&gt;% plot()\n\n\n\n\n\n\nCodespde.posterior(fit, \"space\", what = \"log.variance\") %&gt;% plot()\n\n\n\n\n\n\n\nTo map the spatial intensity we first need to define a grid of points where we want to predict.\n\nWe do this using the function fm_pixel() which creates a regular grid of points covering the mesh\nThen, we use the predict function which takes as input\n\nthe fitted model (fit)\nthe prediction points (pxl)\nthe model components we want to predict (e.g., \\(e^{\\beta_0 + \\xi(s)}\\))\n\n\nTo plot this you can use ggplot and add a gg() layer with your output of interest (E.g., pr.int$spatial)\n\n\nCodelibrary(patchwork)\npxl &lt;- fm_pixels(mesh, dims = c(200, 100), mask = mexdolphin$ppoly)\npr.int &lt;- predict(fit, pxl, ~ data.frame(spatial = space,\n                                      loglambda = Intercept + space,\n                                      lambda = exp(Intercept + space)))\n\n\n\nCodeggplot() +\n  gg(pr.int$spatial, geom = \"tile\")\n\n\n\n\n\n\n\n\n\n\nWe can also use the predict function to predict the detection probabilities:\n\nCodedistdf &lt;- data.frame(distance = seq(0, 8, length.out = 100))\ndfun &lt;- predict(fit, distdf, ~ hn(distance, sigma))\nplot(dfun)\n\n\n\n\n\n\n\nData level prediction* {.smaller}\n47 groups were seen. How many would be seen along the transects under perfect detection?\n\nCodepredpts_transect &lt;- fm_int(mexdolphin$mesh, mexdolphin$samplers)\nLambda_transect &lt;- predict(fit,\n                           predpts_transect,~ 16 * sum(weight * exp(space + Intercept)))\n\n\n\n\n\n\n\n\nmean\nsd\nq0.025\nq0.5\nq0.975\nmedian\nsd.mc_std_err\nmean.mc_std_err\n\n\n97.45\n31.17\n54.29\n92.42\n151.01\n92.42\n5.18\n4.15\n\n\n\n\n\nHow many would be seen under perfect detection across the whole study area (i.e., the mean expected number of dolphins)?\n\nCodepredpts &lt;- fm_int(mexdolphin$mesh, mexdolphin$ppoly)\nLambda &lt;- predict(fit, predpts, ~ sum(weight * exp(space + Intercept)))\n\n\n\n\n\n\n\n\nmean\nsd\nq0.025\nq0.5\nq0.975\nmedian\nsd.mc_std_err\nmean.mc_std_err\n\n\n315.70\n90.62\n181.54\n297.15\n542.25\n297.15\n8.77\n10.82\n\n\n\n\n\nexpected counts\nWhat’s the predictive distribution of group counts?\nWe can get Monte Carlo samples for the expected number of dolphins as follows:\n\nCodeNs &lt;- seq(50, 450, by = 1)\n\nNest &lt;- predict(fit, predpts,\n  ~ data.frame(\n    N = Ns,\n    density = dpois(\n      Ns,\n      lambda = sum(weight * exp(space + Intercept))\n    )\n  ),\n  n.samples = 2000\n)\n\nNest &lt;- dplyr::bind_rows(\n  cbind(Nest, Method = \"Posterior\"),\n  data.frame(\n    N = Nest$N,\n    mean = dpois(Nest$N, lambda = Lambda$mean),\n    mean.mc_std_err = 0,\n    Method = \"Plugin\"\n  )\n)"
  },
  {
    "objectID": "slides/slides_8.html#geostatistical-data",
    "href": "slides/slides_8.html#geostatistical-data",
    "title": "Modelling Geostatistical Data",
    "section": "Geostatistical Data",
    "text": "Geostatistical Data\nIn many ecological and environmental settings, measurements are taken from fixed sampling units, aiming to quantify spatial variation and interpolate values at unobserved sites.\n\n\n\n\nGeostatistical data are the most common form of spatial data found in environmental and ecological settings.\nWe regularly take measurements of an environmental variable of interest at a set of fixed locations.\nThis could be data from samples taken across a region (eg., water depth in a lake) or from monitoring stations as part of a network (e.g., air pollution).\nIn each of these cases, our goal is to estimate the value of our variable across the entire space."
  },
  {
    "objectID": "slides/slides_8.html#understanding-our-region",
    "href": "slides/slides_8.html#understanding-our-region",
    "title": "Modelling Geostatistical Data",
    "section": "Understanding our region",
    "text": "Understanding our region\n\n\nLet \\(D\\) be our two-dimensional region of interest.\nIn principle, there are infinitely many locations within \\(D\\), each of which can be represented by mathematical coordinates (e.g. latitude and longitude).\nWe can identify any individual location as \\(\\mathbf{s}_i = (x_i, y_i)\\), where \\(x_i\\) and \\(y_i\\) are their coordinates.\nWe can treat our variable of interest as a random variable, \\(Z\\) which can be observed at any location as \\(Z(\\mathbf{s}_i)\\)."
  },
  {
    "objectID": "slides/slides_8.html#geostatistical-process",
    "href": "slides/slides_8.html#geostatistical-process",
    "title": "Modelling Geostatistical Data",
    "section": "Geostatistical process",
    "text": "Geostatistical process\n\nA geostatistical process can therefore be written as: \\[\\{Z(\\mathbf{s}); \\mathbf{s} \\in D\\}\\]\nIn practice, data are observed at a finite number of locations, \\(m\\), and can be denoted as: \\[z = \\{z(\\mathbf{s}_1), \\ldots z(\\mathbf{s}_m) \\}\\]\nWe have observed our data at \\(m\\) locations, but often want to predict this process at a set of unknown locations.\nFor example, what is the value of \\(z(\\mathbf{s}_0)\\), where \\(\\mathbf{s}_0\\) is an unobserved site?"
  },
  {
    "objectID": "slides/slides_8.html#spatial-autocorrelation",
    "href": "slides/slides_8.html#spatial-autocorrelation",
    "title": "Modelling Geostatistical Data",
    "section": "Spatial autocorrelation",
    "text": "Spatial autocorrelation\n\nThe key challenge in modelling geostatistical data is understanding correlation.\nTypically observations close together in space will be more similar than those which are further apart.\nSpatial correlation is usually driven by some unmeasured confounding variable(s) - for example, air pollution is spatially correlated because nearby areas tend to experience similar traffic levels.\nIt is important that we account for these correlations in our analysis - failing to do so will lead to poor inference."
  },
  {
    "objectID": "slides/slides_8.html#modelling-geostatistical-data",
    "href": "slides/slides_8.html#modelling-geostatistical-data",
    "title": "Modelling Geostatistical Data",
    "section": "Modelling geostatistical data",
    "text": "Modelling geostatistical data\nFor a set of geostatistical data \\(\\mathbf{z} = \\{ z(\\mathbf{s}_1), \\ldots, z(\\mathbf{s}_m) \\}\\), we can consider the general model:\n\\[Z(\\mathbf{s}_i) = \\mu(\\mathbf{s}_i) + e(\\mathbf{s}_i)\\]\n\n\n\\(\\mu(\\mathbf{s}_i)\\) is a mean function which models trend and covariate effects.\nThen \\(e(\\mathbf{s}_i)\\) is the error process which accounts for any spatial correlation which exists after accounting for \\(\\mu(\\mathbf{s}_i)\\)\nSpatial statistics is therefore often focused on understanding the process for \\(e(\\mathbf{s}_i)\\)."
  },
  {
    "objectID": "slides/slides_8.html#our-key-problems",
    "href": "slides/slides_8.html#our-key-problems",
    "title": "Modelling Geostatistical Data",
    "section": "Our key problem(s)",
    "text": "Our key problem(s)\n\n\nWe have observations at \\(m\\) locations \\[\\mathbf{z} = \\{ z(\\mathbf{s}_1), \\ldots, z(\\mathbf{s}_m) \\}.\\]\nWe want to use these to obtain an estimate of \\(Z(\\mathbf{s}_0)\\) where \\(\\mathbf{s}_0\\) is an unobserved location.\nHow do we model the spatial dependence between our observed sites \\(\\mathbf{s}_1, \\ldots, \\mathbf{s}_m\\)?\nWhat does this tell us about the dependence between our observed sites and our unobserved site \\(\\mathbf{s}_0\\)?"
  },
  {
    "objectID": "slides/slides_8.html#variograms",
    "href": "slides/slides_8.html#variograms",
    "title": "Modelling Geostatistical Data",
    "section": "Variograms",
    "text": "Variograms\nThe first step is to assess whether there is any evidence of spatial dependency in our data.\n\nSpatial dependence in georeferenced data can be explored by a function known as a variogram \\(2\\gamma(\\cdot)\\) (or semivariogram \\(\\gamma(\\cdot)\\)).\nThe variogram is similar in many ways to the autocorrelation function used in time series modelling.\nIn simple terms, it is a function which measures the difference in the spatial process between a pair of locations a fixed distance apart."
  },
  {
    "objectID": "slides/slides_8.html#variograms-1",
    "href": "slides/slides_8.html#variograms-1",
    "title": "Modelling Geostatistical Data",
    "section": "Variograms",
    "text": "Variograms\nThe variogram measures the variance of the difference in the process \\(Z(\\cdot)\\) at two spatial locations \\(\\mathbf{s}\\) and \\(\\mathbf{s+h}\\) and is defined as :\n\\[\\mathrm{Var}[Z(\\mathbf{s}) - Z(\\mathbf{s} + \\mathbf{h})] = E[(Z(\\mathbf{s}) - Z(\\mathbf{s} + \\mathbf{h}))^2] = 2\\gamma_z(\\mathbf{h}).\\]\nNote that in practice we use the semi-variogram \\(\\gamma_z(\\mathbf{h})\\) because our points come in pairs, and the semi-variance is equivalent to the variance per point at a given lag.\n\n\nWhen the variance of the difference \\(Z(\\mathbf{s}) - Z(\\mathbf{s} + \\mathbf{h})\\) is relatively small, then \\(Z(\\mathbf{s})\\) and \\(Z(\\mathbf{s} + \\mathbf{h})\\) are similar (spatially correlated).\nWhen the variance of the difference \\(Z(\\mathbf{s}) - Z(\\mathbf{s} + \\mathbf{h})\\) is relatively large, then \\(Z(\\mathbf{s})\\) and \\(Z(\\mathbf{s} + \\mathbf{h})\\) are less similar (closer to independence)."
  },
  {
    "objectID": "slides/slides_8.html#variograms-2",
    "href": "slides/slides_8.html#variograms-2",
    "title": "Modelling Geostatistical Data",
    "section": "Variograms",
    "text": "Variograms\n\nA plot of the empirical semivariogram against the separation distance conveys important information about the continuity and spatial variability of the process.\n\n\n\nThe sill is the maximum variance as \\(h \\to \\infty\\).\nThe partial sill represents the spatially structured variability\nThe nugget is the minimum variance as \\(h \\to 0\\) and represents the variability at distances smaller than sampling interval.\nThe range is the distance to the sill.\nPoints further apart than the range are assumed to be uncorrelated."
  },
  {
    "objectID": "slides/slides_8.html#example-construction-a-variogram",
    "href": "slides/slides_8.html#example-construction-a-variogram",
    "title": "Modelling Geostatistical Data",
    "section": "Example: Construction a Variogram",
    "text": "Example: Construction a Variogram\nIn practice, we only have access to \\(m\\) realisations of this process, and therefore we have to estimate the variogram. This is known as the empirical variogram.\nThe empirical semivariogram can be used as exploratory tool to assess whether data present spatial correlation.\nWe obtain this by computing the semi-variance for all possible pairs of observations: \\(\\gamma(\\mathbf{s}, \\mathbf{s} + \\mathbf{h}) = 0.5(Z(\\mathbf{s}) - Z(\\mathbf{s} + \\mathbf{h}))^2\\)."
  },
  {
    "objectID": "slides/slides_8.html#example-constructing-a-variogram",
    "href": "slides/slides_8.html#example-constructing-a-variogram",
    "title": "Modelling Geostatistical Data",
    "section": "Example: Constructing a Variogram",
    "text": "Example: Constructing a Variogram\nThe data Paraná from the geoR Package contains the average rainfall over different years for the period May to June at 123 monitoring stations in Paraná state, Brazil.\n\nRainfall values measured at 143 recording stations in Paraná state, Brazil with low values being represented in blue and high values in red."
  },
  {
    "objectID": "slides/slides_8.html#example-construction-a-variogram-1",
    "href": "slides/slides_8.html#example-construction-a-variogram-1",
    "title": "Modelling Geostatistical Data",
    "section": "Example: Construction a Variogram",
    "text": "Example: Construction a Variogram\nTo illustrate how an empirical variogram is computed, consider the two highlighted locations below."
  },
  {
    "objectID": "slides/slides_8.html#example-construction-a-variogram-2",
    "href": "slides/slides_8.html#example-construction-a-variogram-2",
    "title": "Modelling Geostatistical Data",
    "section": "Example: Construction a Variogram",
    "text": "Example: Construction a Variogram\nTo illustrate how an empirical variogram is computed, consider the two highlighted locations below.\n\n\n\n\n\n\n\n\n\n\nWe can first compute the distance between the two locations using the Euclidean distance formula \\[h = \\sqrt{(475.1 - 403)^2 + (83.6 - 164.5)^2} = 108.36\\]\nNext, we compute the semi-variance between the points using their observed values as \\[\\begin{aligned}\\gamma(\\mathbf{s}, \\mathbf{s}+\\mathbf{h}) &= 0.5(Z(\\mathbf{s}) - Z(\\mathbf{s}+\\mathbf{h}))^2 \\\\ &= 0.5(315.33 - 306.9)^2 = 35.53\\end{aligned}\\]\nWe repeat this process for every possible pair of points, and plot \\(h\\) against \\(\\gamma(\\mathbf{s}, \\mathbf{t})\\) for each."
  },
  {
    "objectID": "slides/slides_8.html#example-construction-a-variogram-3",
    "href": "slides/slides_8.html#example-construction-a-variogram-3",
    "title": "Modelling Geostatistical Data",
    "section": "Example: Construction a Variogram",
    "text": "Example: Construction a Variogram\nWe can calculate the empirical variogram for the data using the variogram function from the gstat library.\n\n\n\n\n\n\n\nEmpirical variogram values corresponding to the rainfall data in Paraná state, Brazil.\n\n\n\n\n\n\nThis plot shows the semi-variances for each pair of points.\nEach pair of points has a different distance, making it difficult to use this for prediction."
  },
  {
    "objectID": "slides/slides_8.html#binned-variogram",
    "href": "slides/slides_8.html#binned-variogram",
    "title": "Modelling Geostatistical Data",
    "section": "Binned variogram",
    "text": "Binned variogram\nTo make the variogram easier to use and interpret, we divide the distances into a set of discrete bins, and compute the average semi-variance in each.\n\nWe compute this binned empirical variogram as \\[\\gamma(\\mathbf{h}) = \\frac{1}{2N(h_k)}\\sum_{(\\mathbf{s},\\mathbf{t}) \\in N(h_k)}[z(\\mathbf{s}) - z(\\mathbf{s}+\\mathbf{h})]^2\\]\nHere, \\(k\\) is the number of bins and \\(N(h_k)\\) is the number of points in the bin with average distance \\(h\\).\nWe then construct a plot of our empirical variogram and use this to estimate the covariance structure."
  },
  {
    "objectID": "slides/slides_8.html#binned-variogram-1",
    "href": "slides/slides_8.html#binned-variogram-1",
    "title": "Modelling Geostatistical Data",
    "section": "Binned variogram",
    "text": "Binned variogram\nThe bins are illustrated on the left, and the empirical variogram obtained from them is shown on the right."
  },
  {
    "objectID": "slides/slides_8.html#do-we-observe-any-spatial-dependence",
    "href": "slides/slides_8.html#do-we-observe-any-spatial-dependence",
    "title": "Modelling Geostatistical Data",
    "section": "Do we observe any spatial dependence?",
    "text": "Do we observe any spatial dependence?\nWe can construct null envelope based on permutations of the data values across the locations, i.e. envelopes built under the assumption of no spatial correlation.\n\n\n\n\nBy overlapping these envelopes with the empirical variograms we can determine whether there is some spatial dependence in our data\nWe can construct permutation envelopes on the empirical variogram using the envelope function from the variosig R package.\nIn this example, we observe that the variogram only falls outside of the null envelope at distances \\(&lt;200\\)m and also at distances above \\(300\\)m.\nOnce we have computed an empirical variogram, we have to think about model fitting.\n\n\n\n\n\n\n\n\n\n\n\n\n[1] \"There are 13 out of 15 variogram estimates outside the 95% envelope.\""
  },
  {
    "objectID": "slides/slides_8.html#what-we-observed-and-what-we-did-not",
    "href": "slides/slides_8.html#what-we-observed-and-what-we-did-not",
    "title": "Modelling Geostatistical Data",
    "section": "What We Observed and What We Did Not",
    "text": "What We Observed and What We Did Not\nWe treat the observed process of interest as being measured with error\n\\[\n(\\text{observed value})_i = (\\text{true value at location } i) + (\\text{error})_i\n\\]\nalternatively\n\\[\ny_i = Z(\\mathbf{s}_i) + \\varepsilon_i\n\\]\nWhen geostatistical data are considered, we can often assume that there is a spatially continuous variable underlying the observations that can be modeled using a random field.\n\n\nwe have a process that is occurring everywhere in space \\(\\rightarrow\\) natural to try to model it using some sort of function (of space)\na random field is a random function that generates smooth surfaces\nThis is hard\nWe typically make our lives easier by making everything Gaussian"
  },
  {
    "objectID": "slides/slides_8.html#gaussian-random-fields",
    "href": "slides/slides_8.html#gaussian-random-fields",
    "title": "Modelling Geostatistical Data",
    "section": "Gaussian Random Fields",
    "text": "Gaussian Random Fields\nA Gaussian random field (GRF) is a collection of random variables, where observations occur in a continuous domain, and where every finite collection of random variables has a multivariate normal distribution\n\\[\n\\mathbf{z} = (z(\\mathbf{s}_1),\\ldots,z(\\mathbf{s}_m)) \\sim N(\\mu(\\mathbf{s}_1),\\ldots,\\mu(\\mathbf{s}_m),\\Sigma),\n\\]\nwhere \\(\\Sigma_{ij} = \\mathrm{Cov}(z(\\mathbf{s}_i),z(\\mathbf{s}_j))\\) is a dense \\(m \\times m\\) matrix.\n\n\nThis is actually quite tricky: \\(\\Sigma\\) will need to depend on the set of observation sites and has to behave well (be “positive definite”)\nUse a covariance function \\(C_z(\\cdot,\\cdot)\\) that depends on the distance (\\(\\Sigma_{ij} = C_z(\\mathbf{s}_i,\\mathbf{s}_j)\\)) between two points and that\n\nhas no negative variances\nis symmetric\nis decreasing, with maximum at distance = 0\n\n\\(C_z(\\mathbf{s}_i, \\mathbf{s}_j)\\) measures the strength of the linear dependence between \\(Z(\\mathbf{s}_i)\\) and \\(Z(\\mathbf{s}_j)\\).\n\\(C_z(\\mathbf{s}_i, \\mathbf{s}_j) = \\mathrm{Var}(Z(\\mathbf{s}_i))\\) for \\(i = j\\)."
  },
  {
    "objectID": "slides/slides_8.html#stationary-and-isotropy",
    "href": "slides/slides_8.html#stationary-and-isotropy",
    "title": "Modelling Geostatistical Data",
    "section": "Stationary and isotropy",
    "text": "Stationary and isotropy\nWe will assume our Gaussian process can be described as weakly stationary if the following criteria are met:\n\\(E[{Z(\\mathbf{s})}] = \\mu_z(\\mathbf{s}) = \\mu_z\\) - a finite constant which does not depend on \\(\\mathbf{s}\\).\n\\(\\text{Cov}(Z(\\mathbf{s}_i),Z(\\mathbf{s}_j)) = C_z(\\mathbf{s}_i-\\mathbf{s}_j)\\) - a finite constant which can depend on distance \\((\\mathbf{s}_i-\\mathbf{s}_j)\\).\n\nCondition 1 states that our mean function must be constant in space, with no overall spatial trend.\nCondition 2 states that for any two locations, their covariance depends only on how far apart they are (their spatial lag, \\(h\\)), not their absolute position.\n\nOur process is said to be isotropic if the covariance function is directionally invariant. This means that the covariance only depends on the euclidean distance \\((||\\mathbf{s}_i-\\mathbf{s}_j||)\\) and not the direction."
  },
  {
    "objectID": "slides/slides_8.html#building-the-model",
    "href": "slides/slides_8.html#building-the-model",
    "title": "Modelling Geostatistical Data",
    "section": "Building the model",
    "text": "Building the model\nThe first step in defining a model for a random field in a hierarchical framework is to identify a probability distribution for the observations available at \\(m\\) sampled spatial locations and represented by the vector \\(\\mathbf{y} = y_1,\\ldots,y_m\\).\nFor example, if we assume our observations follow a Gaussian distribution then\n\\[\n\\begin{aligned}\nY_i &\\sim N(\\mu_i,\\tau_e^{-1})\\\\\n\\eta_i &=\\mu_i = \\beta_0 + \\ldots + Z(\\mathbf{s}_i)\n\\end{aligned}\n\\]\n\n\\(\\tau_e^{-1} = \\sigma^2_e\\) represents the variance of the zero-mean measurement error (equivalent to the nugget effect)\nThe response mean \\(\\mu_i\\) which coincides with the linear predictor \\(\\eta_i\\) is defined based on:\n\nthe intercept \\(\\beta_0\\) and any additional covariates\nthe realization of the latent (unobservable) GF \\(Z(\\mathbf{s}) \\sim \\mathrm{MVN}(0,\\Sigma)\\) which accounts for the spatial correlation through \\(\\Sigma = C_z(\\cdot,\\cdot)\\)."
  },
  {
    "objectID": "slides/slides_8.html#the-matérn-field",
    "href": "slides/slides_8.html#the-matérn-field",
    "title": "Modelling Geostatistical Data",
    "section": "The Matérn Field",
    "text": "The Matérn Field\nA commonly used covariance function is the Matérn covariance function. The covariance of two points which are a distance \\(h\\) apart is:\n\\[\n    \\Sigma =C_{\\nu}(h) = \\sigma^2 \\frac{2^{1-\\nu}}{\\Gamma(\\nu)} \\left( \\frac{\\sqrt{2\\nu} h}{\\rho} \\right)^{\\nu} K_{\\nu} \\left( \\frac{\\sqrt{2\\nu} h}{\\rho} \\right)\n\\]\n\n\\(\\Gamma(\\cdot)\\) is the gamma function\n\\(K_{\\nu}(\\cdot)\\) is the modified Bessel function of the second kind.\nParameters \\(\\sigma^2\\), \\(\\rho\\) and \\(\\nu\\) are non-negative values of the covariance function.\n\n\\(\\sigma^2\\) is the spatially structured variance component\n\\(\\rho\\) is the range of the spatial process\n\\(\\nu\\) controls smoothness of the spatial process."
  },
  {
    "objectID": "slides/slides_8.html#big-n-problem",
    "href": "slides/slides_8.html#big-n-problem",
    "title": "Modelling Geostatistical Data",
    "section": "Big n problem!",
    "text": "Big n problem!\nThe disadvantage of the modelling approach involving the spatial covariance function is known as “big n problem” and concerns the computational costs required for algebra operations with dense covariance matrices (such as \\(\\Sigma\\)).\nIn particular dense matrix operations scale cubically with the matrix size, given by the number of locations where the process is observed. A computationally effective alternative is given by the stochastic partial differential equation (SPDE) approach"
  },
  {
    "objectID": "slides/slides_8.html#the-spde-approach",
    "href": "slides/slides_8.html#the-spde-approach",
    "title": "Modelling Geostatistical Data",
    "section": "The SPDE approach",
    "text": "The SPDE approach\nWe define a (Matérn) GRF as the solution of a stochastic partial differential equation (SPDE)\n\\[\n(\\kappa^2-\\Delta)^{\\alpha/2}Z(t) = W(t)\n\\]\nWhat is this?\n\n\\(W(t)\\) is random noise\n\\(\\omega(t)\\) is the smooth process we want\n\\((\\kappa^2-\\Delta)^{\\alpha/2}\\) is an operator that “smooths” the white noise.\n\\(\\kappa\\) and \\(\\alpha\\) are parameters"
  },
  {
    "objectID": "slides/slides_8.html#one-analogy",
    "href": "slides/slides_8.html#one-analogy",
    "title": "Modelling Geostatistical Data",
    "section": "One analogy",
    "text": "One analogy\n\n\nImagine a guitar string stretched from left to right.\n\nNow imagine someone randomly taps along it at many locations:\n\n\nEach tap is independent\nSome taps are strong, some weak\nThere is no coordination between taps\nA tap at one location tells you nothing about a tap nearby\n\n\n\n\nThis is pure randomness. But a real string does not behave like this\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo smoothness\nSharp up/down jumps\nNo gentle waves\nNo continuity in behavior\nThat’s because nothing connects one tap to the next."
  },
  {
    "objectID": "slides/slides_8.html#one-analogy-1",
    "href": "slides/slides_8.html#one-analogy-1",
    "title": "Modelling Geostatistical Data",
    "section": "One analogy",
    "text": "One analogy\n\n\nImagine a guitar string stretched from left to right.\n\nThe rope has tension and stiffness\n\nthe tension spreads each tap to nearby points\nSharp jumps are softened. \\((\\kappa^2-\\Delta)^{\\color{red}{\\alpha}/2}Z(t) = W(t)\\)\n\n\n\n\n\\(\\Delta\\) measures the local curvature\n\\(\\kappa\\) controls how far randomness propagates\nstronger tension (large \\(\\kappa\\)) = stronger pull\n\n\\[(\\color{red}{\\kappa^2}-\\Delta)Z(t) = W(t), ~~~\\text{for } \\alpha=2\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\nMeasure the bending at \\(z_i\\) by taking the with respect the average of the neighbours (i.e. \\(z_i - (z_{i-1}+z_i/2) = z_{i-1} + 2z_i +z_{i+1}\\)).\n\nif \\(= 0 \\rightarrow\\) straight\nif the diff is large \\(\\rightarrow\\) sharply bent\nthe sign tells you which way it bends\n\nThe curvature need to divide by the spacing \\(h\\) between points (assuming equal spacing), i.e., \\((z_{i-1} + 2z_i +z_{i+1}) \\times h^{-2}\\).\nIn a continuous space we measured this by taking the second derivative, i.e. the Laplacian operator\nAs \\(h\\to 0\\) we measure the rate of change of slope, i.e. curvature as:\n\\[(z_{i-1} + 2z_i +z_{i+1}) \\times h^{-2}  \\to \\frac{\\partial z}{\\partial t^2} = \\Delta z\\] Where \\(z(x)\\) is the displacement of the string at position \\(t\\).\nTension (\\(\\kappa\\)): resists large displacement If a tap pushes part of the string far up or down:\n\ntension pulls it back\nstronger tension = stronger pull\n\n\\[\n\\begin{aligned}\n(\\color{red}{\\kappa^2}-\\Delta)Z(t) &= W(t) \\\\\n\\color{red}{\\kappa^2}Z(t) - \\Delta Z(t) &= W(t)\\\\\n\\text{if } Z(t) \\text{ is large then }& \\color{tomato}{\\kappa^2 Z(t)} \\text{ becomes large too} \\\\\n\\text{large pull}-\\text{back bending}&=\\text{random tap}\n\\end{aligned}\n\\] In this case, To balance the equation, \\(Z(t)\\) must be smaller."
  },
  {
    "objectID": "slides/slides_8.html#one-analogy-2",
    "href": "slides/slides_8.html#one-analogy-2",
    "title": "Modelling Geostatistical Data",
    "section": "One analogy",
    "text": "One analogy\n\n\nImagine a guitar string stretched from left to right.\n\nThe rope has tension and stiffness\n\nthe tension spreads each tap to nearby points\nSharp jumps are softened.\n\n\n\n\nStiffness = controls how smooth the field becomes (\\(\\alpha\\))\nLarger \\(\\alpha\\) smoother the process will be\n\n\\[\n(\\kappa^2-\\Delta)^{\\color{red}{\\alpha}/2}Z(t) = W(t)\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\nLastly \\(\\alpha\\) controls how smooth the final shape. E.g.,\n\\[\nW(t) = \\begin{cases}\n(\\kappa^2-\\Delta)Z(t) & \\text{if } \\alpha= 2 \\text{ i.e., string resists curvature once}\\\\\n(\\kappa^2-\\Delta)^2Z(t) & \\text{if } \\alpha= 4 \\text{ i.e., string resists curvature twice}\n\\end{cases}\n\\]\nTake the white noise \\(W(t)\\) and smooth it once by the operator \\((\\kappa^2 - \\Delta)^{-1}\\) so the operator filters the white noise once\n\nit reduces sharp spikes\nbut still leaves some visible bumps\n\nAt \\(\\alpha=2\\) we smooth the result again.. Each pass removes high-frequency components (sharp bumps)"
  },
  {
    "objectID": "slides/slides_8.html#solving-the-spde",
    "href": "slides/slides_8.html#solving-the-spde",
    "title": "Modelling Geostatistical Data",
    "section": "Solving the SPDE",
    "text": "Solving the SPDE\nOk…but we still need to solve the SPDE to find \\(Z(t)\\)!\n\nNow we need to discretize the domain into T points (we cannot compute on the continuous!)\n\n\n\n\nWe represent our solution as\n\\[\nZ(t) = \\sum_{i = 1}^T\\psi_i(t)w_i\n\\]\nWhere\n\n\\(\\psi_i(t)\\) are (known) basis functions for nodes \\(i=1,\\ldots,T\\)\n\n\\(\\psi_i(t_i)= 1\\)\n\n\n\n\\(\\psi_i(t_j) = 0 ~~\\forall~~i \\neq j\\)\n\n\n\nLinear between neighboring nodes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n“solving the SPDE” means Find a random function \\(Z(t)\\) such that the equality \\((\\kappa^2-\\Delta)^{\\alpha/2}Z(t) = W(t)\\) holds in distribution.\nin other words find \\(Z(s)\\) such that when the operator \\((\\kappa^2-\\Delta)^{\\alpha/2}\\) is applied to it, it produces white noise\nOR, The Matérn field is the Gaussian field \\(Z(s)\\) produced by smoothing white noise \\(W(t)\\) through the inverse operator \\((\\kappa^2-\\Delta)^{\\alpha/2}\\).\nThe Matérn field is infinite-dimensional so we approximate it using a finite basis expansion\n\nwe want a simple local basis\neach basis only affects neighbors\nso computations become sparse\n\nEach weight \\(w_i\\) is basically the value of the field at node \\(i\\)\nThe basis functions are:\n\nHeight = 1 at node\nZero at neighboring nodes\nLinear in between\n\nA hat function is not the field. It is a building block for the field.\nthe field between nodes is interpolated linearly so the vector of weights fully defines the approximate field.\nSince the SPDE is linear and the white noise is Gaussian, the solution is Gaussian"
  },
  {
    "objectID": "slides/slides_8.html#solving-the-spde-1",
    "href": "slides/slides_8.html#solving-the-spde-1",
    "title": "Modelling Geostatistical Data",
    "section": "Solving the SPDE",
    "text": "Solving the SPDE\nOk…but we still need to solve the SPDE to find \\(Z(t)\\)!\nNow we need to discretize the domain into T points (we cannot compute on the continuous!)\n\n\nWe represent our solution as\n\\[\nZ(t) = \\sum_{i = 1}^T\\psi_i(t)w_i\n\\]\nWhere\n\n\\(\\psi_i(t)\\) are (known) basis functions for nodes \\(i=1,\\ldots,T\\)\n\\(w_i\\) are (unknown) weights\n\nthe field value \\(Z(s)\\) is a linear interpolation between the two neighboring weights\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n“solving the SPDE” means Find a random function \\(Z(t)\\) such that the equality \\((\\kappa^2-\\Delta)^{\\alpha/2}Z(t) = W(t)\\) holds in distribution.\nin other words find \\(Z(s)\\) such that when the operator \\((\\kappa^2-\\Delta)^{\\alpha/2}\\) is applied to it, it produces white noise\nOR, The Matérn field is the Gaussian field \\(Z(s)\\) produced by smoothing white noise \\(W(t)\\) through the inverse operator \\((\\kappa^2-\\Delta)^{\\alpha/2}\\).\nThe Matérn field is infinite-dimensional so we approximate it using a finite basis expansion\n\nwe want a simple local basis\neach basis only affects neighbors\nso computations become sparse\n\nEach weight \\(w_i\\) is basically the value of the field at node \\(i\\)\nThe basis functions are:\n\nHeight = 1 at node\nZero at neighboring nodes\nLinear in between\n\nA hat function is not the field. It is a building block for the field.\nthe field between nodes is interpolated linearly so the vector of weights fully defines the approximate field.\nSince the SPDE is linear and the white noise is Gaussian, the solution is Gaussian"
  },
  {
    "objectID": "slides/slides_8.html#solving-the-spde-2",
    "href": "slides/slides_8.html#solving-the-spde-2",
    "title": "Modelling Geostatistical Data",
    "section": "Solving the SPDE",
    "text": "Solving the SPDE\nOk…but we still need to solve the SPDE to find \\(Z(t)\\)!\nNow we need to discretize the domain into T points (we cannot compute on the continuous!)\n\n\nWe represent our solution as\n\\[\nZ(t) = \\sum_{i = 1}^T\\psi_i(t)w_i\n\\]\nWhere\n\n\\(\\psi_i(t)\\) are (known) basis functions for nodes \\(i=1,\\ldots,T\\)\n\\(w_i\\) are (unknown) weights\nThis solution is then approximated using a finite combination of piece-wise linear basis functions.\nThe solution is completely defined by a Gaussian vector of weights with zero mean and a sparse precision matrix."
  },
  {
    "objectID": "slides/slides_8.html#the-spde-approach-on-2d",
    "href": "slides/slides_8.html#the-spde-approach-on-2d",
    "title": "Modelling Geostatistical Data",
    "section": "The SPDE approach on 2D",
    "text": "The SPDE approach on 2D\nNow we approximate the GRF using a triangulated mesh.\nThe SPDE approach represents the continuous spatial process as a continuously indexed Gaussian Markov Random Field (GMRF)\n\nWe construct an appropriate lower-resolution approximation of the surface by sampling it in a set of well designed points and constructing a piece-wise linear interpolant."
  },
  {
    "objectID": "slides/slides_8.html#the-spde-approach-on-2d-1",
    "href": "slides/slides_8.html#the-spde-approach-on-2d-1",
    "title": "Modelling Geostatistical Data",
    "section": "The SPDE approach on 2D",
    "text": "The SPDE approach on 2D\nNow we approximate the GRF using a triangulated mesh.\nThe SPDE approach represents the continuous spatial process as a continuously indexed Gaussian Markov Random Field (GMRF)\n\nWe construct an appropriate lower-resolution approximation of the surface by sampling it in a set of well designed points and constructing a piece-wise linear interpolant.\nNote that \\(\\nu = \\alpha - d/2\\). For \\(\\alpha=2 \\Rightarrow \\nu= 1\\) since \\(d=2\\) we have that:\n\n\n\n\nNote\n\n\n\\[\n\\begin{aligned}\nZ(s) &= \\sum_{i = 1}^K\\psi_i(s)w_i \\\\\n\\mathbf{w} &\\sim N(\\mathbf{0},Q^{-1}) \\leftarrow \\text{GMRF}\\\\\nQ^{-1} &= \\tau^2(\\kappa^4 \\mathbf{C} + 2\\kappa^2 \\mathbf{G}+\\mathbf{G}\\mathbf{C}^{-1}\\mathbf{G})\n\\end{aligned}\n\\]\n\n\\(\\mathbf{C}\\) is diagonal with entries \\(C_{ii} =\\int \\psi_i(s)\\mathrm{d}s\\) and measures how much of the domain each basis function covers.\n\\(G_{ij} = \\int \\nabla \\psi_i(s) \\nabla \\psi_j(s) \\mathrm{d}s\\) reflects the connectivity of the mesh nodes.\nbecause each basis function overlaps only with nearby ones, the resulting precision matrix is sparse, meaning each coefficient depends directly only on its neighbors\n\n\n\n\n\nwe approximate a continuous string/field using local “hat2 shapes;\n\nC just measures how much space each hat covers (mass/area),\nG measures how strongly neighboring hats resist bending"
  },
  {
    "objectID": "slides/slides_8.html#in-summary",
    "href": "slides/slides_8.html#in-summary",
    "title": "Modelling Geostatistical Data",
    "section": "In summary",
    "text": "In summary\n\nThe continuous Matérn GRF is the solution of a SPDE and is represented as\n\n\\[\nZ(s) = \\sum_{i = 1}^K\\psi_i(s)w_i\n\\]\n\nThe weights vector \\(\\mathbf{w} = (w_1,\\dots,w_K)\\) is Gaussian with a sparse precision matrix \\(\\longrightarrow\\) Computational convenience\nThe field has two parameters\n\nThe range \\(\\rho\\)\nThe marginal variance \\(\\sigma^2\\)\n\nThese parameters are linked to the parameters of the SPDE\nWe need to assign prior to them"
  },
  {
    "objectID": "slides/slides_8.html#penalized-complexity-pc-priors",
    "href": "slides/slides_8.html#penalized-complexity-pc-priors",
    "title": "Modelling Geostatistical Data",
    "section": "Penalized Complexity (PC) priors",
    "text": "Penalized Complexity (PC) priors\nPenalized Complexity (PC) priors proposed by Simpson et al. (2017) allow us to control the amount of spatial smoothing and avoid overfitting.\n\nPC priors shrink the model towards a simpler baseline unless the data provide strong evidence for a more complex structure.\nTo define the prior for the marginal precision \\(\\sigma^{-2}\\) and the range parameter \\(\\rho\\), we use the probability statements:\n\nDefine the prior for the range \\(\\text{Prob}(\\rho&lt;\\rho_0) = p_{\\rho}\\)\nDefine the prior for the range \\(\\text{Prob}(\\sigma&gt;\\sigma_0) = p_{\\sigma}\\)"
  },
  {
    "objectID": "slides/slides_8.html#learning-about-the-spde-approach",
    "href": "slides/slides_8.html#learning-about-the-spde-approach",
    "title": "Modelling Geostatistical Data",
    "section": "Learning about the SPDE approach",
    "text": "Learning about the SPDE approach\n\n\n\n\nF. Lindgren, H. Rue, and J. Lindström. An explicit link between Gaussian fields and Gaussian Markov random fields: The SPDE approach (with discussion). In: Journal of the Royal Statistical Society, Series B 73.4 (2011), pp. 423–498.\nH. Bakka, H. Rue, G. A. Fuglstad, A. Riebler, D. Bolin, J. Illian, E. Krainski, D. Simpson, and F. Lindgren. Spatial modelling with R-INLA: A review. In: WIREs Computational Statistics 10:e1443.6 (2018). (Invited extended review). DOI: 10.1002/wics.1443.\nE. T. Krainski, V. Gómez-Rubio, H. Bakka, A. Lenzi, D. Castro-Camilio, D. Simpson, F. Lindgren, and H. Rue. Advanced Spatial Modeling with Stochastic Partial Differential Equations using R and INLA. Github version . CRC press, Dec. 20"
  },
  {
    "objectID": "slides/slides_8.html#example-modelling-rainfall-in-brazil",
    "href": "slides/slides_8.html#example-modelling-rainfall-in-brazil",
    "title": "Modelling Geostatistical Data",
    "section": "Example: Modelling Rainfall in Brazil",
    "text": "Example: Modelling Rainfall in Brazil\nLets revisit the Paraná data containing the average rainfall over different years for the period May to June at 123 monitoring stations in Paraná state, Brazil.\n\nRainfall values measured at 143 recording stations in Paraná state, Brazil with low values being represented in blue and high values in red."
  },
  {
    "objectID": "slides/slides_8.html#the-model",
    "href": "slides/slides_8.html#the-model",
    "title": "Modelling Geostatistical Data",
    "section": "The Model",
    "text": "The Model\n\nStage 1 Model for the response \\[\ny(s)|\\eta(s)\\sim\\text{Normal}(\\mu(s),\\sigma^2_e)\n\\]\nStage 2 Latent field model \\[\n\\eta(s) = \\mu(s) = \\beta_0 + Z(s)\n\\]\nStage 3 Hyperparameters"
  },
  {
    "objectID": "slides/slides_8.html#the-model-1",
    "href": "slides/slides_8.html#the-model-1",
    "title": "Modelling Geostatistical Data",
    "section": "The Model",
    "text": "The Model\n\nStage 1 Model for the response \\[\ny(s)|\\eta(s)\\sim\\text{Normal}(\\mu(s),\\sigma^2_e)\n\\]\nStage 2 Latent field model \\[\n\\eta(s) = \\mu(s) = \\beta_0 + Z(s)\n\\]\n\nA global intercept \\(\\beta_0\\)\nA Gaussian field \\(Z(s)\\)\n\nStage 3 Hyperparameters"
  },
  {
    "objectID": "slides/slides_8.html#the-model-2",
    "href": "slides/slides_8.html#the-model-2",
    "title": "Modelling Geostatistical Data",
    "section": "The Model",
    "text": "The Model\n\nStage 1 Model for the response \\[\ny(s)|\\eta(s)\\sim\\text{Normal}(\\mu(s),\\sigma^2_e)\n\\]\nStage 2 Latent field model \\[\n\\eta(s) = \\mu(s) = \\beta_0 + Z(s)\n\\]\nStage 3 Hyperparameters\n\nPrecision for the observational error \\(\\tau_e = 1/\\sigma^{2}_e\\)\nRange and sd in the Gaussian field \\(\\sigma_{\\omega}, \\tau_{\\omega}\\)"
  },
  {
    "objectID": "slides/slides_8.html#step-1-define-the-spde-representation-the-mesh",
    "href": "slides/slides_8.html#step-1-define-the-spde-representation-the-mesh",
    "title": "Modelling Geostatistical Data",
    "section": "Step 1: Define the SPDE representation: The mesh",
    "text": "Step 1: Define the SPDE representation: The mesh\n\n\nFirst, we need to create the mesh used to approximate the random field.\n\nlibrary(fmesher)\nlibrary(inlabru)\nlibrary(INLA)\n\nmesh &lt;- fm_mesh_2d(\n  loc = parana_sf, \n  offset = c(50, 100),\n  cutoff = 1,\n  max.edge = c(30, 60)\n)\n\n\nmax.edge for maximum triangle edge lengths\noffset for inner and outer extensions (to prevent edge effects)\ncutoff to avoid overly small triangles in clustered areas"
  },
  {
    "objectID": "slides/slides_8.html#step-1-define-the-spde-representation-the-mesh-1",
    "href": "slides/slides_8.html#step-1-define-the-spde-representation-the-mesh-1",
    "title": "Modelling Geostatistical Data",
    "section": "Step 1: Define the SPDE representation: The mesh",
    "text": "Step 1: Define the SPDE representation: The mesh\n\n\nAll random field models need to be discretised for practical calculations.\nThe SPDE models were developed to provide a consistent model definition across a range of discretisations.\nWe use finite element methods with local, piecewise linear basis functions defined on a triangulation of a region of space containing the domain of interest.\nDeviation from stationarity is generated near the boundary of the region.\nThe choice of region and choice of triangulation affects the numerical accuracy."
  },
  {
    "objectID": "slides/slides_8.html#step-1-define-the-spde-representation-the-mesh-2",
    "href": "slides/slides_8.html#step-1-define-the-spde-representation-the-mesh-2",
    "title": "Modelling Geostatistical Data",
    "section": "Step 1: Define the SPDE representation: The mesh",
    "text": "Step 1: Define the SPDE representation: The mesh\n\nIf the mesh is too fine \\(\\rightarrow\\) heavy computation\nIf the mesh is to coarse \\(\\rightarrow\\) not accurate enough"
  },
  {
    "objectID": "slides/slides_8.html#step-1-define-the-spde-representation-the-mesh-3",
    "href": "slides/slides_8.html#step-1-define-the-spde-representation-the-mesh-3",
    "title": "Modelling Geostatistical Data",
    "section": "Step 1: Define the SPDE representation: The mesh",
    "text": "Step 1: Define the SPDE representation: The mesh\nSome guidelines\n\nCreate triangulation meshes with fm_mesh_2d():\nedge length should be around a third to a tenth of the spatial range\nMove undesired boundary effects away from the domain of interest by extending to a smooth external boundary:\nUse a coarser resolution in the extension to reduce computational cost (max.edge=c(inner, outer)), i.e., add extra, larger triangles around the border"
  },
  {
    "objectID": "slides/slides_8.html#step-1-define-the-spde-representation-the-mesh-4",
    "href": "slides/slides_8.html#step-1-define-the-spde-representation-the-mesh-4",
    "title": "Modelling Geostatistical Data",
    "section": "Step 1: Define the SPDE representation: The mesh",
    "text": "Step 1: Define the SPDE representation: The mesh\n\nUse a fine resolution (subject to available computational resources) for the domain of interest (inner correlation range) and avoid small edges ,i.e., filter out small input point clusters (0 \\(&lt;\\) cutoff \\(&lt;\\) inner)\nCoastlines and similar can be added to the domain specification in fm_mesh_2d() through the boundary argument.\nsimplify the border"
  },
  {
    "objectID": "slides/slides_8.html#step-2-define-the-spde-representation-the-spde",
    "href": "slides/slides_8.html#step-2-define-the-spde-representation-the-spde",
    "title": "Modelling Geostatistical Data",
    "section": "Step 2: Define the SPDE representation: The SPDE",
    "text": "Step 2: Define the SPDE representation: The SPDE\nWe use the inla.spde2.pcmatern to define the SPDE model using PC priors through the following probability statements.\n\nThe Paraná state is around 663.8711 kilometers width by 464.7481 kilometers height.\nThe PC-prior for the practical range is built considering the probability of the practical range being less than a chosen distance.\nWe chose to set the prior considering the median as 100 kilometers.\n\n\n\n\n\n\n\\(P(\\rho &lt; 100) = 0.5\\)\n\\(P(\\sigma &gt; 1) = 0.5\\)"
  },
  {
    "objectID": "slides/slides_8.html#fit-the-model",
    "href": "slides/slides_8.html#fit-the-model",
    "title": "Modelling Geostatistical Data",
    "section": "Fit the Model",
    "text": "Fit the Model\n\n\nThe Model\n\\[\n\\begin{aligned}\ny(s)|\\eta(s)&\\sim\\text{Normal}(\\mu(s),\\sigma^2_e)\\\\\n\\eta(s) &  = \\color{red}{\\boxed{\\beta_0}} + \\color{red}{\\boxed{ Z(s)}}\\\\\n\\end{aligned}\n\\]\n\n\nparana_sf %&gt;% print(n = 3)\n\nSimple feature collection with 143 features and 1 field\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 150.122 ymin: 70.36 xmax: 768.5087 ymax: 461.9681\nCRS:           NA\nFirst 3 features:\n   value                  geometry\n1 306.09 POINT (402.9529 164.5284)\n2 200.88  POINT (501.7049 428.771)\n3 167.07 POINT (556.3262 445.2706)\n\n\n\nThe code\n\n# define model component\ncmp = ~ -1 + Intercept(1) + space(geometry, model = spde_model)\n\n# define model predictor\neta = value ~ Intercept  + space\n\n# build the observation model\nlik = bru_obs(formula = eta,\n              data = parana_sf,\n              family = \"gaussian\")\n\n# fit the model\nfit = bru(cmp, lik)"
  },
  {
    "objectID": "slides/slides_8.html#fit-the-model-1",
    "href": "slides/slides_8.html#fit-the-model-1",
    "title": "Modelling Geostatistical Data",
    "section": "Fit the Model",
    "text": "Fit the Model\n\n\nThe Model\n\\[\n\\begin{aligned}\ny(s)|\\eta(s)&\\sim\\text{Normal}(\\mu(s),\\sigma^2_e)\\\\\n\\color{red}{\\boxed{\\eta(s)}} &  =\\color{red}{\\boxed{ \\beta_0 + Z(s)}}\\\\\n\\end{aligned}\n\\]\n\n\nparana_sf %&gt;% print(n = 3)\n\nSimple feature collection with 143 features and 1 field\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 150.122 ymin: 70.36 xmax: 768.5087 ymax: 461.9681\nCRS:           NA\nFirst 3 features:\n   value                  geometry\n1 306.09 POINT (402.9529 164.5284)\n2 200.88  POINT (501.7049 428.771)\n3 167.07 POINT (556.3262 445.2706)\n\n\n\nThe code\n\n# define model component\ncmp = ~ -1 + Intercept(1) + space(geometry, model = spde_model)\n\n# define model predictor\neta = value ~ Intercept  + space\n\n# build the observation model\nlik = bru_obs(formula = eta,\n              data = parana_sf,\n              family = \"gaussian\")\n\n# fit the model\nfit = bru(cmp, lik)"
  },
  {
    "objectID": "slides/slides_8.html#fit-the-model-2",
    "href": "slides/slides_8.html#fit-the-model-2",
    "title": "Modelling Geostatistical Data",
    "section": "Fit the Model",
    "text": "Fit the Model\n\n\nThe Model\n\\[\n\\begin{aligned}\n\\color{red}{\\boxed{y(s)|\\eta(s)}} &\\sim\\text{Normal}(\\mu(s),\\sigma^2_e)\\\\\n\\eta(s) &  =\\beta_0 + Z(s)\\\\\n\\end{aligned}\n\\]\n\n\nparana_sf %&gt;% print(n = 3)\n\nSimple feature collection with 143 features and 1 field\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 150.122 ymin: 70.36 xmax: 768.5087 ymax: 461.9681\nCRS:           NA\nFirst 3 features:\n   value                  geometry\n1 306.09 POINT (402.9529 164.5284)\n2 200.88  POINT (501.7049 428.771)\n3 167.07 POINT (556.3262 445.2706)\n\n\n\nThe code\n\n# define model component\ncmp = ~ -1 + Intercept(1) + space(geometry, model = spde_model)\n\n# define model predictor\neta = value ~ Intercept  + space\n\n# build the observation model\nlik = bru_obs(formula = eta,\n              data = parana_sf,\n              family = \"gaussian\")\n\n# fit the model\nfit = bru(cmp, lik)"
  },
  {
    "objectID": "slides/slides_8.html#fit-the-model-3",
    "href": "slides/slides_8.html#fit-the-model-3",
    "title": "Modelling Geostatistical Data",
    "section": "Fit the Model",
    "text": "Fit the Model\n\n\nThe Model\n\\[\n\\begin{aligned}\ny(s)|\\eta(s) &\\sim\\text{Normal}(\\mu(s),\\sigma^2_e)\\\\\n\\eta(s) &  =\\beta_0 + Z(s)\\\\\n\\end{aligned}\n\\]\n\n\nparana_sf %&gt;% print(n = 3)\n\nSimple feature collection with 143 features and 1 field\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 150.122 ymin: 70.36 xmax: 768.5087 ymax: 461.9681\nCRS:           NA\nFirst 3 features:\n   value                  geometry\n1 306.09 POINT (402.9529 164.5284)\n2 200.88  POINT (501.7049 428.771)\n3 167.07 POINT (556.3262 445.2706)\n\n\n\nThe code\n\n# define model component\ncmp = ~ -1 + Intercept(1) + space(geometry, model = spde_model)\n\n# define model predictor\neta = value ~ Intercept  + space\n\n# build the observation model\nlik = bru_obs(formula = eta,\n              data = parana_sf,\n              family = \"gaussian\")\n\n# fit the model\nfit = bru(cmp, lik)"
  },
  {
    "objectID": "slides/slides_8.html#results",
    "href": "slides/slides_8.html#results",
    "title": "Modelling Geostatistical Data",
    "section": "Results",
    "text": "Results\n\n\n\n\n\n\n\n\n\n\n\n\nMean\n2.5%\n97.5%\n\n\n\n\nIntercept\n249.714\n232.748\n264.983\n\n\nPrecision for the Gaussian observations\n4.482\n3.197\n5.511\n\n\nRange for space\n57.328\n46.234\n70.330\n\n\nStdev for space\n46.654\n41.222\n52.736\n\n\n\n\n\nIntercept represents the average rainfall values\nPrecision for the Gaussian observations are the observational errors\nRange for space is the correlation of the Matérn field\nStdev for space is the marginal std deviation of the Matérn field"
  },
  {
    "objectID": "slides/slides_8.html#how-do-we-predict-at-unsampled-locations",
    "href": "slides/slides_8.html#how-do-we-predict-at-unsampled-locations",
    "title": "Modelling Geostatistical Data",
    "section": "How do we predict at unsampled locations?",
    "text": "How do we predict at unsampled locations?\nIn geostatistical applications, the main interest resides in the spatial prediction of the spatial latent field or of the response variable in new locations\n\n\nSuppose we observe a spatial process \\({Z(s): s \\in \\mathcal{D}}\\) at locations \\(s_1,\\dots,s_n\\).\nOur goal: predict the variable of interest at an unobserved location \\(s_0 \\in \\mathcal{D}\\).\n\ngiven the data \\(y = (y_1,\\dots,y_n)\\), what can we say about \\(Z(s_0)\\)?\n\nRather than a single guess, we want a full uncertainty-aware prediction.\nIn a Bayesian setting, prediction is a probabilistic task."
  },
  {
    "objectID": "slides/slides_8.html#posterior-predictive-density",
    "href": "slides/slides_8.html#posterior-predictive-density",
    "title": "Modelling Geostatistical Data",
    "section": "Posterior predictive density",
    "text": "Posterior predictive density\nThe key lies in the posterior predictive distribution\n\\[\n  \\pi(\\tilde{Y} \\mid y)\n  = \\int \\pi(\\tilde{Y} \\mid \\Theta, y)\\, \\pi(\\Theta \\mid y)\\, d\\Theta,\n\\] where \\(\\Theta\\) denotes all latent components and hyperparameters.\n\n\nThe prediction likelihood \\(\\pi(\\tilde{Y} \\mid \\Theta, y)\\) depends on the task:\n\nextrapolation(e.g. forecasting of an AR(1)): \\(\\pi(Y_{n+1} \\mid \\Theta, y_n)\\),\ninterporlation: \\(\\pi(Y_i \\mid y_{i-1}, y_{i+1}, \\Theta)\\),\n\nSpatial prediction fits naturally into this framework:\n\n\\(\\tilde{Y}\\) may represent \\(Z(s_0)\\), \\(\\eta_0\\), or the response at \\(y(s_0)\\),\nconditioning reflects the assumed spatial dependence.\n\nINLA approximates \\(\\pi(\\Theta \\mid y)\\) efficiently, enabling full uncertainty propagation when predicting over \\(s_0 \\in \\mathcal{D}\\)."
  },
  {
    "objectID": "slides/slides_8.html#posterior-predictive-density-1",
    "href": "slides/slides_8.html#posterior-predictive-density-1",
    "title": "Modelling Geostatistical Data",
    "section": "Posterior predictive density",
    "text": "Posterior predictive density\n\n\nStep 0 — Augment the data with prediction locations\n\nIntroduce locations where predictions are desired.\nThe response is set to NA at these locations.\nCovariates and spatial coordinates are still provided.\n\nStep 1 — Build the projector matrix A\nStep 2 — Joint posterior inference\nStep 3 — Posterior evaluation at prediction locations\n\n\ndims = c(150, 150)\npred.df &lt;- fm_pixels(mesh,dims = dims,mask =border,  format = \"sf\")"
  },
  {
    "objectID": "slides/slides_8.html#posterior-predictive-density-2",
    "href": "slides/slides_8.html#posterior-predictive-density-2",
    "title": "Modelling Geostatistical Data",
    "section": "Posterior predictive density",
    "text": "Posterior predictive density\n\n\nStep 0 — Augment the data with prediction locations\nStep 1 — Build the projector matrix A\n\nA projector matrix \\(A\\) linking the latent Gaussian field to the prediction locations.\nThis ensures that the spatial effects and covariates at new locations are properly included in the model.\nLinear predictors are computed at these new locations\n\nStep 2 — Joint posterior inference\nStep 3 — Posterior evaluation at prediction locations\n\n\npred &lt;- predict(fit,pred.df,\n                formula ~ data.frame(\n    spde =  space,\n    eta = Intercept + space\n  )\n)"
  },
  {
    "objectID": "slides/slides_8.html#posterior-predictive-density-3",
    "href": "slides/slides_8.html#posterior-predictive-density-3",
    "title": "Modelling Geostatistical Data",
    "section": "Posterior predictive density",
    "text": "Posterior predictive density\n\n\nStep 0 — Augment the data with prediction locations\nStep 1 — Build the projector matrix A\nStep 2 — Joint posterior inference\n\nINLA computes the posterior of:\n\nthe latent Gaussian field,\nfixed effects,\nhyperparameters.\n\n\nStep 3 — Posterior evaluation at prediction locations\n\n\npred &lt;- predict(fit,pred.df,\n                formula ~ data.frame(\n    spde =  space,\n    eta = Intercept + space\n  )\n)"
  },
  {
    "objectID": "slides/slides_8.html#posterior-predictive-density-4",
    "href": "slides/slides_8.html#posterior-predictive-density-4",
    "title": "Modelling Geostatistical Data",
    "section": "Posterior predictive density",
    "text": "Posterior predictive density\n\n\nStep 0 — Augment the data with prediction locations\nStep 1 — Build the projector matrix A\nStep 2 — Joint posterior inference\nStep 3 — Posterior evaluation at prediction locations\n\nPredictions come from the posterior marginals of the latent field and linear predictor.\nOutputs include posterior means, variances, and credible intervals."
  },
  {
    "objectID": "slides/slides_8.html#example-modelling-pacific-cod-biomass-density",
    "href": "slides/slides_8.html#example-modelling-pacific-cod-biomass-density",
    "title": "Modelling Geostatistical Data",
    "section": "Example: Modelling Pacific Cod Biomass Density",
    "text": "Example: Modelling Pacific Cod Biomass Density\nIn the next example, we will explore data on the Pacific Cod (Gadus macrocephalus) from a trawl survey in Queen Charlotte Sound.\n\nThe dataset the biomass density (kg/km\\(^2\\)) of Pacific cod in the area swept for a given survey in 2003 as well as depth covariate information."
  },
  {
    "objectID": "slides/slides_8.html#exploratory-plots",
    "href": "slides/slides_8.html#exploratory-plots",
    "title": "Modelling Geostatistical Data",
    "section": "Exploratory plots",
    "text": "Exploratory plots\n\n\n\n\n\n\n\n\n\n\n\n\nEnvelope Variogram considering only where biomass \\(&gt;\\) 0\n\n\n\n\n\n\n\n\n\n[1] \"There are 2 out of 15 variogram estimates outside the 95% envelope.\""
  },
  {
    "objectID": "slides/slides_8.html#exploratory-plots-1",
    "href": "slides/slides_8.html#exploratory-plots-1",
    "title": "Modelling Geostatistical Data",
    "section": "Exploratory plots",
    "text": "Exploratory plots\n\n\n\n\n\n\n\n\n\n\n\n\nEnvelope Variogram considering only where biomass \\(&gt;\\) 0\n\n\n\n\n\n\n\n\n\n[1] \"There are 2 out of 15 variogram estimates outside the 95% envelope.\"\n\n\n\nWe thus have a dilemma.\n\nIf we omit the zeros, we’ll get a good, accurate model fit for non-zero data, but we’ll be throwing away all the data with zeros\nIf we include the zeros, we won’t be throwing any data away, but we’ll get a strange-fitting model that both under- and over-predicts values.\nSo what do we do?"
  },
  {
    "objectID": "slides/slides_8.html#a-non-spatial-model",
    "href": "slides/slides_8.html#a-non-spatial-model",
    "title": "Modelling Geostatistical Data",
    "section": "A non-spatial Model",
    "text": "A non-spatial Model\n\nStage 1 Model for the response \\[\ny(s)|\\eta(s)\\sim \\text{Tweedie}(p,\\mu_i,\\phi)\n\\]\n\nTweedie account for positive continuous density values that also contain zeros\n\\(p\\) determines the shape of the variance function (shifts from a Poisson distribution at \\(p=1\\) to a gamma distribution at \\(p=2\\))\n\\(\\mu(s) = \\exp ⁡\\eta (i)\\) is the mean linked to linear predictor by the log link.\n\\(\\phi\\) = dispersion parameter .\n\nStage 2 Latent field model \\[\n\\eta(s) = \\text{log}(\\mu(s)) = \\beta_0 + \\beta_1 \\text{depth} + \\beta_2 \\text{depth}^2\n\\]\nStage 3 Hyperparameters"
  },
  {
    "objectID": "slides/slides_8.html#a-non-spatial-model-1",
    "href": "slides/slides_8.html#a-non-spatial-model-1",
    "title": "Modelling Geostatistical Data",
    "section": "A non-spatial Model",
    "text": "A non-spatial Model\n\nStage 1 Model for the response\\[\ny(s)|\\eta(s)\\sim \\text{Tweedie}(p,\\mu_i,\\phi)\n\\]\nStage 2 Latent field model \\[\n\\eta(s) = \\exp(\\mu(s)) = \\beta_0 + \\beta_1 x(s) + \\beta_2 x(s)^2\n\\]\n\nA global intercept \\(\\beta_0\\)\nA effect of covariate \\(x(s)\\) (depth)\nA quadratic effect of covariate \\(x(s)\\) (depth)\n\nStage 3 Hyperparameters"
  },
  {
    "objectID": "slides/slides_8.html#a-non-spatial-model-2",
    "href": "slides/slides_8.html#a-non-spatial-model-2",
    "title": "Modelling Geostatistical Data",
    "section": "A non-spatial Model",
    "text": "A non-spatial Model\n\nStage 1 Model for the response \\[\ny(s)|\\eta(s)\\sim \\text{Tweedie}(p,\\mu_i,\\phi)\n\\]\nStage 2 Latent field model\\[\n\\eta(s) = \\exp(\\mu(s)) = \\beta_0 + \\beta_1 x(s) + \\beta_2 x(s)^2\n\\]\nStage 3 Hyperparameters\n\ndispersion parameter \\(\\phi\\)\npower parameter \\(p\\)"
  },
  {
    "objectID": "slides/slides_8.html#a-non-spatial-model-3",
    "href": "slides/slides_8.html#a-non-spatial-model-3",
    "title": "Modelling Geostatistical Data",
    "section": "A non-spatial Model",
    "text": "A non-spatial Model\n\n\n\n\n\n\n\n\n\n\nINLA Model Results\n\n\nPosterior summaries of fixed effects and hyperparameters\n\n\nParameter\nMean\n2.5% Quantile\n97.5% Quantile\n\n\n\n\nIntercept\n3.866\n3.690\n4.043\n\n\ndepth\n−1.269\n−1.480\n−1.059\n\n\ndepth2\n−1.077\n−1.256\n−0.898\n\n\np parameter for Tweedie\n1.643\n1.617\n1.668\n\n\nDispersion parameter for Tweedie\n3.804\n3.530\n4.093\n\n\n\n\n\n\n\n\n\n\\(\\beta\\) coef suggest log-biomass peaks at an intermediate depth within the study range and decreases toward both shallower and deeper extremes.\n\\(\\phi&gt;1\\) indicates overdispersion relative to the variance function. Potentially caused by Unobserved heterogeneity.\nTweedie models fitted to biomass usually have convergence issues when there are large spatial areas with many zeros.\nIs there a better approach?"
  },
  {
    "objectID": "slides/slides_8.html#a-multilikelihood-hurdle-geostatistical-model",
    "href": "slides/slides_8.html#a-multilikelihood-hurdle-geostatistical-model",
    "title": "Modelling Geostatistical Data",
    "section": "A multilikelihood Hurdle Geostatistical Model",
    "text": "A multilikelihood Hurdle Geostatistical Model\n\nStage 1 Model for the response(s) \\[\n\\begin{aligned}\ny_i|\\eta^{(1)}_i&\\sim \\text{Binomial}(1,\\pi_i)\\\\\n\\log(z_i)|\\eta^{(2)}_i&\\sim \\text{Normal}(\\mu_i,\\tau_e^{-1})\\\\\n\\end{aligned}\n\\]\n\nWe then define a likelihood for each outcome.\n\n\\(y_i =\\begin{cases} 1 &\\text{if fishes have been caught at location } \\mathbf{s}_i  \\\\ 0 &\\text{otherwise}\\end{cases}\\)\n\\(z_i =\\begin{cases} NA &\\text{if no fish were  caught at location } \\mathbf{s}_i  \\\\ \\text{biomass density at location } \\mathbf{s}_i &\\text{otherwise}\\end{cases}\\)\n\n\nStage 2 Latent field model \\[\n\\begin{aligned}\n\\eta^{(1)}_i &= \\text{logit}(\\pi_i) = X'\\beta + \\xi_i\\\\\n\\eta^{(2)}_i &= \\mu_i = X'\\alpha + \\omega_i\n\\end{aligned}\n\\]\nStage 3 Hyperparameters"
  },
  {
    "objectID": "slides/slides_8.html#a-multilikelihood-hurdle-geostatistical-model-1",
    "href": "slides/slides_8.html#a-multilikelihood-hurdle-geostatistical-model-1",
    "title": "Modelling Geostatistical Data",
    "section": "A multilikelihood Hurdle Geostatistical Model",
    "text": "A multilikelihood Hurdle Geostatistical Model\n\nStage 1 Model for the response(s)] \\[\n\\begin{aligned}\ny_i|\\eta^{(1)}_i&\\sim \\text{Binomial}(1,\\pi_i)\\\\\n\\log(z_i)|\\eta^{(2)}_i&\\sim \\text{Normal}(\\mu_i,\\tau_e^{-1})\\\\\n\\end{aligned}\n\\]\nStage 2 Latent field model \\[\n\\begin{aligned}\n\\eta^{(1)}_i &= \\text{logit}(\\pi_i) = X'\\beta + \\xi_i\\\\\n\\eta^{(2)}_i &= \\mu_i = X'\\alpha + \\omega_i\n\\end{aligned}\n\\]\n\n\\(\\{\\alpha,\\beta\\}\\) = Intercepts + covariate effects.\n\\(\\{\\xi,\\omega\\}\\) = are the Gaussian fields with Matérn covariance (separate for each outcome).\n\nStage 3 Hyperparameters"
  },
  {
    "objectID": "slides/slides_8.html#a-multilikelihood-hurdle-geostatistical-model-2",
    "href": "slides/slides_8.html#a-multilikelihood-hurdle-geostatistical-model-2",
    "title": "Modelling Geostatistical Data",
    "section": "A multilikelihood Hurdle Geostatistical Model",
    "text": "A multilikelihood Hurdle Geostatistical Model\n\nStage 1 Model for the response(s) \\[\n\\begin{aligned}\ny_i|\\eta^{(1)}_i&\\sim \\text{Binomial}(1,\\pi_i)\\\\\n\\log(z_i)|\\eta^{(2)}_i&\\sim \\text{Normal}(\\mu_i,\\tau_e^{-1})\\\\\n\\end{aligned}\n\\]\nStage 2 Latent field model \\[\n\\begin{aligned}\n\\eta^{(1)}_i &= \\text{logit}(\\pi_i) = X'\\beta + \\xi_i\\\\\n\\eta^{(2)}_i &= \\mu_i = X'\\alpha + \\omega_i\n\\end{aligned}\n\\]\nStage 3 Hyperparameters\n\nobservational error (nugget) \\(\\tau_e\\)\nMatérn field(s) parameters \\(\\{\\rho^{(1)},\\rho^{(2)},\\tau_{d}^{(1)},\\tau_{d}^{(2)}\\}\\)"
  },
  {
    "objectID": "slides/slides_8.html#the-mesh-and-spde-representation",
    "href": "slides/slides_8.html#the-mesh-and-spde-representation",
    "title": "Modelling Geostatistical Data",
    "section": "The mesh and SPDE representation",
    "text": "The mesh and SPDE representation"
  },
  {
    "objectID": "slides/slides_8.html#hurdle-model-results",
    "href": "slides/slides_8.html#hurdle-model-results",
    "title": "Modelling Geostatistical Data",
    "section": "Hurdle Model Results",
    "text": "Hurdle Model Results\n\n\n\n\n\n\n\n\n\n\nParameter\nMean\n2.5% Quantile\n97.5% Quantile\n\n\n\n\n\\[\\alpha_0\\]\n3.498\n2.885\n4.227\n\n\n\\[\\alpha_1\\]\n−0.509\n−1.098\n0.059\n\n\n\\[\\alpha_2\\]\n−0.339\n−0.789\n0.106\n\n\n\\[\\beta_0\\]\n1.761\n−3.215\n7.267\n\n\n\\[\\beta_1\\]\n−2.564\n−3.626\n−1.694\n\n\n\\[\\beta_2\\]\n−1.495\n−2.143\n−0.944\n\n\n\\[\\tau_\\epsilon^2\\]\n1.152\n0.464\n2.575\n\n\n\\[\\rho^{[1]}\\]\n36.647\n6.874\n103.820\n\n\n\\[\\tau_{d,1}\\]\n1.001\n0.571\n1.658\n\n\n\\[\\rho^{[2]}\\]\n158.333\n57.011\n398.763\n\n\n\\[\\tau_{d,2}\\]\n2.201\n1.192\n3.768\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\alpha_0\\) is the baseline catching probability on the logit scale\n\\(\\beta_0\\) is the predicted log(biomass density at the average depth (since these have been scaled)\nCoefficients \\(\\alpha_1,\\alpha_2\\) refer to the change in the log-odds of catching fish as we increase 1 depth unit and unit\\(^2\\) respectively.\nCoefficients \\(\\beta_1, \\beta_2\\) indicate that the log-biomass decreases with depth.\n\\(\\rho^{[1]},\\rho^{[2]}\\), suggest spatial correlation decays at 36.65 and 158.33 Km respectively (the extension of the study is approx 46,000 km\\(^2\\))\nunstructured variability is given by \\(\\tau^{-1}_e\\) while ,\\(\\{\\tau_{\\delta,1}^{-1},\\tau^{-1}_{d,2}\\}\\) represent the spatially structured variability."
  },
  {
    "objectID": "slides/slides_8.html#model-comparison",
    "href": "slides/slides_8.html#model-comparison",
    "title": "Modelling Geostatistical Data",
    "section": "Model comparison",
    "text": "Model comparison\nNote that in the hurdle model there is no direct link between the parameters of the two observation parts.\n\n\nthe two likelihoods could share some of the components; for example the Matérn field could be used for both predictors.\nWhat does the previous results suggest in terms of the estimated covariance parameters for the two fields? is it sensible to share the same component between the two parts?\nWe will fit a model that estimates this field jointly and compare it with our two previous models\n\n\n\nThe model being fitted is now:\n\\[\n\\begin{aligned}\ny_i|\\eta^{(1)}_i&\\sim \\text{Binomial}(1,\\pi_i)\\\\\n\\eta^{(1)}_i &= \\text{logit}(\\pi_i) = X'\\beta + \\color{red}{\\xi_i}\\\\\n\\log(z_i)|\\eta^{(2)}_i&\\sim \\text{Normal}(\\mu_i,\\tau_e^{-1})\\\\\n\\eta^{(2)}_i &= \\mu_i = X'\\alpha +  \\color{red}{\\xi_i}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/slides_8.html#model-comparison-1",
    "href": "slides/slides_8.html#model-comparison-1",
    "title": "Modelling Geostatistical Data",
    "section": "Model comparison",
    "text": "Model comparison\nNote that in the hurdle model there is no direct link between the parameters of the two observation parts.\n\nthe two likelihoods could share some of the components; for example the Matérn field could be used for both predictors.\nWhat does the previous results suggest in terms of the estimated covariance parameters for the two fields? is it sensible to share the same component between the two parts?\nWe will fit a model that estimates this field jointly and compare it with our two previous models\n\n\n\n\n\n\n\n\n\nModel\nDIC\nWAIC\nMLIK\n\n\n\n\nTweedie\n1,599.690\n1,612.043\n−979.532\n\n\nHurdle\n1,185.793\n1,200.690\n−660.936\n\n\nHurdle 2\n1,227.128\n1,227.340\n−666.295"
  },
  {
    "objectID": "slides/slides_8.html#spatial-predictions-1",
    "href": "slides/slides_8.html#spatial-predictions-1",
    "title": "Modelling Geostatistical Data",
    "section": "Spatial predictions",
    "text": "Spatial predictions\n\n\nWe need to compute:\n\n\\(\\pi(s)\\) = Catching probability\n\\(\\mathbb{E}[Z(s)|Y(s)] = \\exp\\left(\\mu(s) + \\dfrac{1}{2\\tau_{e}}\\right)\\)\n\\(\\mathbb{E}(Z(s)) =\\pi(s)\\times \\mathbb{E}[Z(s)|Y(s)]\\)"
  }
]