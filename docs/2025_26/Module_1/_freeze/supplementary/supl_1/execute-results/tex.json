{
  "hash": "ae226b3294b7f4b5f5accc4e4952da1b",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Supplementary Notes\"\nsubtitle: \"Estimator properties\"\nformat:\n  html:    \n    code-link: true\n    code-fold: true\n    code-tools:\n      source: false\n      toggle: true\n    toc: true\n    toc-location: left\n    toc-title: Contents\n    number-sections: true\n  pdf:\n    keep-tex: true\n    number-sections: true\n    latex-auto-install: true\neditor: visual\neditor_options: \n  chunk_output_type: console\nexecute: \n  freeze: auto\n---\n\n::: {.cell}\n\n:::\n\n\n\n\n\n\nIn this section we recap some important statistical properties of our estimators (for further details please refer to the statistical inference course).\n\nA population parameter $\\theta$ represents a quantitative measurement of population values. Since $\\theta$ is typically unknown, we need to estimate it by drawing samples from the population of interest. Our estimator, $\\hat{\\theta}$,is then computed based on the samples we have obtained. The question is then, how well does $\\hat{\\theta}$ do its job in estimating $\\theta$? To evaluate this, we examine key statistical properties of estimators.\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Sampling from Normal Distribution. The black curve represent the true population distribution with mean represented by the black solid line. The colored point are two different samples with their corresponding sample mean shown in colured dashed lines.](supl_1_files/figure-pdf/fig-sampling1-1.pdf){#fig-sampling1 fig-align='center'}\n:::\n:::\n\n\n\n\n\n\n**Consistency**\n\nAs we see in @fig-sampling1, there is some discrepancy between the sample means and the true population mean. Thus, it is expected that, as the sample size increases, our estimator $\\hat{\\theta}$ will get closer to the true value $\\theta$. Thus an estimator $\\hat{\\theta}$ is said to be *consistent* if it converges to the true parameter $\\theta$ as the sample size $n$ increases, i.e., $\\hat{\\theta} \\rightarrow \\theta$ as $n\\to\\infty$. E.g., let $\\hat{\\theta} = \\bar{x} + \\frac{1}{n}$ be a biased but consistent estimator of $\\theta = 5$ ($\\bar{x}$ represents the sample mean). @fig-sampling2 shows how as sample size increases, the estimator converges to the true parameter.\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![ Demonstration of estimator consistency. The dashed red line indicates the true parameter value. The blue line tracks the mean estimate across increasing sample sizes, while the shaded region represents the 95% confidence interval. ](supl_1_files/figure-pdf/fig-sampling2-1.pdf){#fig-sampling2 fig-align='center'}\n:::\n:::\n\n\n\n\n\n\n**Expected value**\n\nThe expected value of an estimator is a weighted average of all possible estimates. Here, the weights are given by the probability of selecting a particular sample $s$, i.e., $p(s)$. Mathematically, this can be written as:\n\n$$\n\\mathbb{E}(\\hat{\\theta})= \\sum_{x\\in\\Omega} p(s)\\hat{\\theta(s)}\n$$\n\nWhere $\\Omega$ is the sample space, i.e. the number of possible samples. Furthermore, if all possible samples are equally likely then $p(s)=\\frac{1}{\\Omega}$ and the expected values becomes:\n\n$$\n\\mathbb{E}(\\hat{\\theta}) = \\dfrac{1}{\\Omega} \\sum_{s\\in\\Omega} \\hat{\\theta}(s).\n$$\n\nNote that the expected value is a function of both, the sampling design (due to $p(s)$) and the population being sampled (through the sample estimate $\\hat{\\theta}(s)$).\n\n::: {.callout-important icon=\"false\"}\n## {{< bi pencil-square color=#c8793c >}} Task 1\n\nSuppose we have a tiny population of three trees with the following diameters (cm) :\n\n| Tree | diameter (cm) |\n|------|---------------|\n| 1    | 9.1           |\n| 2    | 10.2          |\n| 3    | 17.2          |\n\nNow, we sample only **2 trees** and measure the diameter of each tree in the sample. It is clear from here that there are only $\\Omega=3$ possible samples we could've taken:\n\n1.  $s_1 = \\{\\text{tree 1},\\text{tree 2}\\}$\n2.  $s_2 = \\{\\text{tree 1},\\text{tree 3}\\}$\n3.  $s_3 = \\{\\text{tree 2},\\text{tree 3}\\}$\n\nImagine that samples are selected with an equal probability so that $p(s_1) = p(s_2) = p(s_3) = 1/3$.\n\nThe true population mean diameter is $\\mu_y = ( 9.1+10.2+ 17.2)/3 =12.17$.\n\nSuppose we chose the sample average $\\bar{y}$ as our estimate of $\\mu_y$. Then, the expected value of $\\bar{y}$ is given by\n\n$$\n\\begin{aligned}\\mathbb{E}(\\bar{y}) &= p(s_1) \\left(\\dfrac{9.1+10.2}{2}\\right) +  p(s_2) \\left(\\dfrac{9.1+17.2}{2}\\right) +  p(s_3) \\left(\\dfrac{10.2+17.2}{2}\\right)\\\\&= \\dfrac{1}{3}\\left(\\dfrac{19.3}{2} + \\dfrac{26.3}{2} + \\dfrac{27.4}{2} \\right)\\\\&= 12.17 = \\mu_y\\end{aligned}\n$$\n\nWhat would have happened if our sampling design had unequal sampling probabilities given by $p(s_1) = 1/2$, $p(s_2) = 1/3$ and $p(s_3) = 1/6$? Calculate $\\mathbb{E}(\\hat{y})$ under this sampling scenario.\n\n\n<div class='webex-solution'><button>See Solution</button>\n\n\n$$\n\\begin{aligned}\n\\mathbb{E}(\\bar{y}) &= p(s_1) \\left(\\dfrac{9.1+10.2}{2}\\right) +  p(s_2) \\left(\\dfrac{9.1+17.2}{2}\\right) +  p(s_3) \\left(\\dfrac{10.2+17.2}{2}\\right)\\\\\n&= \\dfrac{1}{2}\\left(\\dfrac{19.3}{2} \\right) + \\frac{1}{3} \\left(\\dfrac{26.3}{2}\\right) + \\dfrac{1}{6}\\left(\\dfrac{27.4}{2} \\right)\\\\\n&= \\dfrac{19.3}{4}  + \\dfrac{26.3}{6} + \\dfrac{27.4}{12} \\\\\n&= 11.5 \\neq \\mu_y\n\\end{aligned}\n$$\n\n\n</div>\n\n:::\n\nIn practice, $\\mathbb{E}(\\hat{\\theta})$ cannot be evaluated because we can not measure all the $N$ elements of our population of interest! (and if we could then $\\theta$ could be evaluated directly on our population rather than obtaining a samples from it). However, understanding this concept is important for two other relevant quantities.\n\n**Bias**\n\nThe bias of an estimator is the difference in magnitude between its expected value and the population parameter for which an estimated is desired:\n\n$$\n\\textbf{Bias}(\\hat{\\theta}) = \\mathbb{E}(\\hat{\\theta}) - \\theta\n$$\n\nWhen $\\mathbb{E}(\\hat{\\theta}) = \\theta$ , $\\hat{\\theta}$ is said to be an *unbiased estimator* of $\\theta$.\n\nNote that the bias is not a property of an individual estimate (e.g., $\\hat{\\theta}(s)$). E.g., we can see in @fig-sampling1 there are some discrepancies between the true population parameter $\\theta$ and the sample estimates $\\hat{\\theta}(1)$ and $\\hat{\\theta}(2)$. Such discrepancies $\\hat{\\theta}(s) - \\theta$ are known as *sampling errors*. This doesn't mean that an error has been made during the sampling, it just indicates that the value being estimated will differ from the true value because is being estimated from just a fraction of the elements of the population. Thus, one may wonder how much an estimate $\\hat{\\theta}$ from one sample will differ from that calculated from a different sample. In principle we would like this difference to be small because that will ensure that no matter which sample we take, the estimated value will be similar across samples.\n\n**Variance**\n\nThe **variance** of an estimator or *sampling variance* is the average squared distance between individual estimates $\\hat{\\theta}(s)$ and their expected value $\\mathbb{E}(\\hat{\\theta})$, i.e.,\n\n$$\n\\text{Var}(\\hat{\\theta}) = \\sum_{s\\in \\Omega} p(s) \\left(\\hat{\\theta}(s)-\\mathbb{E}(\\hat{\\theta})\\right)^2 \n$$ Notice how the variance of an estimator does not depend (unlike the bias) on the true parameter $\\theta$\n\n::: {.callout-important icon=\"false\"}\n## {{< bi pencil-square color=#c8793c >}} Task 2\n\nUsing the data from the previous task and assuming equal sampling probabilities, calculate $\\text{Var}(\\hat{y})$\n\n\n<div class='webex-solution'><button>See Solution</button>\n\n\n$$\n\\begin{aligned}\n\\text{Var}(\\bar{y}) &= p(s_1) \\left(\\dfrac{19.3}{2} - 12.17\\right)^2 +  p(s_2) \\left(\\dfrac{26.3}{2} - 12.17\\right)^2 +  p(s_3) \\left(\\dfrac{27.4}{2} - 12.17\\right)^2\\\\\n&= \\dfrac{1}{3}\\left(  9.65 \\right) =  3.22 \n\\end{aligned}\n$$\n\n\n</div>\n\n:::\n\n**Precision**\n\nThe **precision** of an estimator is a qualitative measurement that assess how small or large the variability of an estimator is and does not relate to the true value. To illustrate this, suppose we draw 20 random samples with equal probability from our population of interest . The figure below shows 100 replicates/data sets of this experiment with 4 different estimators:\n\n-   $\\hat{\\theta}_{1}(s) =\\frac{1}{20}\\sum_{i=1}^{20} x_i$ - the mean of the $s$th sample which is **unbiased** and **precise**\n\n-   $\\hat{\\theta}_{2}(s) = \\text{arg min } (x_1,\\ldots, x_{20})$ - the $s$th sample minimum value which is **biased** and **imprecise**.\n\n-   $\\hat{\\theta}_{3}(s)  = x_1$ - the first observation of the unordered sample which is **unbiased** and **imprecise.**\n\n-   $\\hat{\\theta}_{4}(s) =2 + \\frac{1}{20}\\sum_{i=1}^{20} x_i$ - the mean + constant which is **biased** but **precise**.\n\n::: {.content-visible when-format=\"html\"}\n![Illustration of bias and precision of an estimator. The red dashed line shows the true population parameter. the grey point shows the sample of the *j*-th data set, and the colored points indicate the estimates for each sample.](images/four_estimators.gif){#fig-bias_prec fig-align=\"center\"}\n:::\n\nImagine a dartboard where the bullseye represents the true population parameter ($\\theta$). Each throw corresponds to a sample estimate ($\\hat{\\theta}(s)$). Precision refers to how tightly clustered the darts are - a precise estimator produces estimates that land close together, regardless of their position relative to the bullseye. Bias, on the other hand, is the systematic offset from the bullseye - even with perfect precision, a biased estimator would consistently miss the center in the same direction.\n\n![](images/bias_prec2.png){fig-align=\"center\" width=\"413\"}\n",
    "supporting": [
      "supl_1_files\\figure-pdf"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}